---
title: "cdcdata-exercise"
author: "Zane Chumley"
date: "2024-07-05"
output: html_document
---

# Assignment 5: Processing Data

## Startup

### Introduction

```{r}
# DA 6833 02T
# Summer 2024
# School of Data Science
# University of Texas at San Antonio

# Zane Chumley
# Banner ID: @01318598
# UTSAid: wgs999
```

### Install Packages as Needed

```{r}
# install.packages("simstudy")
```

### Load Libraries as Needed

```{r}
# library(readxl) #for loading Excel files
# library(dplyr) #for data processing/cleaning
# library(tidyr) #for data processing/cleaning
# library(tidyverse)
# library(skimr) #for nice visualization of data 
# library(here) #to set paths
# library(dslabs) #for data used in this assignment
# library(tibble)
# library(simstudy)
library(sqldf)
```

### Define Global Variables

```{r}
# Define path to .csv file containing data
ZC.filepath <- "Nutrition__Physical_Activity__and_Obesity_-_Behavioral_Risk_Factor_Surveillance_System_20240703.csv"
# The master dataset may undergo dimensional changes throughout this exercise.  The changing dimensions will be tracked over time.  Variables to track these changing dimensions will be created.
ZC.dimindex <- 0
ZC.datarows <- list()
ZC.datacols <- list()
# There may be columns with data arbitrarily judged to have worthless data.  Variables will be created to store these column names for subsequent examination.
ZC.columnindex <- 0
ZC.examinedcolumns <- list()
# There may be lookup tables created during this exercise to simplify the master dataset.  When created, the column containing the lookup table and the name of the lookup table will be stored.
ZC.lookupindex <- 0
ZC.lookupcolumn <- list()
ZC.lookuptable <- list()
```

## Part 1

### Part 1-a: Finding the Data

_Previously, you did a quick exploration of a dataset that came with an R package (`gapminder` data inside `dslabs` package). A lot of datasets can be found inside R packages. For instance, this page lists what is likely only a small fractionLinks to an external site.. The good and the bad about datasets that come with R packages is that they are often fairly clean/tidy. That’s unfortunately not how most “real world” datasets look like. Getting dirty and messy datasets and wrangling them into a form that is suitable for statistical analysis is part of most workflows and often takes a lot of time. We’ll start practicing this here by getting data that might or might not be very clean._

_Go to the CDC’s data website at https://data.cdc.gov/Links to an external site.. Browse through the site and identify a dataset of interest._

_Which dataset you choose is up to you. I suggest you pick a dataset that has at least 100 observations with 5 different variables, and a mix of continuous and categorical ones. Often, 5 variables means 5 columns. That would be the case in properly formatted data. However, some of the data provided by the CDC is rather poorly formatted. For instance CDC’s dataset on traumatic brain injuryLinks to an external site. has the same variable (age) in separate columns, and it is also discretized. As we’ll discuss, these are two really bad things you can do to your data, so I recommend staying away from such datasets. There are plenty on that website, so I’m sure you’ll find one that is suitable and interesting to you._

### Part 1-b: Getting the Data

_To get the dataset you selected, it is easiest if you download the file to your computer and place it inside your portfolio repository. Note that in general, you should make each data analysis (or other) project its own GitHub repository, and always use a structure like the one provided in the `Data Analysis Template` (or something similar). However, for this small exercise and for logistic reasons, you’ll use your portfolio/website repository, and just a single folder. Make a new folder called `cdcdata-exercise` inside your portfolio repository. Place the data into that folder._

_Remember that GitHub doesn’t like large files. So if you pick a large data file (>50MB), first place it somewhere outside your repository, then reduce it by e.g., writing some R code that selects only a portion of the data. Once it’s small enough, you can place it into the GitHub repository._

_While you should be able to find data for direct download from the CDC website, sometimes you need to write a bit of code to pull data from a source. This is usually done through an API. R has packages that make this relatively easy. If you ever encounter that situation, search online for instructions. Google/Stackoverflow are your friends to figure out what commands you need to write)._

### Part 1-c: Exploring the Data

_Now, write code that explores the data. Add a new Quarto document called `cdcdata-exercise.qmd` to the folder you just created._

_Start by providing a brief description of the data, where you got it, what it contains. Also add a link to the source._

This assignment uses a dataset from the United States Center for Disease Control (CDC) titled, "Nutrition, Physical Activity, and Obesity - Behavioral Risk Factor Surveillance System."  The dataset and column definition can be found [here](https://chronicdata.cdc.gov/Nutrition-Physical-Activity-and-Obesity/Nutrition-Physical-Activity-and-Obesity-Behavioral/hn4x-zwk7/about_data).

The CDC describes the dataset as follows:

> This dataset includes data on adult's diet, physical activity, and weight status from Behavioral Risk Factor Surveillance System. This data is used for DNPAO's Data, Trends, and Maps database, which provides national and state specific data on obesity, nutrition, physical activity, and breastfeeding.

:::{.callout-note}
The DNPAO is the CDC's Division of Nutrition, Physical Activity, and Obesity.
:::

_Then write code that reads/loads the data. As needed, process the data (e.g., if there are weird symbols in the data, or missing values coded as `999` or anything of that sort, write code to fix it.) If your dataset has a lot of variables, pick a few of them (at least 5)._

The dataset is described by the CDC as 93.2k rows with 33 columns each.  For this assignment, the entire dataset will be loaded.

```{r}
# Track time it takes to load
ZC.START_TIMESTAMP <- proc.time()
# load the data into a dataframe
ZC.data <- read.csv(ZC.filepath)
ZC.loadtime <- proc.time() - ZC.START_TIMESTAMP
# Report the load time
ZC.printline <- paste("The data took"
                      , ZC.loadtime[3]
                      , "seconds to load."
                      , sep=" "
)
ZC.printline
```

There were no issues encountered while loading the data.

```{r}
# Report master dataset dimensions
# Increase dimension index
ZC.dimindex <- ZC.dimindex+1
ZC.TheseDims <- dim(ZC.data)
ZC.datarows[ZC.dimindex] <- ZC.TheseDims[1]
ZC.datacols[ZC.dimindex] <- ZC.TheseDims[2]
ZC.printline <- paste("The master dataset is currently"
                        , ZC.datarows[ZC.dimindex]
                        , "rows by"
                        , ZC.datacols[ZC.dimindex]
                        , "columns."
                        , sep=" ")
ZC.printline
```

To get an overview of what we have just loaded, a table with summary information about each column is constructed. 

```{r}
# Setup control variables and lists
ZC.index <- 0
ZC.colname <- list()
ZC.coltype <- list()
ZC.NAvals <- list()
ZC.unique <- list()
ZC.example <- list()
# Loop through the columns to populate the lists
for (ZC.ThisColumn in colnames(ZC.data))
{
  #Increment the index for the lists
  ZC.index <- ZC.index+1 
  # Assign the column name
  ZC.colname[ZC.index] <- ZC.ThisColumn 
  # Assign the column type
  ZC.coltype[ZC.index] <- typeof(ZC.data[[ZC.ThisColumn]]) 
  # Count the number of NAs in the column
  ZC.NAvals[ZC.index] <- sum(is.na(ZC.data[[ZC.ThisColumn]])) 
  # Count the numner of unique values in the column
  ZC.TheseValues <- table(ZC.data[[ZC.ThisColumn]])
  ZC.unique[ZC.index] <- length(ZC.TheseValues)
  # Find a suitable example value
  # Prepare to loop through all the values to find an acceptable example
  for (ZC.ThisRow in 1:nrow(ZC.data)) 
  {
    # Start this FOR loop but prepare to break out of it
    ZC.BreakFlag = 0
    # As soon as an acceptable value is found, set the break flag to 1
    # Retrieve the value in ThisRow
    ZC.ThisExp <- ZC.data[[ZC.ThisColumn]][ZC.ThisRow]
    # Continue with this example only if it is not NA
    if (!is.na(ZC.ThisExp))
    {
      # Subsequent processing depends on data type of current example
      # Is the current example a character string?
      if (typeof(ZC.ThisExp) == "character")
      {
        # Trim any leading and trailing spaces from the current example
        # If there is nothing but spaces, only an empty string will survive
        ZC.ThisExp <- trimws(ZC.ThisExp)
        # Continue with this example only if it is not empty
        if (nchar(ZC.ThisExp)>0)
        {
          # We have an acceptable example
          ZC.BreakFlag <- 1
          # Let's cut it short if the example is too long
          if (nchar(ZC.ThisExp)>8)
          {
            ZC.ThisExp <- paste(substr(ZC.ThisExp, 1, 5), "...", sep="")
          }
        }
      } else if (typeof(ZC.ThisExp) == "integer" | typeof(ZC.ThisExp) == "double")
      {
        # We would prefer a nonzero value, and we will loop through all the values if necessary to find one
        if (!ZC.ThisExp==0) ZC.BreakFlag <- 1
      } else
      { # The value could be boolean, complex, or something else
        # We will take it
        ZC.BreakFlag <- 1
      }
    }
    # If we have an acceptable value, we can stop looping through the rows
    if (ZC.BreakFlag == 1) break
  }
  ZC.example[ZC.index] <- ZC.ThisExp
}
# Bind the lists together in a single object
ZC.datasum01 <- cbind(ZC.colname, ZC.coltype, ZC.NAvals, ZC.unique, ZC.example)
# Apply more friendly column names
colnames(ZC.datasum01) <- c("Column"
                          , "Data Type"
                          , "NA Values"
                          , "Uniques"
                          , "Example"
)
ZC.datasum01
```

The above output stipulates that the values in **Data_Value_Unit** are worthless, confirmed by two (2) distinct findings: 

+ all 93,239 values are "NA"
+ zero (0) unique values

Additionally, there is only one unique value in the following columns:

+ **Data_Value_Type** 
+ **DataValueTypeID** 

Therefore, all three (3) of these columns will be dropped from the dataframe.

```{r}
ZC.data <- subset(ZC.data, select = -c(Data_Value_Unit, Data_Value_Type, DataValueTypeID))
# Report master dataset dimensions
# Increase dimension index
ZC.dimindex <- ZC.dimindex+1
ZC.TheseDims <- dim(ZC.data)
ZC.datarows[ZC.dimindex] <- ZC.TheseDims[1]
ZC.datacols[ZC.dimindex] <- ZC.TheseDims[2]
ZC.printline <- paste("The master dataset is currently"
                        , ZC.datarows[ZC.dimindex]
                        , "rows by"
                        , ZC.datacols[ZC.dimindex]
                        , "columns."
                        , sep=" ")
ZC.printline
```

There are also some column with unique counts so low they appear to offer little or no value.  They will be dropped if they are so judged to be unvalued.  The columns to be examined are:

+ **Datasource**
+ **Data_Value_Footnote_Symbol**
+ **Data_Value_Footnote**
+ **Total**

For each of these columns, the distinct values will be displayed for evaluation.

```{r}
# create list of columns to be examined
ZC.examinedcolumns[1] <- "Datasource"
ZC.examinedcolumns[2] <- "Data_Value_Footnote_Symbol"
ZC.examinedcolumns[3] <- "Data_Value_Footnote"
ZC.examinedcolumns[4] <- "Total"
# loop through the columns to be examined, building and displaying a temporary table of the unique values and their counts.
for (ZC.ExamineThis in ZC.examinedcolumns)
{
  # create section separator
  ZC.printline <- "********************************"
  print (ZC.printline)
  # build SQL statement syntax
  ZC.RunThis <- paste("select "
                      , ZC.ExamineThis
                      ,", count(*) as Occurs from 'ZC.data' group by "
                      , ZC.ExamineThis
                      , sep = "")
  # display SQL statement for posterity
  ZC.printline <- paste("Running: "
                        , ZC.RunThis
                        , sep = "")
  print(ZC.printline)
  # load results into temporary table
  ZC.ThisTable <- sqldf(ZC.RunThis)
  # display contents of temporary table
  ZC.printline <- paste("Unique Values of "
                        , ZC.ExamineThis
                        , ":"
                        , sep=""
                        )
  print(ZC.printline)
  # display results of temporary table
  print(ZC.ThisTable)
}
```

Based on the above results, **Datasource** will be dropped immediately, as the unique values seem synonyms for each other, so there is really only one value in the column.  The other three (3) columns require additional consideration.

```{r}
ZC.data <- subset(ZC.data, select = -c(Datasource))
# Report master dataset dimensions
# Increase dimension index
ZC.dimindex <- ZC.dimindex+1
ZC.TheseDims <- dim(ZC.data)
ZC.datarows[ZC.dimindex] <- ZC.TheseDims[1]
ZC.datacols[ZC.dimindex] <- ZC.TheseDims[2]
ZC.printline <- paste("The master dataset is currently"
                        , ZC.datarows[ZC.dimindex]
                        , "rows by"
                        , ZC.datacols[ZC.dimindex]
                        , "columns."
                        , sep=" ")
ZC.printline
```

The **Total** column unique values don't seem to tell much.  The CDC describes the data in this column as:

> Total/Overall breakout category

Given the vagueness of the description and the unique values, the **Total** column will also be dropped.

```{r}
ZC.data <- subset(ZC.data, select = -c(Total))
# Report master dataset dimensions
# Increase dimension index
ZC.dimindex <- ZC.dimindex+1
ZC.TheseDims <- dim(ZC.data)
ZC.datarows[ZC.dimindex] <- ZC.TheseDims[1]
ZC.datacols[ZC.dimindex] <- ZC.TheseDims[2]
ZC.printline <- paste("The master dataset is currently"
                        , ZC.datarows[ZC.dimindex]
                        , "rows by"
                        , ZC.datacols[ZC.dimindex]
                        , "columns."
                        , sep=" ")
ZC.printline
```

The **GeoLocation** column appears to have highly-coded data.  The CDC describes this column as:

> Latitude & Longitude to be provided for formatting GeoLocation or Geocode in the format (latitude, longitude)

Therefore, this column will also be dropped.

```{r}
ZC.data <- subset(ZC.data, select = -c(GeoLocation))
# Report master dataset dimensions
# Increase dimension index
ZC.dimindex <- ZC.dimindex+1
ZC.TheseDims <- dim(ZC.data)
ZC.datarows[ZC.dimindex] <- ZC.TheseDims[1]
ZC.datacols[ZC.dimindex] <- ZC.TheseDims[2]
ZC.printline <- paste("The master dataset is currently"
                        , ZC.datarows[ZC.dimindex]
                        , "rows by"
                        , ZC.datacols[ZC.dimindex]
                        , "columns."
                        , sep=" ")
ZC.printline
```

The count of the only nonblank value in both the **Data_Value_Footnote_Symbol** and **Data_Value_Footnote** have been seen before: coincidence or not, there are the exact name number of NA values in the **Data_Value** and **Data_Value_Alt** columns.  If it turns out there is a meaningful value in **Data_Value_Footnote_Symbol** and **Data_Value_Footnote** for each NA in **Data_Value** and **Data_Value_Alt**, then the **Data_Value_Footnote_Symbol** and **Data_Value_Footnote** columns can be considered duplicative information for NA values in the **Data_Value** and **Data_Value_Alt** columns.

To examine, we will build lists of the rows where the following conditions occur, and then compare them to see if they are equal:

+ Where **Data_Value_Footnote_Symbol** is not blank
+ Where **Data_Value_Footnote** is not blank
+ Where **Data_Value** is NA
+ Where **Data_Value_Alt** is NA

```{r}
# create lists to store row numbers where specified conditions occur
# store rows where Data_Value_Footnote_Symbol is not blank
ZC.DVFS <- list() 
# store rows where Data_Value_Footnote is not blank
ZC.DVF <- list() 
# store rows where Data_Value is NA
ZC.DV <- list() 
# store rows where Data_Value_Alt is NA
ZC.DVA <- list() 
# create index variable for each list
ZC.DVFSIndex <- 0
ZC.DVFIndex <- 0
ZC.DVIndex <- 0
ZC.DVAIndex <- 0
# loop through the rows in the master dataset and store indices where conditions are met
ZC.RowRange <- 1:nrow(ZC.data)
for (ZC.CheckRow in ZC.RowRange)
{
  # pull Data_Value_Footnote_Symbol value in current row
  ZC.ThisValue <- ZC.data$Data_Value_Footnote_Symbol[ZC.CheckRow]
  if (!(is.null(ZC.ThisValue)))
  {
    # Value is not NULL
    # But is it empty or blank?
    ZC.ThisValue <- trimws(ZC.ThisValue)
    if (nchar(ZC.ThisValue)>0)
    {
      # value is not blank, so store the row number
      ZC.DVFSIndex <- ZC.DVFSIndex+1
      ZC.DVFS[ZC.DVFSIndex] <- ZC.CheckRow
    }
  }
  # pull Data_Value_Footnote value in current row
  ZC.ThisValue <- ZC.data$Data_Value_Footnote[ZC.CheckRow]
  if (!(is.null(ZC.ThisValue)))
  {
    # Value is not NULL
    # But is it empty or blank?
    ZC.ThisValue <- trimws(ZC.ThisValue)
    if (nchar(ZC.ThisValue)>0)
    {
      # value is not blank, so store the row number
      ZC.DVFIndex <- ZC.DVFIndex+1
      ZC.DVF[ZC.DVFIndex] <- ZC.CheckRow
    }
  }
  # is Data_Value an NA value?
  if (is.na(ZC.data$Data_Value[ZC.CheckRow]))
  {
      # value is NA, so store the row number
      ZC.DVIndex <- ZC.DVIndex+1
      ZC.DV[ZC.DVIndex] <- ZC.CheckRow
  }
  # is Data_Value_Alt an NA value?
  if (is.na(ZC.data$Data_Value_Alt[ZC.CheckRow]))
  {
      # value is NA, so store the row number
      ZC.DVAIndex <- ZC.DVAIndex+1
      ZC.DVA[ZC.DVAIndex] <- ZC.CheckRow
  }
}
```

Now that the lists of the row numbers where the conditions occur have been built, they need to be compared.  They only need to be compared if they are all the same length, so that will be checked first. 

```{r}
# Compare the list of row numbers where conditions were met
# Are all the lists of the same length?  No need to compare if not.
# Create an indicator flag to indicate if the row numbers match.
# They can't match if the count of row numbers does not match, so assume it is FALSE
ZC.CompEquals <- FALSE
if (ZC.DVFSIndex==ZC.DVFIndex&ZC.DVFIndex==ZC.DVIndex&ZC.DVIndex==ZC.DVAIndex)
{
  # The lists are the same length.  A loop will be used to compare value by value.
  # No need to continue the loop as soon as a difference if found
  # Set the indicator to TRUE; change to FALSE if an unequal row number is found
  ZC.CompEquals <- TRUE
  # The row numbers in the list are actually embedded inside lists of one (1) item. 
  ZC.CompRange <- 1:ZC.DVIndex
  for (ZC.CompRow in ZC.CompRange)
  {
    # The row numbers in the list are actually embedded inside lists of one (1) item. 
    # We'll break each value out of it's list before we compare.
    ZC.ThisDVFS <- unlist(ZC.DVFS[ZC.CompRow])
    ZC.ThisDVF <- unlist(ZC.DVF[ZC.CompRow])
    ZC.ThisDV <- unlist(ZC.DV[ZC.CompRow])
    ZC.ThisDVA <- unlist(ZC.DVA[ZC.CompRow])
    if(!(ZC.ThisDVFS==ZC.ThisDVF&ZC.ThisDVF==ZC.ThisDV&ZC.ThisDV==ZC.ThisDVA))
    {
      # We found a row number that did not match.  Set indicator value to FALSE and stop
      ZC.CompEquals <- FALSE
      break
    }
  }
}
ZC.ThisDVFS # TDL
typeof(ZC.ThisDVFS) #TDL
ZC.CompRow # TDL
typeof(ZC.CompRow) #TDL
ZC.DVFS[ZC.CompRow] # TDL
typeof(ZC.DVFS[ZC.CompRow]) # TDL
ZC.DVF[ZC.CompRow] #TDL
typeof(ZC.DVF[ZC.CompRow]) # TDL
# Did the indicator flag survive every row number comparison?
if (ZC.CompEquals)
{
  print("Row numbers where conditions are met match.")
} else
{
  print("Row numbers where conditions are met do not match.")
}
```

The lists of row numbers where the conditions were met match exactly.  Therefore, the data in columns **Data_Value_Footnote_Symbol** and **Data_Value_Footnote** can be considered redundant for cases when the values in **Data_Value** and **Data_Value_Alt** are NA.  Having already established that the data in columns **Data_Value_Footnote_Symbol** and **Data_Value_Footnote** is blank when the values in **Data_Value** and **Data_Value_Alt** are not NA, we can remove the columns with blank/redundant data.

```{r}
ZC.data <- subset(ZC.data, select = -c(Data_Value_Footnote_Symbol, Data_Value_Footnote))
# Report master dataset dimensions
# Increase dimension index
ZC.dimindex <- ZC.dimindex+1
ZC.TheseDims <- dim(ZC.data)
ZC.datarows[ZC.dimindex] <- ZC.TheseDims[1]
ZC.datacols[ZC.dimindex] <- ZC.TheseDims[2]
ZC.printline <- paste("The master dataset is currently"
                        , ZC.datarows[ZC.dimindex]
                        , "rows by"
                        , ZC.datacols[ZC.dimindex]
                        , "columns."
                        , sep=" ")
ZC.printline
```

Next, the count of unique values, coupled with the column names, suggests a one-to-one relationship between the data in one column and the data in another.  In proven true, we can reduce repetitiveness by archiving off the duplicated data into a separate lookup table.  Column combinations to consider are:

+ **LocationID**, a potential lookup for both **LocationAbbr** and **LocationDesc**
+ **StratificationCategoryId1**, a potential lookup for **StratificationCategory1**
+ **StratificationID1**, a potential lookup for **Stratification1**

The validity of an index/lookup relationship between the above potentials will be explored, beginning with **LocationID**.

A SQL statement will be utilized to count the number of times each combination of the **LocationID**, **LocationAbbr**, and **LocationDesc** occurs in the data.  Since there are 55 unique values in each of these columns, if there are a total of 55 combinations, then a one-to-one(-to-one) correlation is established.

```{r}
sqldf("select LocationID, LocationAbbr, LocationDesc, count(*) as Occurs from 'ZC.data' group by LocationID, LocationAbbr, LocationDesc")
```

There are only 55 total unique combinations for these three (3) variables, so a lookup table for **LocationAbbr** and another for **LocationDesc** will be created, both with **LocationID** as the lookup value.  The lookup value and the first lookup table will be stored as a relationship.  Afterwards, the columns containing the lookup values will be dropped.

```{r}
# Creating and displaying top of lookup tables
ZC.lookupLocationAbbr <- sqldf("select distinct LocationID, LocationAbbr from 'ZC.data' order by LocationID")
head(ZC.lookupLocationAbbr)
ZC.lookupLocationDesc <- sqldf("select distinct LocationID, LocationDesc from 'ZC.data' order by LocationID")
head(ZC.lookupLocationDesc)
# Storing lookup columns and lookup table names
ZC.lookupindex <- ZC.lookupindex+1
ZC.lookupcolumn[ZC.lookupindex] <- "LocationID"
ZC.lookuptable[ZC.lookupindex] <- "LocationAbbr"
ZC.data <- subset(ZC.data, select = -c(LocationAbbr, LocationDesc))
# Report master dataset dimensions
# Increase dimension index
ZC.dimindex <- ZC.dimindex+1
ZC.TheseDims <- dim(ZC.data)
ZC.datarows[ZC.dimindex] <- ZC.TheseDims[1]
ZC.datacols[ZC.dimindex] <- ZC.TheseDims[2]
ZC.printline <- paste("The master dataset is currently"
                        , ZC.datarows[ZC.dimindex]
                        , "rows by"
                        , ZC.datacols[ZC.dimindex]
                        , "columns."
                        , sep=" ")
ZC.printline
```

A SQL statement will be utilized to count the number of times each combination of the **StratificationCategoryId1** and **StratificationCategory1** occurs in the data.  Since there are seven (7) unique values in each of these columns, if there are a total of seven (7) combinations, then a one-to-one correlation is established.

```{r}
sqldf("select StratificationCategoryId1, StratificationCategory1, count(*) as Occurs from 'ZC.data' group by StratificationCategoryId1, StratificationCategory1")
```

There are only seven (7) total unique combinations for these two (2) variables, so a lookup table for **StratificationCategory1** will be created with **StratificationCategoryId1** as the lookup value.  The lookup value and the lookup table will be stored as a relationship.  Afterwards, the columns containing the lookup values will be dropped.

```{r}
# Creating and displaying top of lookup tables
ZC.StratificationCategory1 <- sqldf("select distinct StratificationCategoryId1, StratificationCategory1 from 'ZC.data' order by StratificationCategoryId1")
head(ZC.StratificationCategory1)
# Storing lookup columns and lookup table names
ZC.lookupindex <- ZC.lookupindex+1
ZC.lookupcolumn[ZC.lookupindex] <- "StratificationCategoryId1"
ZC.lookuptable[ZC.lookupindex] <- "StratificationCategory1"
ZC.data <- subset(ZC.data, select = -c(StratificationCategory1))
# Report master dataset dimensions
# Increase dimension index
ZC.dimindex <- ZC.dimindex+1
ZC.TheseDims <- dim(ZC.data)
ZC.datarows[ZC.dimindex] <- ZC.TheseDims[1]
ZC.datacols[ZC.dimindex] <- ZC.TheseDims[2]
ZC.printline <- paste("The master dataset is currently"
                        , ZC.datarows[ZC.dimindex]
                        , "rows by"
                        , ZC.datacols[ZC.dimindex]
                        , "columns."
                        , sep=" ")
ZC.printline
```
A SQL statement will be utilized to count the number of times each combination of the **StratificationID1** and **Stratification1** occurs in the data.  Since there are 29 unique values in each of these columns, if there are a total of 29 combinations, then a one-to-one correlation is established.

```{r}
sqldf("select StratificationID1, Stratification1, count(*) as Occurs from 'ZC.data' group by StratificationID1, Stratification1")
```

There are only 29 total unique combinations for these two (2) variables, so a lookup table for **Stratification1** will be created with **StratificationID1** as the lookup value.  The lookup value and the lookup table will be stored as a relationship.  Afterwards, the columns containing the lookup values will be dropped.

```{r}
# Creating and displaying top of lookup tables
ZC.Stratification1 <- sqldf("select distinct StratificationID1, Stratification1 from 'ZC.data' order by StratificationID1")
head(ZC.Stratification1)
# Storing lookup columns and lookup table names
ZC.lookupindex <- ZC.lookupindex+1
ZC.lookupcolumn[ZC.lookupindex] <- "StratificationID1"
ZC.lookuptable[ZC.lookupindex] <- "Stratification1"
ZC.data <- subset(ZC.data, select = -c(Stratification1))
# Report master dataset dimensions
# Increase dimension index
ZC.dimindex <- ZC.dimindex+1
ZC.TheseDims <- dim(ZC.data)
ZC.datarows[ZC.dimindex] <- ZC.TheseDims[1]
ZC.datacols[ZC.dimindex] <- ZC.TheseDims[2]
ZC.printline <- paste("The master dataset is currently"
                        , ZC.datarows[ZC.dimindex]
                        , "rows by"
                        , ZC.datacols[ZC.dimindex]
                        , "columns."
                        , sep=" ")
ZC.printline
```

Similarly, it's possible **YearStart** always equals **YearEnd**, **Class** always equals **Topic**, and **Data_Value** always equals **Data_Value_Alt**.  Code will be executed to see if these possible pairs always duplicate a value in the other; if so, the duplicate column will be dropped.

Since there are NA values in both the **Data_Value** and **Data_Value_Alt**, and R code will halt if it tries to compare an NA with anything (even another NA value), this comparison will be conditional.  For the purposes of this exercise:

+ if both values are NA, the values are considered equal (but a comparison cannot be attempted by R).
+ if one value is NA but the other is not, the values are considered not equal.
+ if neither value is NA, a comparison will be performed to determine if the values are equal or not.

```{r}
# Set up variables to count differences
ZC.YearDeltas <- 0
ZC.ClassDeltas <- 0
ZC.DataDeltas <- 0
for (ZC.ThisRow in 1:nrow(ZC.data))
{
  # Count if YearStart and YearEnd are different values
  if (!(ZC.data$YearStart[ZC.ThisRow]==ZC.data$YearEnd[ZC.ThisRow]))
    {
      # Brackets not necessary; just respecting page margins
      ZC.YearDeltas <- ZC.YearDeltas+1
    }
  # Count if Class and Topic are different values
  if(!(ZC.data$Class[ZC.ThisRow]==ZC.data$Topic[ZC.ThisRow]))
    {
      # Brackets not necessary; just respecting page margins
      ZC.ClassDeltas <- ZC.ClassDeltas+1
    }
  # There could be NA values for either Data_Value and/or Data_Value_Alt
  # If both are NA, there is no difference
  # If only one is NA, there is a difference
  # Only compare if neither are NA
  if (is.na(ZC.data$Data_Value[ZC.ThisRow])==is.na(ZC.data$Data_Value_Alt[ZC.ThisRow]))
  {
    # Either both are NA or neither or NA
    # Comparison is not necessary (or valid!) if both are NA
    if (!(is.na(ZC.data$Data_Value[ZC.ThisRow])))
    {
      # Neither are NA - compare values
      if (!(ZC.data$Data_Value[ZC.ThisRow]==ZC.data$Data_Value_Alt[ZC.ThisRow]))
      {
        # Values are not equal
        ZC.DataDeltas <- ZC.DataDeltas+1
      }
    }
  }
  else
  {
    # One value is NA but not the other, so they are automatically not equal
    ZC.DataDeltas <- ZC.DataDeltas+1
  }
}
# Report on deltas
ZC.printline <- paste("There were"
                      , ZC.YearDeltas
                      , "row(s) with different values in YearStart and YearEnd."
                      , sep = " ")
ZC.printline
ZC.printline <- paste("There were"
                      , ZC.ClassDeltas
                      , "row(s) with different values in Class and Topic."
                      , sep = " ")
ZC.printline
ZC.printline <- paste("There were"
                      , ZC.DataDeltas
                      , "row(s) with different values in Data_Value and Data_Value_Alt."
                      , sep = " ")
ZC.printline
# Drop YearEnd if it always equals YearStart
if (ZC.YearDeltas==0) 
  {
    print("Dropping column YearEnd")
    ZC.data <- subset(ZC.data, select = -c(YearEnd))
  }
# Drop Topic if it always equals Class
if (ZC.ClassDeltas==0) 
  {
    print("Dropping column Topic")
    ZC.data <- subset(ZC.data, select = -c(Topic))
  }
# Drop Data_Value_Alt if it always equals Data_Value
if (ZC.DataDeltas==0)
  {
    print("Dropping column Data_Value_Alt")
    ZC.data <- subset(ZC.data, select = -c(Data_Value_Alt))
  }
# Report master dataset dimensions
# Increase dimension index
ZC.dimindex <- ZC.dimindex+1
ZC.TheseDims <- dim(ZC.data)
ZC.datarows[ZC.dimindex] <- ZC.TheseDims[1]
ZC.datacols[ZC.dimindex] <- ZC.TheseDims[2]
ZC.printline <- paste("The master dataset is currently"
                        , ZC.datarows[ZC.dimindex]
                        , "rows by"
                        , ZC.datacols[ZC.dimindex]
                        , "columns."
                        , sep=" ")
ZC.printline
```

Finally, some column names do not play well with SQL (when they contain unallowed characters), which will be used later to build the data for the charts.  So, there will be a few manual manipulations of some column names:

+ **Age.years.** will become **AgeYears**
+ **Race.Ethnicity** will become **RaceEthnic**

```{r}
# change column names to be more SQL-friendly
print(typeof(ZC.data)) # TDL
names(ZC.data)[names(ZC.data) == 'Age.years.'] <- 'AgeYears'
names(ZC.data)[names(ZC.data) == 'Race.Ethnicity'] <- 'RaceEthnic'
```

The summary table will be rebuilt before continuing.  This time, a new column, LU (for lookup) will be created.  If a lookup table has been created for the master dataset column, LU will be set to one (1); otherwise, it will be set to zero (0).

```{r}
# Setup control variables and lists
ZC.index <- 0
ZC.colname <- list()
ZC.coltype <- list()
ZC.NAvals <- list()
ZC.unique <- list()
ZC.lookups <- list()
ZC.example <- list()
# Loop through the columns to populate the lists
for (ZC.ThisColumn in colnames(ZC.data))
{
  #Increment the index for the lists
  ZC.index <- ZC.index+1 
  # Assign the column name
  ZC.colname[ZC.index] <- ZC.ThisColumn 
  # Assign the column type
  ZC.coltype[ZC.index] <- typeof(ZC.data[[ZC.ThisColumn]]) 
  # Count the number of NAs in the column
  ZC.NAvals[ZC.index] <- sum(is.na(ZC.data[[ZC.ThisColumn]])) 
  # Count the number of unique values in the column
  ZC.TheseValues <- table(ZC.data[[ZC.ThisColumn]])
  ZC.unique[ZC.index] <- length(ZC.TheseValues)
  # Determine if the column has an associated lookup table
  # Assume there is no lookup table
  ZC.lookups[ZC.index] <- 0
  # Search for the column in the list of columns with associated lookup tables
  # Flag it is the column is found to be associated with a lookup table
  for (ZC.ThisLookupColumn in ZC.lookupcolumn)
  {
    if (ZC.ThisLookupColumn==ZC.colname[ZC.index]) 
    {
      # Column found, flag it as having a lookup table association
      ZC.lookups[ZC.index] <- 1
    }
  }
  # Find a suitable example value
  # Prepare to loop through all the values to find an acceptable example
  for (ZC.ThisRow in 1:nrow(ZC.data)) 
  {
    # Start this FOR loop but prepare to break out of it
    ZC.BreakFlag = 0
    # As soon as an acceptable value is found, set the break flag to 1
    # Retrieve the value in ThisRow
    ZC.ThisExp <- ZC.data[[ZC.ThisColumn]][ZC.ThisRow]
    # Continue with this example only if it is not NA
    if (!is.na(ZC.ThisExp))
    {
      # Subsequent processing depends on data type of current example
      # Is the current example a character string?
      if (typeof(ZC.ThisExp) == "character")
      {
        # Trim any leading and trailing spaces from the current example
        # If there is nothing but spaces, only an empty string will survive
        ZC.ThisExp <- trimws(ZC.ThisExp)
        # Continue with this example only if it is not empty
        if (nchar(ZC.ThisExp)>0)
        {
          # We have an acceptable example
          ZC.BreakFlag <- 1
          # Let's cut it short if the example is too long
          if (nchar(ZC.ThisExp)>8)
          {
            ZC.ThisExp <- paste(substr(ZC.ThisExp, 1, 5), "...", sep="")
          }
        }
      } else if (typeof(ZC.ThisExp) == "integer" | typeof(ZC.ThisExp) == "double")
      {
        # We would prefer a nonzero value, and we will loop through all the values if necessary to find one
        if (!ZC.ThisExp==0) ZC.BreakFlag <- 1
      } else
      { # The value could be boolean, complex, or something else
        # We will take it
        ZC.BreakFlag <- 1
      }
    }
    # If we have an acceptable value, we can stop looping through the rows
    if (ZC.BreakFlag == 1) break
  }
  ZC.example[ZC.index] <- ZC.ThisExp
}
# Bind the lists together in a single object
ZC.datasum02 <- cbind(ZC.colname, ZC.coltype, ZC.NAvals, ZC.unique, ZC.lookups, ZC.example)
# Apply more friendly column names
colnames(ZC.datasum02) <- c("Column"
                          , "Data Type"
                          , "NA Values"
                          , "Uniques"
                          , "LU"
                          , "Example"
)
ZC.datasum02
print(typeof(ZC.datasum02)) # TDL
```

_Once you have the data processed and cleaned, perform some exploratory/descriptive analysis on this cleaned dataset. Make some summary tables, produce some figures. Try to summarize each variable in a way that it can be described by a distribution. For instance if you have a categorical variable, show what % are in each category. If you have a continuous variable, make a plot to see if it’s approximately normal, then try to summarize it to determine its mean and standard deviation._ 

_The idea is that your descriptive analysis will provide enough information for your classmate to make synthetic data that looks similar, along the lines discussed in the synthetic data module._

_Remember to add both text to your Quarto file and comments into your code to explain what you are doing._

Now that the dataset has been whittled down to nineteen (19) pertinent columns, graphics for each column will be generated.  Using the summary table from above, chart type will be chosen as follows:

+ All columns with ten (10) or less unique values will be represented by a pie chart.
+ All columns with more than ten (10) but with twenty (20) or less unique values will be represented by a histogram (bar chart).
+ All other columns will be represented by a line plot.  
+ Additionally, where the data is predetermined to be a continuous variable, a standard statistical summary will also be produced.  

```{r}
# Convert summary table from list to dataframe
ZC.Summary <- as.data.frame(ZC.datasum02)
# Create list of columns predetermined to contain values in a continuous variable
ZC.StatsPlease <- c("Data_Value"
                    , "Low_Confidence_Limit"
                    , "High_Confidence_Limit"
                    , "Sample_Size")
ZC.StatsPlease <- unlist(ZC.StatsPlease)
print(ZC.StatsPlease) # TDL
print(typeof(ZC.StatsPlease)) # TDL
# Loop through each of the columns
ZC.ChartColumns <- 1:nrow(ZC.Summary)
for (ZC.ChartThis in ZC.ChartColumns)
{
  # Each value loaded is embedded in a one-item list, so it has to be unlisted before using
  # Load column name
  ZC.ThisColumn <- ZC.Summary$Column[ZC.ChartThis]
  ZC.ThisColumn <- unlist(ZC.ThisColumn)
  # Load unique column count - determines chart type
  ZC.ValueCount <- ZC.Summary$Uniques[ZC.ChartThis]
  ZC.ValueCount <- unlist(ZC.ValueCount)
  # source of chart data will be the same regardless of chart type
  ZC.ThisQuery <- paste ("select "
                           , ZC.ThisColumn
                           , " as Labels, count(*) as Occurs from 'ZC.data' "
                           , "group by "
                           , ZC.ThisColumn
                           , " order by "
                           , ZC.ThisColumn
                           , sep="")
    # Run the Query
    ZC.DataTable <- sqldf(ZC.ThisQuery)
    ZC.DataTable <- as.data.frame(ZC.DataTable)
    ZC.printline <- paste("Data for "
                          , ZC.ThisColumn
                          , ":"
                          , sep="") # TDL
    print(ZC.printline) # TDL
    if(!(ZC.ValueCount>20))
      print(ZC.DataTable) # TDL
    else
    {
      ZC.printline <- paste("Data suppressed.  "
                            , ZC.ValueCount
                            , " rows."
                            , sep=""
      )
      print(ZC.printline)
    }
  if (!(ZC.ValueCount>10))
  {
    # Drawing a pie chart
  } else if (!(ZC.ValueCount>20))
  {
    # building a histogram
  } else
  {
    # Drawing a distribution plot
  }
  # printing summary statistics?
    # if (grepl(ZC.ThisColumn, ZC.StatsPlease))
    # {
      # print(summary(ZC.data[[ZC.ThisColumn]]))
    # }
  print("******") # TDL
}
```

_In a final step, update the `_quarto.yml` file and include a menu item for “Data Analysis Exercise” pointing to the new file. Follow the format of the existing entries. Remember to be very careful about the right amount of empty space. Re-create your website and make sure it all works and the new project shows up on the website._

```{r}
# No code changes were made here for this part of the assignment.
```

Done.  

### Part 1-d: A new GitHub Workflow

_If everything works as expected, commit and push your changes to GitHub. Instead of using the fork + pull-request workflow we’ve tried a few times, we’ll explore a different collaborative approach. In this approach, you and your collaborator work on the same repository. To that end, you need to add your classmate as collaborator. Go to Github.com, find your portfolio repository, go to `Settings`, then `Collaborators`. Choose `Add Collaborator` and add your classmate. Your classmate should receive an invitation, which they need to accept. With this, they are now able to directly push and pull to your repository, without them needing to create a fork. (You can remove them after this exercise if you don’t want them to be able to continue having write access to your repository)._

```{r}
# No code changes were made here for this part of the assignment.
```

Done.  

## Part 2

```{r}
# START - Space Reserved for Collaborator
```



```{r}
# END - Space Reserved for Collaborator
```

## END of Assignment #5

```{r}
# Assignment 5 - END
# Go Roadrunners!
```
