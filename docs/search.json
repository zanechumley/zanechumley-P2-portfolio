[
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Zane Chumley Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Zane’s Starter Analysis Exercise",
    "section": "",
    "text": "VINAYAK REVISANKER contributed to this exercise\nThe structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Zane’s Starter Analysis Exercise",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Zane’s Starter Analysis Exercise",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section.\nI added two (2) columns to exampledata.xslx to form exampledata2.xslx. These columns were “BMI” (body mass index) and “Race” (from the list of races used in the 2020 US Census)."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Zane’s Starter Analysis Exercise",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Zane’s Starter Analysis Exercise",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Zane’s Starter Analysis Exercise",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Zane’s Starter Analysis Exercise",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Zane’s Starter Analysis Exercise",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nRace\n0\n1\n4\n16\n0\n6\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nF: 5, M: 4, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n156.00000\n34.899857\n60.0\n155.00\n166.0\n176.50\n183.0\n▁▁▁▂▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n62.81818\n28.361305\n0.0\n52.50\n60.0\n78.00\n110.0\n▂▁▇▅▃\n\n\nnumeric\nBMI\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n24.79091\n7.350708\n15.9\n20.55\n23.4\n25.05\n39.9\n▃▇▁▁▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Zane’s Starter Analysis Exercise",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender.\n\n\n\n\n\n(bmi-dist?) shows a bar graph of Body Mass Index (BMI) distribution.\n\n\n\n\n\nBody Mass Index (BMI) distribution.\n\n\n\n\n(mean-height-race?) shows a bar graph of the average height by race.\n\n\n\n\n\nAverage Height by Race.\n\n\n\n\n(weight-bmi?) shows a scatterplot of weight and BMI.\n\n\n\n\n\nDistribution of Weight and BMI."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Zane’s Starter Analysis Exercise",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871\n\n\n\n\n\n\n\n\nTable 3 shows a linear model with Height as the outcome and ShoeSize and EyeColor as the predictors.\n\n\n\n\nTable 3: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Zane’s Starter Analysis Exercise",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Zane’s Starter Analysis Exercise",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Zane’s Starter Analysis Exercise",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Zane Chumley Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Zane Chumley Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at C:/Users/Zane/Documents/UTSA-MSDA/Coursework/2024-02 Summer/DA 6833 02T - Thursday 6pm/P2-Portfolio Clone/zanechumley-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`        \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                   \n1 Height          height in centimeters                 numeric value &gt;0 or NA  \n2 Weight          weight in kilograms                   numeric value &gt;0 or NA  \n3 Gender          identified gender (male/female/other) M/F/O/NA                \n4 BMI             Body Mass Index                       numerical value &gt;0 or NA\n5 Race            Race as defined by 2020 US Census     American Indian, Asian,…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height &lt;dbl&gt; 180, 175, 60, 178, 192, 6, 156, 166, 155, 145, 165, 133, 166, 1…\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, 0, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n$ BMI    &lt;dbl&gt; 24.7, 22.9, 23.4, 24.0, 24.4, 15.9, 37.0, 39.9, 22.5, 33.3, 18.…\n$ Race   &lt;chr&gt; \"White\", \"White\", \"Black\", \"Asian\", \"Arab\", \"Arab\", \"Asian\", \"P…\n\nsummary(rawdata)\n\n     Height          Weight           Gender               BMI       \n Min.   :  6.0   Min.   :   0.00   Length:14          Min.   :15.90  \n 1st Qu.:147.2   1st Qu.:  54.25   Class :character   1st Qu.:21.45  \n Median :160.5   Median :  65.00   Mode  :character   Median :23.70  \n Mean   :145.1   Mean   : 559.64                      Mean   :25.21  \n 3rd Qu.:172.8   3rd Qu.:  87.50                      3rd Qu.:25.23  \n Max.   :192.0   Max.   :7000.00                      Max.   :39.90  \n     Race          \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender   BMI Race \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;\n1    180     80 M       24.7 White\n2    175     70 O       22.9 White\n3     60     60 F       23.4 Black\n4    178     76 F       24   Asian\n5    192     90 NA      24.4 Arab \n6      6     55 F       15.9 Arab \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nRace\n0\n1\n4\n16\n0\n7\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n145.07\n50.91\n6.0\n147.25\n160.5\n172.75\n192.0\n▁▁▁▂▇\n\n\nWeight\n0\n1\n559.64\n1853.85\n0.0\n54.25\n65.0\n87.50\n7000.0\n▇▁▁▁▁\n\n\nBMI\n0\n1\n25.21\n6.88\n15.9\n21.45\n23.7\n25.22\n39.9\n▃▇▁▁▂\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nRace\n0\n1\n4\n16\n0\n7\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n145.07\n50.91\n6.0\n147.25\n160.5\n172.75\n192.0\n▁▁▁▂▇\n\n\nWeight\n0\n1\n559.64\n1853.85\n0.0\n54.25\n65.0\n87.50\n7000.0\n▇▁▁▁▁\n\n\nBMI\n0\n1\n25.21\n6.88\n15.9\n21.45\n23.7\n25.22\n39.9\n▃▇▁▁▂\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nRace\n0\n1\n4\n16\n0\n7\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n157.71\n32.29\n60.0\n154.25\n165.5\n177.25\n192.0\n▁▁▁▆▇\n\n\nWeight\n0\n1\n559.64\n1853.85\n0.0\n54.25\n65.0\n87.50\n7000.0\n▇▁▁▁▁\n\n\nBMI\n0\n1\n25.21\n6.88\n15.9\n21.45\n23.7\n25.22\n39.9\n▃▇▁▁▂\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nRace\n0\n1\n4\n16\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n158.69\n33.40\n60.0\n155.0\n166.0\n178.0\n192.0\n▁▁▁▅▇\n\n\nWeight\n0\n1\n64.23\n27.13\n0.0\n54.0\n60.0\n80.0\n110.0\n▁▁▇▃▃\n\n\nBMI\n0\n1\n24.58\n6.74\n15.9\n21.1\n23.4\n24.7\n39.9\n▃▇▁▁▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nRace\n0\n1\n4\n16\n0\n6\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nF: 5, M: 4, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n158.69\n33.40\n60.0\n155.0\n166.0\n178.0\n192.0\n▁▁▁▅▇\n\n\nWeight\n0\n1\n64.23\n27.13\n0.0\n54.0\n60.0\n80.0\n110.0\n▁▁▇▃▃\n\n\nBMI\n0\n1\n24.58\n6.74\n15.9\n21.1\n23.4\n24.7\n39.9\n▃▇▁▁▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nRace\n0\n1\n4\n16\n0\n6\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nF: 5, M: 4, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n156.00\n34.90\n60.0\n155.00\n166.0\n176.50\n183.0\n▁▁▁▂▇\n\n\nWeight\n0\n1\n62.82\n28.36\n0.0\n52.50\n60.0\n78.00\n110.0\n▂▁▇▅▃\n\n\nBMI\n0\n1\n24.79\n7.35\n15.9\n20.55\n23.4\n25.05\n39.9\n▃▇▁▁▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Zane Chumley Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Zane Chumley Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zane Chumley’s website and data analysis portfolio",
    "section": "",
    "text": "Welcome to my website and data analysis portfolio.\n\nThank you for your interest.\nPlease use the Menu Bar above to look around.\nThis is an evolving website, so feel free to check back often!"
  },
  {
    "objectID": "index.html#hello-fellow-roadrunners",
    "href": "index.html#hello-fellow-roadrunners",
    "title": "Zane Chumley’s website and data analysis portfolio",
    "section": "",
    "text": "Welcome to my website and data analysis portfolio.\n\nThank you for your interest.\nPlease use the Menu Bar above to look around.\nThis is an evolving website, so feel free to check back often!"
  },
  {
    "objectID": "index.html#origin",
    "href": "index.html#origin",
    "title": "Zane Chumley’s website and data analysis portfolio",
    "section": "Origin",
    "text": "Origin\n\nBirthplace:\n\nTexas City, Texas\n\n\n\nGrew up in:\n\nClear Lake City (Houston neighborhood)\nTaylor Lake Village (Houston Metro area)\n\n\n\nAdditional Residences:\n\nDenton, Texas\nAbilene, Texas\nFriendswood, Texas\nNashville, Tennessee\nMechanicsburg, Pennsylvania\nDallas, Texas\nGarland, Texas\nSan Antonio, Texas (current)"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Portrait\n\n\n\nZane Chumley\n\n\n\n\nProfessional Biography:\nThirty-plus (30+) years business professional performing Supervisor-, Manager-, Director-, and Executive-level experience with keen focuses on optimizing internal quality and attaining peak operational efficiency tailored to current and emerging enterprise scale factors.\n\n\nCareer Highlights:\n\nGeneral Manager, The Adept Group\nDirector of Operations, The Tracking Corporation\nLead Quality Analyst, Gallagher Wipro Solutions\nLead Requirements Analyst, Unisys Space Systems\n\n\n\nProgramming Languages:\n\nSQL (MSSQL, Oracle, and PROC SQL dialects)\nPython (and Anaconda platform)\nR (and RStudio platform)\nXML\nVisual Basic\nHTML\nPascal\nCOBOL\n\n\n\nData Analysis Highlights:\n\nConsultant, Counsel for the Defense, Belcher, et. al., vs. Shoney’s, Inc. (“Belcher I”) ; Belcher, et. al., vs. Shoney’s Inc. (“Belcher II”); Griffin, et. al., vs. Shoney’s\nAnalyst, SIM Card Data Usage\nAnalyst, Call Center Transactional Costs\n\n\n\nFormal Education:\n\nBachelor of Arts, Abilene Christian University\nMasters in Business Administration, University of Texas at San Antonio\nMasters of Science in Data Analytics, University of Texas at San Antonio (Candidate)\n\n\n\nResearch Interests:\n\nEconomics\nVolunteerism\nSustainability\nPhilanthropy\n\n\n\nFun Facts:\n\n(Slightly) older than UTSA\nOn assignment at KSC for Space Shuttle Columbia launch (mission STS-55)\nBackup Singer (uncredited), R.I.O.T. (Righteous Invasion of Truth), Carman, Sparrow Records\nKeyboardist, Ally Brooke (Fifth Harmony) On My Knees (Yes, I’m the Long Blond-Haired Muppet-like creature in the background)\nOnce was enough: Skydive\n\n\n\nObligatory Link:\nStock Market Data\n\n\n\nFollowing receipt of my MSDA degree, I intend to launch a conglomerate, the first operational unit of which will be an Investor’s Club. Mining historical stock market data will be a cornerstone activity contributing to the success of the Club."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "# DA 6833 02T\n# Summer 2024\n# School of Data Science\n# University of Texas at San Antonio\n\n# Zane Chumley\n# Banner ID: @01318598\n# UTSAid: wgs999\n\n\n\n\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at C:/Users/Zane/Documents/UTSA-MSDA/Coursework/2024-02 Summer/DA 6833 02T - Thursday 6pm/P2-Portfolio Clone/zanechumley-P2-portfolio\n\nlibrary(dslabs) #for data used in this assignment\n\n\n\n\n\n# review help file for gapminder dataset\n# after the 20th time, opening this file just got excessively and needlessly annoying\n# help(gapminder)\n\n# assign dataset to internal variable\nA03 &lt;- gapminder \n\n# look at the data's structure\nstr(A03)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n# look at a summary of the data\nsummary(A03)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n# look at the data's type\nclass(A03)\n\n[1] \"data.frame\"\n\n\n\n\n\n\n\n\n\n# create new data.frame with African data only\nA03.africadata &lt;- A03[A03$continent==\"Africa\",]\n\n# look at the subset's structure\nstr(A03.africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n# look at a summary of the subset\nsummary(A03.africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\n\n\n\n\n#Create new African data.frame with only infant mortality and life expectancy\nA03.africadata.infantlife &lt;- A03.africadata[, c('infant_mortality', 'life_expectancy')]\n\n# look at the subset's structure\nstr(A03.africadata.infantlife)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\n# look at a summary of the subset\nsummary(A03.africadata.infantlife)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n\n\n\n\n\n#Create new African data.frame with only Population and life expectancy\nA03.africadata.poplife &lt;- A03.africadata[, c('population', 'life_expectancy')]\n\n# look at the subset's structure\nstr(A03.africadata.poplife)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\n# look at a summary of the subset\nsummary(A03.africadata.poplife)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51                         \n\n\n\n\n\n\n\n\n\n# Life expectancy as a function of infant mortality\nplot(life_expectancy~infant_mortality\n     , data=A03.africadata.infantlife\n     , main=\"Africa: Infant Mortality and Life Expectancy\"\n     , xlab=\"Infant Mortality\"\n     , ylab=\"Life Expectancy\"\n     )\n\n\n\n\n\n\n\n\n\n\n\n\n# Life expectancy as a function of population size\nplot(life_expectancy~population\n     , data=A03.africadata.poplife\n     , main=\"Africa: Population and Life Expectancy\"\n     , xlab=\"Population Size\"\n     , log=\"x\"\n     , ylab=\"Life Expectancy\"\n     )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Get African rows with missing values for infant mortality\nA03.africadata.noinfants &lt;-  A03.africadata[A03.africadata$infant_mortality==\"NA\",]\n\n# look at the subset's structure\nstr(A03.africadata.noinfants)\n\n'data.frame':   226 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: NA NA NA NA NA NA NA NA NA NA ...\n $ year            : int  NA NA NA NA NA NA NA NA NA NA ...\n $ infant_mortality: num  NA NA NA NA NA NA NA NA NA NA ...\n $ life_expectancy : num  NA NA NA NA NA NA NA NA NA NA ...\n $ fertility       : num  NA NA NA NA NA NA NA NA NA NA ...\n $ population      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ gdp             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: NA NA NA NA NA NA NA NA NA NA ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: NA NA NA NA NA NA NA NA NA NA ...\n\n# look at a summary of the subset\nsummary(A03.africadata.noinfants)\n\n                country         year     infant_mortality life_expectancy\n Albania            :  0   Min.   : NA   Min.   : NA      Min.   : NA    \n Algeria            :  0   1st Qu.: NA   1st Qu.: NA      1st Qu.: NA    \n Angola             :  0   Median : NA   Median : NA      Median : NA    \n Antigua and Barbuda:  0   Mean   :NaN   Mean   :NaN      Mean   :NaN    \n Argentina          :  0   3rd Qu.: NA   3rd Qu.: NA      3rd Qu.: NA    \n (Other)            :  0   Max.   : NA   Max.   : NA      Max.   : NA    \n NA's               :226   NA's   :226   NA's   :226      NA's   :226    \n   fertility     population       gdp         continent  \n Min.   : NA   Min.   : NA   Min.   : NA   Africa  :  0  \n 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA   Americas:  0  \n Median : NA   Median : NA   Median : NA   Asia    :  0  \n Mean   :NaN   Mean   :NaN   Mean   :NaN   Europe  :  0  \n 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA   Oceania :  0  \n Max.   : NA   Max.   : NA   Max.   : NA   NA's    :226  \n NA's   :226   NA's   :226   NA's   :226                 \n                       region   \n Australia and New Zealand:  0  \n Caribbean                :  0  \n Central America          :  0  \n Central Asia             :  0  \n Eastern Africa           :  0  \n (Other)                  :  0  \n NA's                     :226  \n\n# This is not good ... all the values are \"NA\", not just those for $infant_mortality\n\n\n\n\nGarbage in, garbage out … all the $year values are inexplicably “NA”\n\n# Which years of African data are missing infant mortality rates?\nA03.africadata.noinfants.years &lt;- table(A03.africadata.noinfants$year)\n\n# look at the subset's structure\nstr(A03.africadata.noinfants.years)\n\n 'table' int[0 (1d)] \n - attr(*, \"dimnames\")=List of 1\n  ..$ : NULL\n\n# look at a summary of the subset\nsummary(A03.africadata.noinfants.years)\n\nNumber of cases in table: 0 \nNumber of factors: 1 \n\n\n\n\n\n\n# Get African rows for the year 2000\nA03.africadata.Y2K &lt;-  A03.africadata[A03.africadata$year==\"2000\",]\n\n# look at the subset's structure\nstr(A03.africadata.Y2K)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n# look at a summary of the subset\nsummary(A03.africadata.Y2K)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0  \n\n\n\n\n\n\n\n\n\n# Life expectancy as a function of infant mortality\nplot(life_expectancy~infant_mortality\n     , data=A03.africadata.Y2K\n     , main=\"Africa 2000: Infant Mortality and Life Expectancy\"\n     , xlab=\"Infant Mortality\"\n     , ylab=\"Life Expectancy\"\n     )\n\n\n\n\n\n\n\n\n\n\n\n\n# Life expectancy as a function of population size\nplot(life_expectancy~population\n     , data=A03.africadata.Y2K\n     , main=\"Africa 2000: Population and Life Expectancy\"\n     , xlab=\"Population Size\"\n     , log=\"x\"\n     , ylab=\"Life Expectancy\"\n     )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Fit life expectancy as the outcome, and infant mortality as the predictor.\nA03.fit1 &lt;- lm(life_expectancy~infant_mortality\n               , data=A03.africadata.Y2K\n               )\n# Fit life expectancy as the outcome, and population as the predictor.\nA03.fit2 &lt;- lm(life_expectancy~population\n               , data=A03.africadata.Y2K\n               )\n\n\n\n\n\n# Summary where infant mortality is the predictor\nsummary(A03.fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = A03.africadata.Y2K)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n# Summary where population is the predictor\nsummary(A03.fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = A03.africadata.Y2K)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\n\n\n\nRegarding Infant Mortality:\n\nOur NULL hypothesis is that infant mortality is not a predictor of life expectancy.\nOur alternative hypothesis is that infant mortality is a predictor.\n\nGiven the p-value of 2.826e-08 is well below all typical significance levels of 0.10, 0.05, and 0.01, we can reject the NULL hypothesis and conclude infant mortality is a predictor of life expectancy.\nRegarding Population:\n\nOur NULL hypothesis is that population is not a predictor of life expectancy.\nOur alternative hypothesis is that population is a predictor.\n\nGiven the p-value of 0.6159 is well above all typical significance levels of 0.10, 0.05, and 0.01, we fail to reject the NULL hypothesis and conclude population is not a predictor of life expectancy."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "Placeholder file for the future data/results presentation exercise."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/Zane/Documents/UTSA-MSDA/Coursework/2024-02 Summer/DA 6833 02T - Thursday 6pm/P2-Portfolio Clone/zanechumley-P2-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n# Check column names\ncolnames(mydata)\n\n[1] \"Height\" \"Weight\" \"Gender\" \"BMI\"    \"Race\"  \n\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             11    \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Race                  0             1   4  16     0        6          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 F: 5, M: 4, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean    sd   p0   p25   p50   p75  p100\n1 Height                0             1 156   34.9  60   155   166   176.  183  \n2 Weight                0             1  62.8 28.4   0    52.5  60    78   110  \n3 BMI                   0             1  24.8  7.35 15.9  20.6  23.4  25.0  39.9\n  hist \n1 ▁▁▁▂▇\n2 ▂▁▇▅▃\n3 ▃▇▁▁▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1)  \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\np5 &lt;- mydata %&gt;% ggplot(aes(x = Weight, y = BMI)) + geom_point() + geom_smooth(method = 'lm')\nplot(p5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file &lt;- here::here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-bmi.png\")\nggsave(filename = figure_file, plot = p5) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\np6 &lt;- mydata %&gt;%\n  ggplot(aes(x = Race, y = Height)) +\n  geom_bar(stat = \"summary\", fun = \"mean\") +  \n  labs(title = \"Height by Race\", x = \"Race\", y = \"Height\") +\n  theme_minimal() \nfigure_file &lt;- here::here(\"starter-analysis-exercise\", \"results\", \"figures\", \"mean-height-by-race.png\")\nggsave(filename = figure_file, plot = p6)\n\nSaving 7 x 5 in image\n\nprint(p6)\n\n\n\n\n\n\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at C:/Users/Zane/Documents/UTSA-MSDA/Coursework/2024-02 Summer/DA 6833 02T - Thursday 6pm/P2-Portfolio Clone/zanechumley-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Zane Chumley Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Zane Chumley Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Zane Chumley Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Zane Chumley Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Zane Chumley Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#identification",
    "href": "coding-exercise/coding-exercise.html#identification",
    "title": "R Coding Exercise",
    "section": "",
    "text": "# DA 6833 02T\n# Summer 2024\n# School of Data Science\n# University of Texas at San Antonio\n\n# Zane Chumley\n# Banner ID: @01318598\n# UTSAid: wgs999"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#load-libraries-as-needed",
    "href": "coding-exercise/coding-exercise.html#load-libraries-as-needed",
    "title": "R Coding Exercise",
    "section": "",
    "text": "library(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at C:/Users/Zane/Documents/UTSA-MSDA/Coursework/2024-02 Summer/DA 6833 02T - Thursday 6pm/P2-Portfolio Clone/zanechumley-P2-portfolio\n\nlibrary(dslabs) #for data used in this assignment"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#startup",
    "href": "coding-exercise/coding-exercise.html#startup",
    "title": "R Coding Exercise",
    "section": "",
    "text": "# DA 6833 02T\n# Summer 2024\n# School of Data Science\n# University of Texas at San Antonio\n\n# Zane Chumley\n# Banner ID: @01318598\n# UTSAid: wgs999\n\n\n\n\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at C:/Users/Zane/Documents/UTSA-MSDA/Coursework/2024-02 Summer/DA 6833 02T - Thursday 6pm/P2-Portfolio Clone/zanechumley-P2-portfolio\n\nlibrary(dslabs) #for data used in this assignment\n\n\n\n\n\n# review help file for gapminder dataset\n# after the 20th time, opening this file just got excessively and needlessly annoying\n# help(gapminder)\n\n# assign dataset to internal variable\nA03 &lt;- gapminder \n\n# look at the data's structure\nstr(A03)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n# look at a summary of the data\nsummary(A03)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n# look at the data's type\nclass(A03)\n\n[1] \"data.frame\""
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#processing-data",
    "href": "coding-exercise/coding-exercise.html#processing-data",
    "title": "R Coding Exercise",
    "section": "",
    "text": "# create new data.frame with African data only\nA03.africadata &lt;- A03[A03$continent==\"Africa\",]\n\n# look at the subset's structure\nstr(A03.africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n# look at a summary of the subset\nsummary(A03.africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\n\n\n\n\n#Create new African data.frame with only infant mortality and life expectancy\nA03.africadata.infantlife &lt;- A03.africadata[, c('infant_mortality', 'life_expectancy')]\n\n# look at the subset's structure\nstr(A03.africadata.infantlife)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\n# look at a summary of the subset\nsummary(A03.africadata.infantlife)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n\n\n\n\n\n#Create new African data.frame with only Population and life expectancy\nA03.africadata.poplife &lt;- A03.africadata[, c('population', 'life_expectancy')]\n\n# look at the subset's structure\nstr(A03.africadata.poplife)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\n# look at a summary of the subset\nsummary(A03.africadata.poplife)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#plotting-data",
    "href": "coding-exercise/coding-exercise.html#plotting-data",
    "title": "R Coding Exercise",
    "section": "",
    "text": "# Life expectancy as a function of infant mortality\nplot(life_expectancy~infant_mortality\n     , data=A03.africadata.infantlife\n     , main=\"Africa: Infant Mortality and Life Expectancy\"\n     , xlab=\"Infant Mortality\"\n     , ylab=\"Life Expectancy\"\n     )\n\n\n\n\n\n\n\n\n\n\n\n\n# Life expectancy as a function of population size\nplot(life_expectancy~population\n     , data=A03.africadata.poplife\n     , main=\"Africa: Population and Life Expectancy\"\n     , xlab=\"Population Size\"\n     , log=\"x\"\n     , ylab=\"Life Expectancy\"\n     )"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-data-processing",
    "href": "coding-exercise/coding-exercise.html#more-data-processing",
    "title": "R Coding Exercise",
    "section": "",
    "text": "# Get African rows with missing values for infant mortality\nA03.africadata.noinfants &lt;-  A03.africadata[A03.africadata$infant_mortality==\"NA\",]\n\n# look at the subset's structure\nstr(A03.africadata.noinfants)\n\n'data.frame':   226 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: NA NA NA NA NA NA NA NA NA NA ...\n $ year            : int  NA NA NA NA NA NA NA NA NA NA ...\n $ infant_mortality: num  NA NA NA NA NA NA NA NA NA NA ...\n $ life_expectancy : num  NA NA NA NA NA NA NA NA NA NA ...\n $ fertility       : num  NA NA NA NA NA NA NA NA NA NA ...\n $ population      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ gdp             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: NA NA NA NA NA NA NA NA NA NA ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: NA NA NA NA NA NA NA NA NA NA ...\n\n# look at a summary of the subset\nsummary(A03.africadata.noinfants)\n\n                country         year     infant_mortality life_expectancy\n Albania            :  0   Min.   : NA   Min.   : NA      Min.   : NA    \n Algeria            :  0   1st Qu.: NA   1st Qu.: NA      1st Qu.: NA    \n Angola             :  0   Median : NA   Median : NA      Median : NA    \n Antigua and Barbuda:  0   Mean   :NaN   Mean   :NaN      Mean   :NaN    \n Argentina          :  0   3rd Qu.: NA   3rd Qu.: NA      3rd Qu.: NA    \n (Other)            :  0   Max.   : NA   Max.   : NA      Max.   : NA    \n NA's               :226   NA's   :226   NA's   :226      NA's   :226    \n   fertility     population       gdp         continent  \n Min.   : NA   Min.   : NA   Min.   : NA   Africa  :  0  \n 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA   Americas:  0  \n Median : NA   Median : NA   Median : NA   Asia    :  0  \n Mean   :NaN   Mean   :NaN   Mean   :NaN   Europe  :  0  \n 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA   Oceania :  0  \n Max.   : NA   Max.   : NA   Max.   : NA   NA's    :226  \n NA's   :226   NA's   :226   NA's   :226                 \n                       region   \n Australia and New Zealand:  0  \n Caribbean                :  0  \n Central America          :  0  \n Central Asia             :  0  \n Eastern Africa           :  0  \n (Other)                  :  0  \n NA's                     :226  \n\n# This is not good ... all the values are \"NA\", not just those for $infant_mortality\n\n\n\n\nGarbage in, garbage out … all the $year values are inexplicably “NA”\n\n# Which years of African data are missing infant mortality rates?\nA03.africadata.noinfants.years &lt;- table(A03.africadata.noinfants$year)\n\n# look at the subset's structure\nstr(A03.africadata.noinfants.years)\n\n 'table' int[0 (1d)] \n - attr(*, \"dimnames\")=List of 1\n  ..$ : NULL\n\n# look at a summary of the subset\nsummary(A03.africadata.noinfants.years)\n\nNumber of cases in table: 0 \nNumber of factors: 1 \n\n\n\n\n\n\n# Get African rows for the year 2000\nA03.africadata.Y2K &lt;-  A03.africadata[A03.africadata$year==\"2000\",]\n\n# look at the subset's structure\nstr(A03.africadata.Y2K)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n# look at a summary of the subset\nsummary(A03.africadata.Y2K)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-plotting-data",
    "href": "coding-exercise/coding-exercise.html#more-plotting-data",
    "title": "R Coding Exercise",
    "section": "",
    "text": "# Life expectancy as a function of infant mortality\nplot(life_expectancy~infant_mortality\n     , data=A03.africadata.Y2K\n     , main=\"Africa 2000: Infant Mortality and Life Expectancy\"\n     , xlab=\"Infant Mortality\"\n     , ylab=\"Life Expectancy\"\n     )\n\n\n\n\n\n\n\n\n\n\n\n\n# Life expectancy as a function of population size\nplot(life_expectancy~population\n     , data=A03.africadata.Y2K\n     , main=\"Africa 2000: Population and Life Expectancy\"\n     , xlab=\"Population Size\"\n     , log=\"x\"\n     , ylab=\"Life Expectancy\"\n     )"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#simple-model-fits---africa-2000",
    "href": "coding-exercise/coding-exercise.html#simple-model-fits---africa-2000",
    "title": "R Coding Exercise",
    "section": "",
    "text": "# Fit life expectancy as the outcome, and infant mortality as the predictor.\nA03.fit1 &lt;- lm(life_expectancy~infant_mortality\n               , data=A03.africadata.Y2K\n               )\n# Fit life expectancy as the outcome, and population as the predictor.\nA03.fit2 &lt;- lm(life_expectancy~population\n               , data=A03.africadata.Y2K\n               )\n\n\n\n\n\n# Summary where infant mortality is the predictor\nsummary(A03.fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = A03.africadata.Y2K)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n# Summary where population is the predictor\nsummary(A03.fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = A03.africadata.Y2K)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\n\n\n\nRegarding Infant Mortality:\n\nOur NULL hypothesis is that infant mortality is not a predictor of life expectancy.\nOur alternative hypothesis is that infant mortality is a predictor.\n\nGiven the p-value of 2.826e-08 is well below all typical significance levels of 0.10, 0.05, and 0.01, we can reject the NULL hypothesis and conclude infant mortality is a predictor of life expectancy.\nRegarding Population:\n\nOur NULL hypothesis is that population is not a predictor of life expectancy.\nOur alternative hypothesis is that population is a predictor.\n\nGiven the p-value of 0.6159 is well above all typical significance levels of 0.10, 0.05, and 0.01, we fail to reject the NULL hypothesis and conclude population is not a predictor of life expectancy."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#movie-rating-distribution",
    "href": "coding-exercise/coding-exercise.html#movie-rating-distribution",
    "title": "R Coding Exercise",
    "section": "Movie Rating Distribution",
    "text": "Movie Rating Distribution\n\n# create a histogram of ratings distribution\np3 &lt;- movielens %&gt;%\n        ggplot(aes(x = rating)) +\n        geom_histogram(fill = \"coral3\", bins = 10) +\n        geom_vline(xintercept = mean(movielens$rating),\n                   color = \"coral\",\n                   linetype = 2) +\n        geom_text(aes(x = mean(movielens$rating) - .1,\n                      y = 25000,\n                      label = round(mean(movielens$rating), 2),\n                      angle = 90,\n                      color = \"coral3\")) +\n        ylab(NULL) + xlab(\"Rating\") +\n        labs() +\n        theme_clean() + \n        theme(plot.title = element_text(hjust = .5), legend.position = \"none\")\np3\n\nWarning: Use of `movielens$rating` is discouraged.\nℹ Use `rating` instead.\nUse of `movielens$rating` is discouraged.\nℹ Use `rating` instead.\n\n\nWarning in geom_text(aes(x = mean(movielens$rating) - 0.1, y = 25000, label = round(mean(movielens$rating), : All aesthetics have length 1, but the data has 100004 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nTaking a look at the histogram for ratings we observe that the data is left skewed with a mean rating of 3.54 out of 5. The most common rating being 4 out of 5. We would expect that movie quality more broadly likely follows a fairly normal distribution which makes this distribution of ratings interesting. This perhaps suggests that viewers motivated enough to leave a review tend to be those that view a movie more positively, while those that do not abstain from submitting a rating."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#movie-ratings-through-the-years",
    "href": "coding-exercise/coding-exercise.html#movie-ratings-through-the-years",
    "title": "R Coding Exercise",
    "section": "Movie Ratings Through the Years",
    "text": "Movie Ratings Through the Years\n\n# examining per year sample sizes to determine if there are some we should exclude from analysis\nmovielens %&gt;%\n  select(year) %&gt;%\n  group_by(year) %&gt;%\n  summarise(n = n())\n\n# A tibble: 104 × 2\n    year     n\n   &lt;int&gt; &lt;int&gt;\n 1  1902     6\n 2  1915     2\n 3  1916     1\n 4  1917     2\n 5  1918     2\n 6  1919     1\n 7  1920    15\n 8  1921    12\n 9  1922    28\n10  1923     3\n# ℹ 94 more rows\n\n# years before 1930 had &lt;30 movie ratings so we'll exclude them from analysis due to low sample size\n# create a time-series line graph of movie ratings\np1a &lt;- movielens %&gt;%\n        select(year, rating) %&gt;%\n        filter(year &gt;= 1930) %&gt;% \n        group_by(year) %&gt;%\n        summarise(avg_rating = mean(rating)) %&gt;% \n        ggplot(aes(x = year, y = avg_rating)) + \n        theme_clean() + \n        geom_line(color = \"coral3\", na.rm = TRUE) +\n        geom_vline(xintercept = 1995,\n                   color = \"coral\",\n                   linetype = 2) +\n        geom_text(aes(x = 1993,\n                      y = 3.8,\n                      label = \"1995\",\n                      angle = 90,\n                      color = \"coral\")) +\n        xlab(\"Year\") + ylab(\"Avg. Movie Rating\") +\n        theme(legend.position = \"none\")\n# create a time-series line graph of movie ratings\np1btab &lt;- movielens %&gt;%\n        select(year, rating) %&gt;%\n        filter(year &gt;= 1930) %&gt;% \n        group_by(year) %&gt;%\n        mutate(count = n())\np1b &lt;- p1btab %&gt;% \n        ggplot(aes(x = year, y = count)) + \n        theme_clean() + \n        geom_line(color = \"coral3\", na.rm = TRUE) +\n        geom_vline(xintercept = 1995,\n                   color = \"coral\",\n                   linetype = 2) +\n        geom_text(aes(x = 1993,\n                      y = 1000,\n                      label = \"1995\",\n                      angle = 90,\n                      color = \"coral\")) +\n        xlab(\"Year\") + ylab(\"Number of Ratings\") +\n        theme(legend.position = \"none\")\nwrap_plots(p1a, p1b, ncol = 1)\n\nWarning in geom_text(aes(x = 1993, y = 3.8, label = \"1995\", angle = 90, : All aesthetics have length 1, but the data has 87 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_text(aes(x = 1993, y = 1000, label = \"1995\", angle = 90, : All aesthetics have length 1, but the data has 99778 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nTaking a look at the average rating of movies by release year we can see that average ratings tend to be lower for more recent years. Additionally we see a trend towards less variance in average ratings the more recent the movie release year. Examining the count of movie ratings by release year we can see that there is a significant bias in ratings count for more recently released movies – with a peak number of ratings for movies released in the year 1995. This may be a function of when the ability to rate movies became broadly available with those movies that were still fresh at the time receiving the highest number of overall ratings.\n\n# create a histogram of ratings distribution\np3c &lt;- movielens %&gt;%\n        filter(year == 2000) %&gt;% \n        ggplot(aes(x = rating)) +\n        geom_histogram(fill = \"coral3\", bins = 10) +\n        geom_vline(xintercept = mean(movielens[which(movielens$year == 2000),]$rating),\n                   color = \"coral\",\n                   linetype = 2) +\n        geom_text(aes(x = mean(movielens[which(movielens$year == 2000),]$rating) - .2,\n                      y = 900,\n                      label = round(mean(movielens[which(movielens$year == 2000),]$rating), 2),\n                      angle = 90,\n                      color = \"coral3\")) +\n        ylab(NULL) + xlab(\"Rating\") +\n        labs(title = \"Released in 2000\") +\n        theme_clean() + \n        theme(plot.title = element_text(hjust = .5), legend.position = \"none\")\np3b &lt;- movielens %&gt;%\n        filter(year == 1950) %&gt;% \n        ggplot(aes(x = rating)) +\n        geom_histogram(fill = \"coral3\", bins = 10) +\n        geom_vline(xintercept = mean(movielens[which(movielens$year == 1950),]$rating),\n                   color = \"coral\",\n                   linetype = 2) +\n        geom_text(aes(x = mean(movielens[which(movielens$year == 1950),]$rating) - .2,\n                      y = 90,\n                      label = round(mean(movielens[which(movielens$year == 1950),]$rating), 2),\n                      angle = 90,\n                      color = \"coral3\")) +\n        ylab(NULL) + xlab(\"Rating\") +\n        labs(title = \"Released in 1950\") +\n        theme_clean() + \n        theme(plot.title = element_text(hjust = .5), legend.position = \"none\")\nwrap_plots(p3b, p3c, ncol = 2)\n\nWarning in geom_text(aes(x = mean(movielens[which(movielens$year == 1950), : All aesthetics have length 1, but the data has 236 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_text(aes(x = mean(movielens[which(movielens$year == 2000), : All aesthetics have length 1, but the data has 4054 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nReexamining the distribution of ratings now that we understand there is a recency bias component, we can see that older films tend to be skewed significantly more than newer films with means farther from a neutral rating of 3."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#best-and-worst-movies",
    "href": "coding-exercise/coding-exercise.html#best-and-worst-movies",
    "title": "R Coding Exercise",
    "section": "Best and Worst Movies",
    "text": "Best and Worst Movies\n\n# subset dataframe for best movies by average rating with &gt;= 50 ratings\nbestmovies &lt;- movielens %&gt;%\n                select(title, year, rating) %&gt;%\n                group_by(title) %&gt;%\n                summarise(avg_rating = mean(rating), n = n()) %&gt;%\n                filter(n &gt;= 50) %&gt;% \n                arrange(desc(avg_rating)) %&gt;% \n                slice(1:10) %&gt;%\n                mutate(type = \"best\")\n# subset dataframe for worst movies by average rating with &gt;= 50 ratings\nworstmovies &lt;- movielens %&gt;%\n                select(title, year, rating) %&gt;%\n                group_by(title) %&gt;%\n                summarise(avg_rating = mean(rating), n = n()) %&gt;%\n                filter(n &gt;= 50) %&gt;% \n                arrange(avg_rating) %&gt;% \n                slice(1:10) %&gt;%\n                mutate(type = \"worst\")\n# combined dataframe for barchart\nbestworst &lt;- bind_rows(bestmovies, worstmovies)\n\np2 &lt;- bestworst %&gt;% \n  ggplot(aes(x = avg_rating, y = reorder(title, avg_rating), fill = type)) + \n  geom_col(na.rm = TRUE) +\n  geom_text(aes(label = round(avg_rating, 2)),\n            size = 3,\n            fontface = \"bold\",\n            hjust = 1.25,\n            color = \"white\") +\n  scale_fill_manual(values = c(\"coral2\", \"aquamarine3\")) +\n  ylab(NULL) + xlab(\"Avg. Rating\") +\n  labs(title = \"Best & Worst Rated Movies\", caption = \"For movies with &gt;50 ratings\") +\n  theme_clean() + \n  theme(plot.title = element_text(hjust = .5), legend.position = \"none\")\np2\n\n\n\n\n\n\n\n\nFinally we examine the gulf that exists between the 10 highest- and lowest-rated films. For this analysis piece we focused solely on those movies with at least 50 or more ratings in order to exclude low sample size films of little note. With an average difference between the best and the worst rated movies of little over 1.5 stars we can see there exists a large disparity between highly- and lowly-rated movies. With many of the former camp being dramas and many of the latter camp falling under the umbrella of comedy."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "# DA 6833 02T\n# Summer 2024\n# School of Data Science\n# University of Texas at San Antonio\n\n# Zane Chumley\n# Banner ID: @01318598\n# UTSAid: wgs999\n\n\n\n\n\n# install.packages(\"simstudy\")\n\n\n\n\n\n# library(readxl) #for loading Excel files\n# library(dplyr) #for data processing/cleaning\n# library(tidyr) #for data processing/cleaning\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# library(skimr) #for nice visualization of data \n# library(here) #to set paths\n# library(dslabs) #for data used in this assignment\nlibrary(tibble)\nlibrary(simstudy)\n\n\n\n\n\n\n\nWrite code that generates a synthetic dataset. This dataset should have multiple variables, and there should be some associations between variables.\n\n# Make it reproducible\nset.seed(641)\n# Global Settings\nobservations=500\n# Defining First Variable: x\ndefined &lt;- defData(varname=\"x\"\n                   , formula=10\n                   , variance=1.3\n                   , dist=\"poisson\"\n                   )\n# Defining Second Variable: y\ndefined &lt;- defData(defined\n                   , varname=\"y\"\n                   , formula=\"3*(x-7)\"\n                   , variance=1.5\n                   , dist=\"normal\"\n                   )\n# Defining Third Variable: z\ndefined &lt;- defData(defined\n                   , varname=\"z\"\n                   , formula=\"x*y/2\"\n                   , variance=1.7\n                   , dist=\"normal\"\n                   )\n# Now we will generate desired observations of the synthetic data.\nzaneata &lt;- genData(observations\n             , defined\n             )\n# Finally, we will divide as evenly as possible the desired observations into seven (7) sections.\nzaneata &lt;- trtAssign(zaneata\n               , nTrt = 7\n               , grpName = \"section\"\n               , balanced = TRUE\n               )\n# Now let's look at the first and last ten (10) observations of the data.\nhead(zaneata,10)\n\nKey: &lt;id&gt;\n       id     x         y         z section\n    &lt;int&gt; &lt;int&gt;     &lt;num&gt;     &lt;num&gt;   &lt;int&gt;\n 1:     1    13 18.254358 117.86337       7\n 2:     2    15 24.751103 186.93360       3\n 3:     3     9  7.035150  32.46657       2\n 4:     4     9  7.231929  32.68282       2\n 5:     5     5 -6.831602 -16.96944       1\n 6:     6    14 19.657123 137.63140       1\n 7:     7     9  8.261770  37.22343       1\n 8:     8     8  3.111795  14.84778       6\n 9:     9     9  8.642109  40.30047       6\n10:    10     6 -3.499200 -10.40272       4\n\ntail(zaneata,10)\n\nKey: &lt;id&gt;\n       id     x          y          z section\n    &lt;int&gt; &lt;int&gt;      &lt;num&gt;      &lt;num&gt;   &lt;int&gt;\n 1:   491    13 19.6520625 127.789864       4\n 2:   492    10  9.6177054  48.275649       1\n 3:   493    14 20.9016717 147.469302       7\n 4:   494     7  0.8608051   4.059823       6\n 5:   495    12 16.2010577  97.448963       7\n 6:   496    13 19.0307264 125.239466       2\n 7:   497     7  0.7081732   1.141877       1\n 8:   498    11 11.0524830  60.482550       3\n 9:   499     9  9.3003280  40.294613       5\n10:   500     8  1.7157870   9.459163       5\n\n\n\n\n\nThen write code that explores the data by making plots or tables to confirm that your synthetic data is what you expect it to be.\n\n# Let's get some basic information about the data generated.\nsummary(zaneata$x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   8.000  10.000   9.898  12.000  23.000 \n\nsummary(zaneata$y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-13.427   2.147   8.384   8.770  15.444  48.277 \n\nsummary(zaneata$z)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-24.247   7.947  40.859  58.816  92.287 558.538 \n\ntable(zaneata$section)\n\n\n 1  2  3  4  5  6  7 \n72 71 71 71 71 72 72 \n\n\nNow let’s take a look at the three (3) variables by section.\n\nboxplot(zaneata$x~zaneata$section\n        , main=\"Boxplot of Variable x By section\"\n        , xlab=\"section\"\n        , ylab=\"x\"\n        )\n\n\n\n\n\n\n\nboxplot(zaneata$y~zaneata$section\n        , main=\"Boxplot of Variable y By section\"\n        , xlab=\"section\"\n        , ylab=\"y\"\n        )\n\n\n\n\n\n\n\nboxplot(zaneata$z~zaneata$section\n        , main=\"Boxplot of Variable z By section\"\n        , xlab=\"section\"\n        , ylab=\"z\"\n        )\n\n\n\n\n\n\n\n\nNext, let’s get a qualitative idea at the relationships between the three (3) variables. As we built y off of x and z off of both y and x, we will graph the following relationships:\n\ny=f(x)\nz=f(y)\nz=f(x)\n\n\n# Scatterplots: \nplot(zaneata$y~zaneata$x\n     , main=\"Variable y as a function of x\"\n     , xlab=\"values of x\"\n     , ylab=\"values of y\"\n     )\n\n\n\n\n\n\n\nplot(zaneata$z~zaneata$y\n     , main=\"Variable z as a function of y\"\n     , xlab=\"values of y\"\n     , ylab=\"values of z\"\n     )\n\n\n\n\n\n\n\nplot(zaneata$z~zaneata$x\n     , main=\"Variable z as a function of x\"\n     , xlab=\"values of x\"\n     , ylab=\"values of z\"\n     )\n\n\n\n\n\n\n\n\nAll plots show a fairly strong relationship between the values on the axes of all three (3) plots. This is not surprising, given that the synthetic data was derived primarily from previously-defined variables.\nSince the correlations are so strong, we’ll introduce additional randomness in the values of all three (3) variables, and then we’ll re-run the plots. We’ll base the degree of randomness on the third root of the range of the existing values, then allow a random value in the range to be either added or subtracted to the value.\nFinally, to further destabilize the data, we will round all three (3) variables to integers.\n\n#Calculate range of randomness for each variable\nxrange &lt;- trunc((max(zaneata$x)-min(zaneata$x))^(1/3)+0.5)\nxrangedist=paste(\"Additional randomness of x will be between \"\n                 , 0-xrange\n                 , \" and \"\n                 , xrange\n                 )\nprint(xrangedist)\n\n[1] \"Additional randomness of x will be between  -3  and  3\"\n\nyrange &lt;- trunc((max(zaneata$y)-min(zaneata$y))^(1/3)+0.5)\nyrangedist=paste(\"Additional randomness of y will be between \"\n                 , 0-yrange\n                 , \" and \"\n                 , yrange\n                 )\nprint(yrangedist)\n\n[1] \"Additional randomness of y will be between  -4  and  4\"\n\nzrange &lt;- trunc((max(zaneata$z)-min(zaneata$z))^(1/3)+0.5)\nzrangedist=paste(\"Additional randomness of z will be between \"\n                 , 0-zrange\n                 , \" and \"\n                 , zrange\n                 )\nprint(zrangedist)\n\n[1] \"Additional randomness of z will be between  -8  and  8\"\n\n# Add columns for updated variables\nzaneata &lt;- add_column(zaneata, x2=1:500, .after=\"x\")\nzaneata &lt;- add_column(zaneata, y2=1:500, .after=\"y\")\nzaneata &lt;- add_column(zaneata, z2=1:500, .after=\"z\")\n# Set ranges for sample function\nxsample &lt;- c(-xrange:xrange)\nysample &lt;- c(-yrange:yrange)\nzsample &lt;- c(-zrange:zrange)\n# Update the values based on newly-introduced randomness\nfor (index in 1:observations)\n{\n  xrand &lt;- sample(x=xsample\n                  , 1\n                  , replace = FALSE\n                  , prob = NULL\n                  )\n  yrand &lt;- sample(ysample\n                  , 1\n                  , replace = FALSE\n                  , prob = NULL\n                  )\n  zrand &lt;- sample(zsample\n                  , 1\n                  , replace = FALSE\n                  , prob = NULL\n                  )\n  zaneata[index,]$x2 &lt;- trunc(zaneata[index,]$x + xrand + 0.5)\n  zaneata[index,]$y2 &lt;- trunc(zaneata[index,]$y + yrand + 0.5)\n  zaneata[index,]$z2 &lt;- trunc(zaneata[index,]$z + zrand + 0.5)\n}\n# Looking again at the first and last ten (10) observations of the data.\nhead(zaneata,10)\n\nKey: &lt;id&gt;\n       id     x    x2         y    y2         z    z2 section\n    &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;num&gt; &lt;int&gt;     &lt;num&gt; &lt;int&gt;   &lt;int&gt;\n 1:     1    13    14 18.254358    19 117.86337   119       7\n 2:     2    15    18 24.751103    25 186.93360   184       3\n 3:     3     9     9  7.035150     5  32.46657    29       2\n 4:     4     9    11  7.231929     8  32.68282    30       2\n 5:     5     5     3 -6.831602    -7 -16.96944   -20       1\n 6:     6    14    15 19.657123    21 137.63140   136       1\n 7:     7     9     9  8.261770     6  37.22343    44       1\n 8:     8     8     8  3.111795     3  14.84778     8       6\n 9:     9     9    12  8.642109    10  40.30047    47       6\n10:    10     6     9 -3.499200    -1 -10.40272    -5       4\n\ntail(zaneata,10)\n\nKey: &lt;id&gt;\n       id     x    x2          y    y2          z    z2 section\n    &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;num&gt; &lt;int&gt;      &lt;num&gt; &lt;int&gt;   &lt;int&gt;\n 1:   491    13    14 19.6520625    18 127.789864   126       4\n 2:   492    10    13  9.6177054    13  48.275649    47       1\n 3:   493    14    16 20.9016717    17 147.469302   144       7\n 4:   494     7     9  0.8608051     5   4.059823     5       6\n 5:   495    12    15 16.2010577    20  97.448963    91       7\n 6:   496    13    11 19.0307264    16 125.239466   125       2\n 7:   497     7     9  0.7081732     0   1.141877     7       1\n 8:   498    11     9 11.0524830    14  60.482550    60       3\n 9:   499     9     6  9.3003280    13  40.294613    40       5\n10:   500     8     5  1.7157870     1   9.459163     9       5\n\n# Let's get some basic information about the destablized data.\nsummary(zaneata$x2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   7.000  10.000   9.872  12.000  21.000 \n\nsummary(zaneata$y2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -16.00    2.00    8.00    8.99   15.00   45.00 \n\nsummary(zaneata$z2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -27.00    8.00   40.00   59.46   93.00  565.00 \n\n\nNow let’s compare, side-by-side, the three (3) functions using both the original and the destabilized data. Note that the axes on the side-by-side plots were not equalized, but set automatically based on each variable’s range.\nplot(zaneata$y~zaneata$x\n     , main=\"Variable y as a function of x\"\n     , xlab=\"values of x\"\n     , ylab=\"values of y\"\n     )\nplot(zaneata$y2~zaneata$x2\n     , main=\"Variable y2 as a function of x2\"\n     , xlab=\"values of x2\"\n     , ylab=\"values of y2\"\n     )\n\n\n\n\n\n\n\n\n\n\nplot(zaneata$z~zaneata$y\n     , main=\"Variable z as a function of y\"\n     , xlab=\"values of y\"\n     , ylab=\"values of z\"\n     )\nplot(zaneata$z2~zaneata$y2\n     , main=\"Variable z2 as a function of y2\"\n     , xlab=\"values of y2\"\n     , ylab=\"values of z2\"\n     )\n\n\n\n\n\n\n\n\n\n\nplot(zaneata$z~zaneata$x\n     , main=\"Variable z as a function of x\"\n     , xlab=\"values of x\"\n     , ylab=\"values of z\"\n     )\nplot(zaneata$z2~zaneata$x2\n     , main=\"Variable z2 as a function of x2\"\n     , xlab=\"values of x2\"\n     , ylab=\"values of z2\"\n     )\n\n\n\n\n\n\n\n\n\n\nComparing the plots for the original and destabilized data of all three (3) functions, nontrivial instability has been introduced by the destablization. Nonetheless, the correlation in all three (3) functions remains apparent.\n\n\n\nThen fit a few simple models to the data. For instance, use the lm or glm functions to fit a linear or logistic model. Make sure your model can recover the associations you built into the data. Explore if and how different models might be able to capture the patterns you see.\n\n# Let's run general linear models on the three (3) functions listed above, comparing both the original and destabilized values.\nglm(y~x\n   ,data=zaneata\n   )\n\n\nCall:  glm(formula = y ~ x, data = zaneata)\n\nCoefficients:\n(Intercept)            x  \n    -21.099        3.018  \n\nDegrees of Freedom: 499 Total (i.e. Null);  498 Residual\nNull Deviance:      47250 \nResidual Deviance: 754.4    AIC: 1631\n\nglm(y2~x2\n   ,data=zaneata\n   )\n\n\nCall:  glm(formula = y2 ~ x2, data = zaneata)\n\nCoefficients:\n(Intercept)           x2  \n    -11.932        2.119  \n\nDegrees of Freedom: 499 Total (i.e. Null);  498 Residual\nNull Deviance:      47820 \nResidual Deviance: 16580    AIC: 3176\n\nglm(z~y\n   ,data=zaneata\n   )\n\n\nCall:  glm(formula = z ~ y, data = zaneata)\n\nCoefficients:\n(Intercept)            y  \n     -1.421        6.868  \n\nDegrees of Freedom: 499 Total (i.e. Null);  498 Residual\nNull Deviance:      2483000 \nResidual Deviance: 254100   AIC: 4540\n\nglm(z2~y2\n   ,data=zaneata\n   )\n\n\nCall:  glm(formula = z2 ~ y2, data = zaneata)\n\nCoefficients:\n(Intercept)           y2  \n     0.1496       6.5969  \n\nDegrees of Freedom: 499 Total (i.e. Null);  498 Residual\nNull Deviance:      2489000 \nResidual Deviance: 407400   AIC: 4776\n\nglm(z~x\n   ,data=zaneata\n   )\n\n\nCall:  glm(formula = z ~ x, data = zaneata)\n\nCoefficients:\n(Intercept)            x  \n    -147.41        20.83  \n\nDegrees of Freedom: 499 Total (i.e. Null);  498 Residual\nNull Deviance:      2483000 \nResidual Deviance: 266800   AIC: 4565\n\nglm(z2~x2\n   ,data=zaneata\n   )\n\n\nCall:  glm(formula = z2 ~ x2, data = zaneata)\n\nCoefficients:\n(Intercept)           x2  \n     -86.98        14.83  \n\nDegrees of Freedom: 499 Total (i.e. Null);  498 Residual\nNull Deviance:      2489000 \nResidual Deviance: 958000   AIC: 5204\n\n\nWhen comparing the linear models between the original and destabilized versions of the data, all three (3) functions shared multiple characteristics:\n\nThe Null Deviance was always greater for the destabilized data than the original data, but only slightly so.\nThe AIC was also always greater for the destabilized data than the original data, but, similarly, always less than twice the AIC for the original data.\nCuriously, all the coefficients, for both intercept and function variable, were closer to zero (0) for the destabilized data. This seems counter-intuitive: as you increased instability in the function variable, you would think the coefficient for the function variable would increase, not decrease.\nUnlike the Null Deviance, the Residual Deviance was always markedly larger for the destabilized data than the original data."
  },
  {
    "objectID": "data-exercise/data-exercise.html#startup",
    "href": "data-exercise/data-exercise.html#startup",
    "title": "Data Exercise",
    "section": "",
    "text": "# DA 6833 02T\n# Summer 2024\n# School of Data Science\n# University of Texas at San Antonio\n\n# Zane Chumley\n# Banner ID: @01318598\n# UTSAid: wgs999\n\n\n\n\n\n# install.packages(\"simstudy\")\n\n\n\n\n\n# library(readxl) #for loading Excel files\n# library(dplyr) #for data processing/cleaning\n# library(tidyr) #for data processing/cleaning\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# library(skimr) #for nice visualization of data \n# library(here) #to set paths\n# library(dslabs) #for data used in this assignment\nlibrary(tibble)\nlibrary(simstudy)"
  },
  {
    "objectID": "data-exercise/data-exercise.html#selecting-option-2-generate-and-explore-synthetic-data",
    "href": "data-exercise/data-exercise.html#selecting-option-2-generate-and-explore-synthetic-data",
    "title": "Data Exercise",
    "section": "",
    "text": "Write code that generates a synthetic dataset. This dataset should have multiple variables, and there should be some associations between variables.\n\n# Make it reproducible\nset.seed(641)\n# Global Settings\nobservations=500\n# Defining First Variable: x\ndefined &lt;- defData(varname=\"x\"\n                   , formula=10\n                   , variance=1.3\n                   , dist=\"poisson\"\n                   )\n# Defining Second Variable: y\ndefined &lt;- defData(defined\n                   , varname=\"y\"\n                   , formula=\"3*(x-7)\"\n                   , variance=1.5\n                   , dist=\"normal\"\n                   )\n# Defining Third Variable: z\ndefined &lt;- defData(defined\n                   , varname=\"z\"\n                   , formula=\"x*y/2\"\n                   , variance=1.7\n                   , dist=\"normal\"\n                   )\n# Now we will generate desired observations of the synthetic data.\nzaneata &lt;- genData(observations\n             , defined\n             )\n# Finally, we will divide as evenly as possible the desired observations into seven (7) sections.\nzaneata &lt;- trtAssign(zaneata\n               , nTrt = 7\n               , grpName = \"section\"\n               , balanced = TRUE\n               )\n# Now let's look at the first and last ten (10) observations of the data.\nhead(zaneata,10)\n\nKey: &lt;id&gt;\n       id     x         y         z section\n    &lt;int&gt; &lt;int&gt;     &lt;num&gt;     &lt;num&gt;   &lt;int&gt;\n 1:     1    13 18.254358 117.86337       7\n 2:     2    15 24.751103 186.93360       3\n 3:     3     9  7.035150  32.46657       2\n 4:     4     9  7.231929  32.68282       2\n 5:     5     5 -6.831602 -16.96944       1\n 6:     6    14 19.657123 137.63140       1\n 7:     7     9  8.261770  37.22343       1\n 8:     8     8  3.111795  14.84778       6\n 9:     9     9  8.642109  40.30047       6\n10:    10     6 -3.499200 -10.40272       4\n\ntail(zaneata,10)\n\nKey: &lt;id&gt;\n       id     x          y          z section\n    &lt;int&gt; &lt;int&gt;      &lt;num&gt;      &lt;num&gt;   &lt;int&gt;\n 1:   491    13 19.6520625 127.789864       4\n 2:   492    10  9.6177054  48.275649       1\n 3:   493    14 20.9016717 147.469302       7\n 4:   494     7  0.8608051   4.059823       6\n 5:   495    12 16.2010577  97.448963       7\n 6:   496    13 19.0307264 125.239466       2\n 7:   497     7  0.7081732   1.141877       1\n 8:   498    11 11.0524830  60.482550       3\n 9:   499     9  9.3003280  40.294613       5\n10:   500     8  1.7157870   9.459163       5\n\n\n\n\n\nThen write code that explores the data by making plots or tables to confirm that your synthetic data is what you expect it to be.\n\n# Let's get some basic information about the data generated.\nsummary(zaneata$x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   8.000  10.000   9.898  12.000  23.000 \n\nsummary(zaneata$y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-13.427   2.147   8.384   8.770  15.444  48.277 \n\nsummary(zaneata$z)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-24.247   7.947  40.859  58.816  92.287 558.538 \n\ntable(zaneata$section)\n\n\n 1  2  3  4  5  6  7 \n72 71 71 71 71 72 72 \n\n\nNow let’s take a look at the three (3) variables by section.\n\nboxplot(zaneata$x~zaneata$section\n        , main=\"Boxplot of Variable x By section\"\n        , xlab=\"section\"\n        , ylab=\"x\"\n        )\n\n\n\n\n\n\n\nboxplot(zaneata$y~zaneata$section\n        , main=\"Boxplot of Variable y By section\"\n        , xlab=\"section\"\n        , ylab=\"y\"\n        )\n\n\n\n\n\n\n\nboxplot(zaneata$z~zaneata$section\n        , main=\"Boxplot of Variable z By section\"\n        , xlab=\"section\"\n        , ylab=\"z\"\n        )\n\n\n\n\n\n\n\n\nNext, let’s get a qualitative idea at the relationships between the three (3) variables. As we built y off of x and z off of both y and x, we will graph the following relationships:\n\ny=f(x)\nz=f(y)\nz=f(x)\n\n\n# Scatterplots: \nplot(zaneata$y~zaneata$x\n     , main=\"Variable y as a function of x\"\n     , xlab=\"values of x\"\n     , ylab=\"values of y\"\n     )\n\n\n\n\n\n\n\nplot(zaneata$z~zaneata$y\n     , main=\"Variable z as a function of y\"\n     , xlab=\"values of y\"\n     , ylab=\"values of z\"\n     )\n\n\n\n\n\n\n\nplot(zaneata$z~zaneata$x\n     , main=\"Variable z as a function of x\"\n     , xlab=\"values of x\"\n     , ylab=\"values of z\"\n     )\n\n\n\n\n\n\n\n\nAll plots show a fairly strong relationship between the values on the axes of all three (3) plots. This is not surprising, given that the synthetic data was derived primarily from previously-defined variables.\nSince the correlations are so strong, we’ll introduce additional randomness in the values of all three (3) variables, and then we’ll re-run the plots. We’ll base the degree of randomness on the third root of the range of the existing values, then allow a random value in the range to be either added or subtracted to the value.\nFinally, to further destabilize the data, we will round all three (3) variables to integers.\n\n#Calculate range of randomness for each variable\nxrange &lt;- trunc((max(zaneata$x)-min(zaneata$x))^(1/3)+0.5)\nxrangedist=paste(\"Additional randomness of x will be between \"\n                 , 0-xrange\n                 , \" and \"\n                 , xrange\n                 )\nprint(xrangedist)\n\n[1] \"Additional randomness of x will be between  -3  and  3\"\n\nyrange &lt;- trunc((max(zaneata$y)-min(zaneata$y))^(1/3)+0.5)\nyrangedist=paste(\"Additional randomness of y will be between \"\n                 , 0-yrange\n                 , \" and \"\n                 , yrange\n                 )\nprint(yrangedist)\n\n[1] \"Additional randomness of y will be between  -4  and  4\"\n\nzrange &lt;- trunc((max(zaneata$z)-min(zaneata$z))^(1/3)+0.5)\nzrangedist=paste(\"Additional randomness of z will be between \"\n                 , 0-zrange\n                 , \" and \"\n                 , zrange\n                 )\nprint(zrangedist)\n\n[1] \"Additional randomness of z will be between  -8  and  8\"\n\n# Add columns for updated variables\nzaneata &lt;- add_column(zaneata, x2=1:500, .after=\"x\")\nzaneata &lt;- add_column(zaneata, y2=1:500, .after=\"y\")\nzaneata &lt;- add_column(zaneata, z2=1:500, .after=\"z\")\n# Set ranges for sample function\nxsample &lt;- c(-xrange:xrange)\nysample &lt;- c(-yrange:yrange)\nzsample &lt;- c(-zrange:zrange)\n# Update the values based on newly-introduced randomness\nfor (index in 1:observations)\n{\n  xrand &lt;- sample(x=xsample\n                  , 1\n                  , replace = FALSE\n                  , prob = NULL\n                  )\n  yrand &lt;- sample(ysample\n                  , 1\n                  , replace = FALSE\n                  , prob = NULL\n                  )\n  zrand &lt;- sample(zsample\n                  , 1\n                  , replace = FALSE\n                  , prob = NULL\n                  )\n  zaneata[index,]$x2 &lt;- trunc(zaneata[index,]$x + xrand + 0.5)\n  zaneata[index,]$y2 &lt;- trunc(zaneata[index,]$y + yrand + 0.5)\n  zaneata[index,]$z2 &lt;- trunc(zaneata[index,]$z + zrand + 0.5)\n}\n# Looking again at the first and last ten (10) observations of the data.\nhead(zaneata,10)\n\nKey: &lt;id&gt;\n       id     x    x2         y    y2         z    z2 section\n    &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;num&gt; &lt;int&gt;     &lt;num&gt; &lt;int&gt;   &lt;int&gt;\n 1:     1    13    14 18.254358    19 117.86337   119       7\n 2:     2    15    18 24.751103    25 186.93360   184       3\n 3:     3     9     9  7.035150     5  32.46657    29       2\n 4:     4     9    11  7.231929     8  32.68282    30       2\n 5:     5     5     3 -6.831602    -7 -16.96944   -20       1\n 6:     6    14    15 19.657123    21 137.63140   136       1\n 7:     7     9     9  8.261770     6  37.22343    44       1\n 8:     8     8     8  3.111795     3  14.84778     8       6\n 9:     9     9    12  8.642109    10  40.30047    47       6\n10:    10     6     9 -3.499200    -1 -10.40272    -5       4\n\ntail(zaneata,10)\n\nKey: &lt;id&gt;\n       id     x    x2          y    y2          z    z2 section\n    &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;num&gt; &lt;int&gt;      &lt;num&gt; &lt;int&gt;   &lt;int&gt;\n 1:   491    13    14 19.6520625    18 127.789864   126       4\n 2:   492    10    13  9.6177054    13  48.275649    47       1\n 3:   493    14    16 20.9016717    17 147.469302   144       7\n 4:   494     7     9  0.8608051     5   4.059823     5       6\n 5:   495    12    15 16.2010577    20  97.448963    91       7\n 6:   496    13    11 19.0307264    16 125.239466   125       2\n 7:   497     7     9  0.7081732     0   1.141877     7       1\n 8:   498    11     9 11.0524830    14  60.482550    60       3\n 9:   499     9     6  9.3003280    13  40.294613    40       5\n10:   500     8     5  1.7157870     1   9.459163     9       5\n\n# Let's get some basic information about the destablized data.\nsummary(zaneata$x2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   7.000  10.000   9.872  12.000  21.000 \n\nsummary(zaneata$y2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -16.00    2.00    8.00    8.99   15.00   45.00 \n\nsummary(zaneata$z2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -27.00    8.00   40.00   59.46   93.00  565.00 \n\n\nNow let’s compare, side-by-side, the three (3) functions using both the original and the destabilized data. Note that the axes on the side-by-side plots were not equalized, but set automatically based on each variable’s range.\nplot(zaneata$y~zaneata$x\n     , main=\"Variable y as a function of x\"\n     , xlab=\"values of x\"\n     , ylab=\"values of y\"\n     )\nplot(zaneata$y2~zaneata$x2\n     , main=\"Variable y2 as a function of x2\"\n     , xlab=\"values of x2\"\n     , ylab=\"values of y2\"\n     )\n\n\n\n\n\n\n\n\n\n\nplot(zaneata$z~zaneata$y\n     , main=\"Variable z as a function of y\"\n     , xlab=\"values of y\"\n     , ylab=\"values of z\"\n     )\nplot(zaneata$z2~zaneata$y2\n     , main=\"Variable z2 as a function of y2\"\n     , xlab=\"values of y2\"\n     , ylab=\"values of z2\"\n     )\n\n\n\n\n\n\n\n\n\n\nplot(zaneata$z~zaneata$x\n     , main=\"Variable z as a function of x\"\n     , xlab=\"values of x\"\n     , ylab=\"values of z\"\n     )\nplot(zaneata$z2~zaneata$x2\n     , main=\"Variable z2 as a function of x2\"\n     , xlab=\"values of x2\"\n     , ylab=\"values of z2\"\n     )\n\n\n\n\n\n\n\n\n\n\nComparing the plots for the original and destabilized data of all three (3) functions, nontrivial instability has been introduced by the destablization. Nonetheless, the correlation in all three (3) functions remains apparent.\n\n\n\nThen fit a few simple models to the data. For instance, use the lm or glm functions to fit a linear or logistic model. Make sure your model can recover the associations you built into the data. Explore if and how different models might be able to capture the patterns you see.\n\n# Let's run general linear models on the three (3) functions listed above, comparing both the original and destabilized values.\nglm(y~x\n   ,data=zaneata\n   )\n\n\nCall:  glm(formula = y ~ x, data = zaneata)\n\nCoefficients:\n(Intercept)            x  \n    -21.099        3.018  \n\nDegrees of Freedom: 499 Total (i.e. Null);  498 Residual\nNull Deviance:      47250 \nResidual Deviance: 754.4    AIC: 1631\n\nglm(y2~x2\n   ,data=zaneata\n   )\n\n\nCall:  glm(formula = y2 ~ x2, data = zaneata)\n\nCoefficients:\n(Intercept)           x2  \n    -11.932        2.119  \n\nDegrees of Freedom: 499 Total (i.e. Null);  498 Residual\nNull Deviance:      47820 \nResidual Deviance: 16580    AIC: 3176\n\nglm(z~y\n   ,data=zaneata\n   )\n\n\nCall:  glm(formula = z ~ y, data = zaneata)\n\nCoefficients:\n(Intercept)            y  \n     -1.421        6.868  \n\nDegrees of Freedom: 499 Total (i.e. Null);  498 Residual\nNull Deviance:      2483000 \nResidual Deviance: 254100   AIC: 4540\n\nglm(z2~y2\n   ,data=zaneata\n   )\n\n\nCall:  glm(formula = z2 ~ y2, data = zaneata)\n\nCoefficients:\n(Intercept)           y2  \n     0.1496       6.5969  \n\nDegrees of Freedom: 499 Total (i.e. Null);  498 Residual\nNull Deviance:      2489000 \nResidual Deviance: 407400   AIC: 4776\n\nglm(z~x\n   ,data=zaneata\n   )\n\n\nCall:  glm(formula = z ~ x, data = zaneata)\n\nCoefficients:\n(Intercept)            x  \n    -147.41        20.83  \n\nDegrees of Freedom: 499 Total (i.e. Null);  498 Residual\nNull Deviance:      2483000 \nResidual Deviance: 266800   AIC: 4565\n\nglm(z2~x2\n   ,data=zaneata\n   )\n\n\nCall:  glm(formula = z2 ~ x2, data = zaneata)\n\nCoefficients:\n(Intercept)           x2  \n     -86.98        14.83  \n\nDegrees of Freedom: 499 Total (i.e. Null);  498 Residual\nNull Deviance:      2489000 \nResidual Deviance: 958000   AIC: 5204\n\n\nWhen comparing the linear models between the original and destabilized versions of the data, all three (3) functions shared multiple characteristics:\n\nThe Null Deviance was always greater for the destabilized data than the original data, but only slightly so.\nThe AIC was also always greater for the destabilized data than the original data, but, similarly, always less than twice the AIC for the original data.\nCuriously, all the coefficients, for both intercept and function variable, were closer to zero (0) for the destabilized data. This seems counter-intuitive: as you increased instability in the function variable, you would think the coefficient for the function variable would increase, not decrease.\nUnlike the Null Deviance, the Residual Deviance was always markedly larger for the destabilized data than the original data."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "cdcdata-exercise",
    "section": "",
    "text": "# DA 6833 02T\n# Summer 2024\n# School of Data Science\n# University of Texas at San Antonio\n\n# Zane Chumley\n# Banner ID: @01318598\n# UTSAid: wgs999\n\n\n\n\n\n# install.packages(\"simstudy\")\n\n\n\n\n\n# library(readxl) #for loading Excel files\n# library(dplyr) #for data processing/cleaning\n# library(tidyr) #for data processing/cleaning\n# library(tidyverse)\n# library(skimr) #for nice visualization of data \n# library(here) #to set paths\n# library(dslabs) #for data used in this assignment\n# library(tibble)\n# library(simstudy)\nlibrary(sqldf)\n\nLoading required package: gsubfn\n\n\nLoading required package: proto\n\n\nLoading required package: RSQLite\n\n\n\n\n\n\n# Define path to .csv file containing data\nZC.filepath &lt;- \"Nutrition__Physical_Activity__and_Obesity_-_Behavioral_Risk_Factor_Surveillance_System_20240703.csv\"\n# The master dataset may undergo dimensional changes throughout this exercise.  The changing dimensions will be tracked over time.  Variables to track these changing dimensions will be created.\nZC.dimindex &lt;- 0\nZC.datarows &lt;- list()\nZC.datacols &lt;- list()\n# There may be columns with data arbitrarily judged to have worthless data.  Variables will be created to store these column names for subsequent examination.\nZC.columnindex &lt;- 0\nZC.examinedcolumns &lt;- list()\n# There may be lookup tables created during this exercise to simplify the master dataset.  When created, the column containing the lookup table and the name of the lookup table will be stored.\nZC.lookupindex &lt;- 0\nZC.lookupcolumn &lt;- list()\nZC.lookuptable &lt;- list()\n\n\n\n\n\n\n\nPreviously, you did a quick exploration of a dataset that came with an R package (gapminder data inside dslabs package). A lot of datasets can be found inside R packages. For instance, this page lists what is likely only a small fractionLinks to an external site.. The good and the bad about datasets that come with R packages is that they are often fairly clean/tidy. That’s unfortunately not how most “real world” datasets look like. Getting dirty and messy datasets and wrangling them into a form that is suitable for statistical analysis is part of most workflows and often takes a lot of time. We’ll start practicing this here by getting data that might or might not be very clean.\nGo to the CDC’s data website at https://data.cdc.gov/Links to an external site.. Browse through the site and identify a dataset of interest.\nWhich dataset you choose is up to you. I suggest you pick a dataset that has at least 100 observations with 5 different variables, and a mix of continuous and categorical ones. Often, 5 variables means 5 columns. That would be the case in properly formatted data. However, some of the data provided by the CDC is rather poorly formatted. For instance CDC’s dataset on traumatic brain injuryLinks to an external site. has the same variable (age) in separate columns, and it is also discretized. As we’ll discuss, these are two really bad things you can do to your data, so I recommend staying away from such datasets. There are plenty on that website, so I’m sure you’ll find one that is suitable and interesting to you.\n\n\n\nTo get the dataset you selected, it is easiest if you download the file to your computer and place it inside your portfolio repository. Note that in general, you should make each data analysis (or other) project its own GitHub repository, and always use a structure like the one provided in the Data Analysis Template (or something similar). However, for this small exercise and for logistic reasons, you’ll use your portfolio/website repository, and just a single folder. Make a new folder called cdcdata-exercise inside your portfolio repository. Place the data into that folder.\nRemember that GitHub doesn’t like large files. So if you pick a large data file (&gt;50MB), first place it somewhere outside your repository, then reduce it by e.g., writing some R code that selects only a portion of the data. Once it’s small enough, you can place it into the GitHub repository.\nWhile you should be able to find data for direct download from the CDC website, sometimes you need to write a bit of code to pull data from a source. This is usually done through an API. R has packages that make this relatively easy. If you ever encounter that situation, search online for instructions. Google/Stackoverflow are your friends to figure out what commands you need to write).\n\n\n\nNow, write code that explores the data. Add a new Quarto document called cdcdata-exercise.qmd to the folder you just created.\nStart by providing a brief description of the data, where you got it, what it contains. Also add a link to the source.\nThis assignment uses a dataset from the United States Center for Disease Control (CDC) titled, “Nutrition, Physical Activity, and Obesity - Behavioral Risk Factor Surveillance System.” The dataset and column definition can be found here.\nThe CDC describes the dataset as follows:\n\nThis dataset includes data on adult’s diet, physical activity, and weight status from Behavioral Risk Factor Surveillance System. This data is used for DNPAO’s Data, Trends, and Maps database, which provides national and state specific data on obesity, nutrition, physical activity, and breastfeeding.\n\n\n\n\n\n\n\nNote\n\n\n\nThe DNPAO is the CDC’s Division of Nutrition, Physical Activity, and Obesity.\n\n\nThen write code that reads/loads the data. As needed, process the data (e.g., if there are weird symbols in the data, or missing values coded as 999 or anything of that sort, write code to fix it.) If your dataset has a lot of variables, pick a few of them (at least 5).\nThe dataset is described by the CDC as 93.2k rows with 33 columns each. For this assignment, the entire dataset will be loaded.\n\n# Track time it takes to load\nZC.START_TIMESTAMP &lt;- proc.time()\n# load the data into a dataframe\nZC.data &lt;- read.csv(ZC.filepath)\nZC.loadtime &lt;- proc.time() - ZC.START_TIMESTAMP\n# Report the load time\nZC.printline &lt;- paste(\"The data took\"\n                      , ZC.loadtime[3]\n                      , \"seconds to load.\"\n                      , sep=\" \"\n)\nZC.printline\n\n[1] \"The data took 1.03 seconds to load.\"\n\n\nThere were no issues encountered while loading the data.\n\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 33 columns.\"\n\n\nTo get an overview of what we have just loaded, a table with summary information about each column is constructed.\n\n# Setup control variables and lists\nZC.index &lt;- 0\nZC.colname &lt;- list()\nZC.coltype &lt;- list()\nZC.NAvals &lt;- list()\nZC.unique &lt;- list()\nZC.example &lt;- list()\n# Loop through the columns to populate the lists\nfor (ZC.ThisColumn in colnames(ZC.data))\n{\n  #Increment the index for the lists\n  ZC.index &lt;- ZC.index+1 \n  # Assign the column name\n  ZC.colname[ZC.index] &lt;- ZC.ThisColumn \n  # Assign the column type\n  ZC.coltype[ZC.index] &lt;- typeof(ZC.data[[ZC.ThisColumn]]) \n  # Count the number of NAs in the column\n  ZC.NAvals[ZC.index] &lt;- sum(is.na(ZC.data[[ZC.ThisColumn]])) \n  # Count the numner of unique values in the column\n  ZC.TheseValues &lt;- table(ZC.data[[ZC.ThisColumn]])\n  ZC.unique[ZC.index] &lt;- length(ZC.TheseValues)\n  # Find a suitable example value\n  # Prepare to loop through all the values to find an acceptable example\n  for (ZC.ThisRow in 1:nrow(ZC.data)) \n  {\n    # Start this FOR loop but prepare to break out of it\n    ZC.BreakFlag = 0\n    # As soon as an acceptable value is found, set the break flag to 1\n    # Retrieve the value in ThisRow\n    ZC.ThisExp &lt;- ZC.data[[ZC.ThisColumn]][ZC.ThisRow]\n    # Continue with this example only if it is not NA\n    if (!is.na(ZC.ThisExp))\n    {\n      # Subsequent processing depends on data type of current example\n      # Is the current example a character string?\n      if (typeof(ZC.ThisExp) == \"character\")\n      {\n        # Trim any leading and trailing spaces from the current example\n        # If there is nothing but spaces, only an empty string will survive\n        ZC.ThisExp &lt;- trimws(ZC.ThisExp)\n        # Continue with this example only if it is not empty\n        if (nchar(ZC.ThisExp)&gt;0)\n        {\n          # We have an acceptable example\n          ZC.BreakFlag &lt;- 1\n          # Let's cut it short if the example is too long\n          if (nchar(ZC.ThisExp)&gt;8)\n          {\n            ZC.ThisExp &lt;- paste(substr(ZC.ThisExp, 1, 5), \"...\", sep=\"\")\n          }\n        }\n      } else if (typeof(ZC.ThisExp) == \"integer\" | typeof(ZC.ThisExp) == \"double\")\n      {\n        # We would prefer a nonzero value, and we will loop through all the values if necessary to find one\n        if (!ZC.ThisExp==0) ZC.BreakFlag &lt;- 1\n      } else\n      { # The value could be boolean, complex, or something else\n        # We will take it\n        ZC.BreakFlag &lt;- 1\n      }\n    }\n    # If we have an acceptable value, we can stop looping through the rows\n    if (ZC.BreakFlag == 1) break\n  }\n  ZC.example[ZC.index] &lt;- ZC.ThisExp\n}\n# Bind the lists together in a single object\nZC.datasum01 &lt;- cbind(ZC.colname, ZC.coltype, ZC.NAvals, ZC.unique, ZC.example)\n# Apply more friendly column names\ncolnames(ZC.datasum01) &lt;- c(\"Column\"\n                          , \"Data Type\"\n                          , \"NA Values\"\n                          , \"Uniques\"\n                          , \"Example\"\n)\nZC.datasum01\n\n      Column                       Data Type   NA Values Uniques Example   \n [1,] \"YearStart\"                  \"integer\"   0         12      2020      \n [2,] \"YearEnd\"                    \"integer\"   0         12      2020      \n [3,] \"LocationAbbr\"               \"character\" 0         55      \"US\"      \n [4,] \"LocationDesc\"               \"character\" 0         55      \"National\"\n [5,] \"Datasource\"                 \"character\" 0         2       \"Behav...\"\n [6,] \"Class\"                      \"character\" 0         3       \"Physi...\"\n [7,] \"Topic\"                      \"character\" 0         3       \"Physi...\"\n [8,] \"Question\"                   \"character\" 0         9       \"Perce...\"\n [9,] \"Data_Value_Unit\"            \"logical\"   93249     0       NA        \n[10,] \"Data_Value_Type\"            \"character\" 0         1       \"Value\"   \n[11,] \"Data_Value\"                 \"double\"    9235      695     30.6      \n[12,] \"Data_Value_Alt\"             \"double\"    9235      695     30.6      \n[13,] \"Data_Value_Footnote_Symbol\" \"character\" 0         2       \"~\"       \n[14,] \"Data_Value_Footnote\"        \"character\" 0         2       \"Data ...\"\n[15,] \"Low_Confidence_Limit\"       \"double\"    9235      666     29.4      \n[16,] \"High_Confidence_Limit\"      \"double\"    9235      761     31.8      \n[17,] \"Sample_Size\"                \"integer\"   9235      9761    31255     \n[18,] \"Total\"                      \"character\" 0         2       \"Total\"   \n[19,] \"Age.years.\"                 \"character\" 0         7       \"25 - 34\" \n[20,] \"Education\"                  \"character\" 0         5       \"High ...\"\n[21,] \"Gender\"                     \"character\" 0         3       \"Female\"  \n[22,] \"Income\"                     \"character\" 0         8       \"$50,0...\"\n[23,] \"Race.Ethnicity\"             \"character\" 0         9       \"Hispanic\"\n[24,] \"GeoLocation\"                \"character\" 0         106     \"(13.4...\"\n[25,] \"ClassID\"                    \"character\" 0         3       \"PA\"      \n[26,] \"TopicID\"                    \"character\" 0         3       \"PA1\"     \n[27,] \"QuestionID\"                 \"character\" 0         9       \"Q047\"    \n[28,] \"DataValueTypeID\"            \"character\" 0         1       \"VALUE\"   \n[29,] \"LocationID\"                 \"integer\"   0         55      59        \n[30,] \"StratificationCategory1\"    \"character\" 0         7       \"Race/...\"\n[31,] \"Stratification1\"            \"character\" 0         29      \"Hispanic\"\n[32,] \"StratificationCategoryId1\"  \"character\" 0         7       \"RACE\"    \n[33,] \"StratificationID1\"          \"character\" 0         29      \"RACEHIS\" \n\n\nThe above output stipulates that the values in Data_Value_Unit are worthless, confirmed by two (2) distinct findings:\n\nall 93,239 values are “NA”\nzero (0) unique values\n\nAdditionally, there is only one unique value in the following columns:\n\nData_Value_Type\nDataValueTypeID\n\nTherefore, all three (3) of these columns will be dropped from the dataframe.\n\nZC.data &lt;- subset(ZC.data, select = -c(Data_Value_Unit, Data_Value_Type, DataValueTypeID))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 30 columns.\"\n\n\nThere are also some column with unique counts so low they appear to offer little or no value. They will be dropped if they are so judged to be unvalued. The columns to be examined are:\n\nDatasource\nData_Value_Footnote_Symbol\nData_Value_Footnote\nTotal\n\nFor each of these columns, the distinct values will be displayed for evaluation.\n\n# create list of columns to be examined\nZC.examinedcolumns[1] &lt;- \"Datasource\"\nZC.examinedcolumns[2] &lt;- \"Data_Value_Footnote_Symbol\"\nZC.examinedcolumns[3] &lt;- \"Data_Value_Footnote\"\nZC.examinedcolumns[4] &lt;- \"Total\"\n# loop through the columns to be examined, building and displaying a temporary table of the unique values and their counts.\nfor (ZC.ExamineThis in ZC.examinedcolumns)\n{\n  # create section separator\n  ZC.printline &lt;- \"********************************\"\n  print (ZC.printline)\n  # build SQL statement syntax\n  ZC.RunThis &lt;- paste(\"select \"\n                      , ZC.ExamineThis\n                      ,\", count(*) as Occurs from 'ZC.data' group by \"\n                      , ZC.ExamineThis\n                      , sep = \"\")\n  # display SQL statement for posterity\n  ZC.printline &lt;- paste(\"Running: \"\n                        , ZC.RunThis\n                        , sep = \"\")\n  print(ZC.printline)\n  # load results into temporary table\n  ZC.ThisTable &lt;- sqldf(ZC.RunThis)\n  # display contents of temporary table\n  ZC.printline &lt;- paste(\"Unique Values of \"\n                        , ZC.ExamineThis\n                        , \":\"\n                        , sep=\"\"\n                        )\n  print(ZC.printline)\n  # display results of temporary table\n  print(ZC.ThisTable)\n}\n\n[1] \"********************************\"\n[1] \"Running: select Datasource, count(*) as Occurs from 'ZC.data' group by Datasource\"\n[1] \"Unique Values of Datasource:\"\n                                  Datasource Occurs\n1                                      BRFSS   4620\n2 Behavioral Risk Factor Surveillance System  88629\n[1] \"********************************\"\n[1] \"Running: select Data_Value_Footnote_Symbol, count(*) as Occurs from 'ZC.data' group by Data_Value_Footnote_Symbol\"\n[1] \"Unique Values of Data_Value_Footnote_Symbol:\"\n  Data_Value_Footnote_Symbol Occurs\n1                             84014\n2                          ~   9235\n[1] \"********************************\"\n[1] \"Running: select Data_Value_Footnote, count(*) as Occurs from 'ZC.data' group by Data_Value_Footnote\"\n[1] \"Unique Values of Data_Value_Footnote:\"\n                                      Data_Value_Footnote Occurs\n1                                                          84014\n2 Data not available because sample size is insufficient.   9235\n[1] \"********************************\"\n[1] \"Running: select Total, count(*) as Occurs from 'ZC.data' group by Total\"\n[1] \"Unique Values of Total:\"\n  Total Occurs\n1        89919\n2 Total   3330\n\n\nBased on the above results, Datasource will be dropped immediately, as the unique values seem synonyms for each other, so there is really only one value in the column. The other three (3) columns require additional consideration.\n\nZC.data &lt;- subset(ZC.data, select = -c(Datasource))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 29 columns.\"\n\n\nThe Total column unique values don’t seem to tell much. The CDC describes the data in this column as:\n\nTotal/Overall breakout category\n\nGiven the vagueness of the description and the unique values, the Total column will also be dropped.\n\nZC.data &lt;- subset(ZC.data, select = -c(Total))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 28 columns.\"\n\n\nThe GeoLocation column appears to have highly-coded data. The CDC describes this column as:\n\nLatitude & Longitude to be provided for formatting GeoLocation or Geocode in the format (latitude, longitude)\n\nTherefore, this column will also be dropped.\n\nZC.data &lt;- subset(ZC.data, select = -c(GeoLocation))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 27 columns.\"\n\n\nThe count of the only nonblank value in both the Data_Value_Footnote_Symbol and Data_Value_Footnote have been seen before: coincidence or not, there are the exact name number of NA values in the Data_Value and Data_Value_Alt columns. If it turns out there is a meaningful value in Data_Value_Footnote_Symbol and Data_Value_Footnote for each NA in Data_Value and Data_Value_Alt, then the Data_Value_Footnote_Symbol and Data_Value_Footnote columns can be considered duplicative information for NA values in the Data_Value and Data_Value_Alt columns.\nTo examine, we will build lists of the rows where the following conditions occur, and then compare them to see if they are equal:\n\nWhere Data_Value_Footnote_Symbol is not blank\nWhere Data_Value_Footnote is not blank\nWhere Data_Value is NA\nWhere Data_Value_Alt is NA\n\n\n# create lists to store row numbers where specified conditions occur\n# store rows where Data_Value_Footnote_Symbol is not blank\nZC.DVFS &lt;- list() \n# store rows where Data_Value_Footnote is not blank\nZC.DVF &lt;- list() \n# store rows where Data_Value is NA\nZC.DV &lt;- list() \n# store rows where Data_Value_Alt is NA\nZC.DVA &lt;- list() \n# create index variable for each list\nZC.DVFSIndex &lt;- 0\nZC.DVFIndex &lt;- 0\nZC.DVIndex &lt;- 0\nZC.DVAIndex &lt;- 0\n# loop through the rows in the master dataset and store indices where conditions are met\nZC.RowRange &lt;- 1:nrow(ZC.data)\nfor (ZC.CheckRow in ZC.RowRange)\n{\n  # pull Data_Value_Footnote_Symbol value in current row\n  ZC.ThisValue &lt;- ZC.data$Data_Value_Footnote_Symbol[ZC.CheckRow]\n  if (!(is.null(ZC.ThisValue)))\n  {\n    # Value is not NULL\n    # But is it empty or blank?\n    ZC.ThisValue &lt;- trimws(ZC.ThisValue)\n    if (nchar(ZC.ThisValue)&gt;0)\n    {\n      # value is not blank, so store the row number\n      ZC.DVFSIndex &lt;- ZC.DVFSIndex+1\n      ZC.DVFS[ZC.DVFSIndex] &lt;- ZC.CheckRow\n    }\n  }\n  # pull Data_Value_Footnote value in current row\n  ZC.ThisValue &lt;- ZC.data$Data_Value_Footnote[ZC.CheckRow]\n  if (!(is.null(ZC.ThisValue)))\n  {\n    # Value is not NULL\n    # But is it empty or blank?\n    ZC.ThisValue &lt;- trimws(ZC.ThisValue)\n    if (nchar(ZC.ThisValue)&gt;0)\n    {\n      # value is not blank, so store the row number\n      ZC.DVFIndex &lt;- ZC.DVFIndex+1\n      ZC.DVF[ZC.DVFIndex] &lt;- ZC.CheckRow\n    }\n  }\n  # is Data_Value an NA value?\n  if (is.na(ZC.data$Data_Value[ZC.CheckRow]))\n  {\n      # value is NA, so store the row number\n      ZC.DVIndex &lt;- ZC.DVIndex+1\n      ZC.DV[ZC.DVIndex] &lt;- ZC.CheckRow\n  }\n  # is Data_Value_Alt an NA value?\n  if (is.na(ZC.data$Data_Value_Alt[ZC.CheckRow]))\n  {\n      # value is NA, so store the row number\n      ZC.DVAIndex &lt;- ZC.DVAIndex+1\n      ZC.DVA[ZC.DVAIndex] &lt;- ZC.CheckRow\n  }\n}\n\nNow that the lists of the row numbers where the conditions occur have been built, they need to be compared. They only need to be compared if they are all the same length, so that will be checked first.\n\n# Compare the list of row numbers where conditions were met\n# Are all the lists of the same length?  No need to compare if not.\n# Create an indicator flag to indicate if the row numbers match.\n# They can't match if the count of row numbers does not match, so assume it is FALSE\nZC.CompEquals &lt;- FALSE\nif (ZC.DVFSIndex==ZC.DVFIndex&ZC.DVFIndex==ZC.DVIndex&ZC.DVIndex==ZC.DVAIndex)\n{\n  # The lists are the same length.  A loop will be used to compare value by value.\n  # No need to continue the loop as soon as a difference if found\n  # Set the indicator to TRUE; change to FALSE if an unequal row number is found\n  ZC.CompEquals &lt;- TRUE\n  # The row numbers in the list are actually embedded inside lists of one (1) item. \n  ZC.CompRange &lt;- 1:ZC.DVIndex\n  for (ZC.CompRow in ZC.CompRange)\n  {\n    # The row numbers in the list are actually embedded inside lists of one (1) item. \n    # We'll break each value out of it's list before we compare.\n    ZC.ThisDVFS &lt;- unlist(ZC.DVFS[ZC.CompRow])\n    ZC.ThisDVF &lt;- unlist(ZC.DVF[ZC.CompRow])\n    ZC.ThisDV &lt;- unlist(ZC.DV[ZC.CompRow])\n    ZC.ThisDVA &lt;- unlist(ZC.DVA[ZC.CompRow])\n    if(!(ZC.ThisDVFS==ZC.ThisDVF&ZC.ThisDVF==ZC.ThisDV&ZC.ThisDV==ZC.ThisDVA))\n    {\n      # We found a row number that did not match.  Set indicator value to FALSE and stop\n      ZC.CompEquals &lt;- FALSE\n      break\n    }\n  }\n}\nZC.ThisDVFS # TDL\n\n[1] 93241\n\ntypeof(ZC.ThisDVFS) #TDL\n\n[1] \"integer\"\n\nZC.CompRow # TDL\n\n[1] 9235\n\ntypeof(ZC.CompRow) #TDL\n\n[1] \"integer\"\n\nZC.DVFS[ZC.CompRow] # TDL\n\n[[1]]\n[1] 93241\n\ntypeof(ZC.DVFS[ZC.CompRow]) # TDL\n\n[1] \"list\"\n\nZC.DVF[ZC.CompRow] #TDL\n\n[[1]]\n[1] 93241\n\ntypeof(ZC.DVF[ZC.CompRow]) # TDL\n\n[1] \"list\"\n\n# Did the indicator flag survive every row number comparison?\nif (ZC.CompEquals)\n{\n  print(\"Row numbers where conditions are met match.\")\n} else\n{\n  print(\"Row numbers where conditions are met do not match.\")\n}\n\n[1] \"Row numbers where conditions are met match.\"\n\n\nThe lists of row numbers where the conditions were met match exactly. Therefore, the data in columns Data_Value_Footnote_Symbol and Data_Value_Footnote can be considered redundant for cases when the values in Data_Value and Data_Value_Alt are NA. Having already established that the data in columns Data_Value_Footnote_Symbol and Data_Value_Footnote is blank when the values in Data_Value and Data_Value_Alt are not NA, we can remove the columns with blank/redundant data.\n\nZC.data &lt;- subset(ZC.data, select = -c(Data_Value_Footnote_Symbol, Data_Value_Footnote))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 25 columns.\"\n\n\nNext, the count of unique values, coupled with the column names, suggests a one-to-one relationship between the data in one column and the data in another. In proven true, we can reduce repetitiveness by archiving off the duplicated data into a separate lookup table. Column combinations to consider are:\n\nLocationID, a potential lookup for both LocationAbbr and LocationDesc\nStratificationCategoryId1, a potential lookup for StratificationCategory1\nStratificationID1, a potential lookup for Stratification1\n\nThe validity of an index/lookup relationship between the above potentials will be explored, beginning with LocationID.\nA SQL statement will be utilized to count the number of times each combination of the LocationID, LocationAbbr, and LocationDesc occurs in the data. Since there are 55 unique values in each of these columns, if there are a total of 55 combinations, then a one-to-one(-to-one) correlation is established.\n\nsqldf(\"select LocationID, LocationAbbr, LocationDesc, count(*) as Occurs from 'ZC.data' group by LocationID, LocationAbbr, LocationDesc\")\n\n   LocationID LocationAbbr         LocationDesc Occurs\n1           1           AL              Alabama   1736\n2           2           AK               Alaska   1736\n3           4           AZ              Arizona   1736\n4           5           AR             Arkansas   1736\n5           6           CA           California   1736\n6           8           CO             Colorado   1736\n7           9           CT          Connecticut   1736\n8          10           DE             Delaware   1736\n9          11           DC District of Columbia   1736\n10         12           FL              Florida   1736\n11         13           GA              Georgia   1736\n12         15           HI               Hawaii   1736\n13         16           ID                Idaho   1736\n14         17           IL             Illinois   1736\n15         18           IN              Indiana   1736\n16         19           IA                 Iowa   1736\n17         20           KS               Kansas   1736\n18         21           KY             Kentucky   1736\n19         22           LA            Louisiana   1736\n20         23           ME                Maine   1736\n21         24           MD             Maryland   1736\n22         25           MA        Massachusetts   1736\n23         26           MI             Michigan   1736\n24         27           MN            Minnesota   1736\n25         28           MS          Mississippi   1736\n26         29           MO             Missouri   1736\n27         30           MT              Montana   1736\n28         31           NE             Nebraska   1736\n29         32           NV               Nevada   1736\n30         33           NH        New Hampshire   1736\n31         34           NJ           New Jersey   1493\n32         35           NM           New Mexico   1736\n33         36           NY             New York   1736\n34         37           NC       North Carolina   1736\n35         38           ND         North Dakota   1736\n36         39           OH                 Ohio   1736\n37         40           OK             Oklahoma   1736\n38         41           OR               Oregon   1736\n39         42           PA         Pennsylvania   1736\n40         44           RI         Rhode Island   1736\n41         45           SC       South Carolina   1736\n42         46           SD         South Dakota   1736\n43         47           TN            Tennessee   1736\n44         48           TX                Texas   1736\n45         49           UT                 Utah   1736\n46         50           VT              Vermont   1736\n47         51           VA             Virginia   1736\n48         53           WA           Washington   1736\n49         54           WV        West Virginia   1736\n50         55           WI            Wisconsin   1736\n51         56           WY              Wyoming   1736\n52         59           US             National   1736\n53         66           GU                 Guam   1260\n54         72           PR          Puerto Rico   1316\n55         78           VI       Virgin Islands    644\n\n\nThere are only 55 total unique combinations for these three (3) variables, so a lookup table for LocationAbbr and another for LocationDesc will be created, both with LocationID as the lookup value. The lookup value and the first lookup table will be stored as a relationship. Afterwards, the columns containing the lookup values will be dropped.\n\n# Creating and displaying top of lookup tables\nZC.lookupLocationAbbr &lt;- sqldf(\"select distinct LocationID, LocationAbbr from 'ZC.data' order by LocationID\")\nhead(ZC.lookupLocationAbbr)\n\n  LocationID LocationAbbr\n1          1           AL\n2          2           AK\n3          4           AZ\n4          5           AR\n5          6           CA\n6          8           CO\n\nZC.lookupLocationDesc &lt;- sqldf(\"select distinct LocationID, LocationDesc from 'ZC.data' order by LocationID\")\nhead(ZC.lookupLocationDesc)\n\n  LocationID LocationDesc\n1          1      Alabama\n2          2       Alaska\n3          4      Arizona\n4          5     Arkansas\n5          6   California\n6          8     Colorado\n\n# Storing lookup columns and lookup table names\nZC.lookupindex &lt;- ZC.lookupindex+1\nZC.lookupcolumn[ZC.lookupindex] &lt;- \"LocationID\"\nZC.lookuptable[ZC.lookupindex] &lt;- \"LocationAbbr\"\nZC.data &lt;- subset(ZC.data, select = -c(LocationAbbr, LocationDesc))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 23 columns.\"\n\n\nA SQL statement will be utilized to count the number of times each combination of the StratificationCategoryId1 and StratificationCategory1 occurs in the data. Since there are seven (7) unique values in each of these columns, if there are a total of seven (7) combinations, then a one-to-one correlation is established.\n\nsqldf(\"select StratificationCategoryId1, StratificationCategory1, count(*) as Occurs from 'ZC.data' group by StratificationCategoryId1, StratificationCategory1\")\n\n  StratificationCategoryId1 StratificationCategory1 Occurs\n1                                                        9\n2                     AGEYR             Age (years)  19980\n3                       EDU               Education  13320\n4                       GEN                  Gender   6660\n5                       INC                  Income  23310\n6                       OVR                   Total   3330\n7                      RACE          Race/Ethnicity  26640\n\n\nThere are only seven (7) total unique combinations for these two (2) variables, so a lookup table for StratificationCategory1 will be created with StratificationCategoryId1 as the lookup value. The lookup value and the lookup table will be stored as a relationship. Afterwards, the columns containing the lookup values will be dropped.\n\n# Creating and displaying top of lookup tables\nZC.StratificationCategory1 &lt;- sqldf(\"select distinct StratificationCategoryId1, StratificationCategory1 from 'ZC.data' order by StratificationCategoryId1\")\nhead(ZC.StratificationCategory1)\n\n  StratificationCategoryId1 StratificationCategory1\n1                                                  \n2                     AGEYR             Age (years)\n3                       EDU               Education\n4                       GEN                  Gender\n5                       INC                  Income\n6                       OVR                   Total\n\n# Storing lookup columns and lookup table names\nZC.lookupindex &lt;- ZC.lookupindex+1\nZC.lookupcolumn[ZC.lookupindex] &lt;- \"StratificationCategoryId1\"\nZC.lookuptable[ZC.lookupindex] &lt;- \"StratificationCategory1\"\nZC.data &lt;- subset(ZC.data, select = -c(StratificationCategory1))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 22 columns.\"\n\n\nA SQL statement will be utilized to count the number of times each combination of the StratificationID1 and Stratification1 occurs in the data. Since there are 29 unique values in each of these columns, if there are a total of 29 combinations, then a one-to-one correlation is established.\n\nsqldf(\"select StratificationID1, Stratification1, count(*) as Occurs from 'ZC.data' group by StratificationID1, Stratification1\")\n\n   StratificationID1                  Stratification1 Occurs\n1                                                          9\n2          AGEYR1824                          18 - 24   3330\n3          AGEYR2534                          25 - 34   3330\n4          AGEYR3544                          35 - 44   3330\n5          AGEYR4554                          45 - 54   3330\n6          AGEYR5564                          55 - 64   3330\n7        AGEYR65PLUS                      65 or older   3330\n8          EDUCOGRAD                 College graduate   3330\n9           EDUCOTEC Some college or technical school   3330\n10             EDUHS            Less than high school   3330\n11         EDUHSGRAD             High school graduate   3330\n12            FEMALE                           Female   3330\n13           INC1525                $15,000 - $24,999   3330\n14           INC2535                $25,000 - $34,999   3330\n15           INC3550                $35,000 - $49,999   3330\n16           INC5075                $50,000 - $74,999   3330\n17         INC75PLUS               $75,000 or greater   3330\n18         INCLESS15                Less than $15,000   3330\n19             INCNR                Data not reported   3330\n20              MALE                             Male   3330\n21           OVERALL                            Total   3330\n22         RACE2PLUS                  2 or more races   3330\n23           RACEASN                            Asian   3330\n24           RACEBLK               Non-Hispanic Black   3330\n25           RACEHIS                         Hispanic   3330\n26           RACEHPI        Hawaiian/Pacific Islander   3330\n27           RACENAA    American Indian/Alaska Native   3330\n28           RACEOTH                            Other   3330\n29           RACEWHT               Non-Hispanic White   3330\n\n\nThere are only 29 total unique combinations for these two (2) variables, so a lookup table for Stratification1 will be created with StratificationID1 as the lookup value. The lookup value and the lookup table will be stored as a relationship. Afterwards, the columns containing the lookup values will be dropped.\n\n# Creating and displaying top of lookup tables\nZC.Stratification1 &lt;- sqldf(\"select distinct StratificationID1, Stratification1 from 'ZC.data' order by StratificationID1\")\nhead(ZC.Stratification1)\n\n  StratificationID1 Stratification1\n1                                  \n2         AGEYR1824         18 - 24\n3         AGEYR2534         25 - 34\n4         AGEYR3544         35 - 44\n5         AGEYR4554         45 - 54\n6         AGEYR5564         55 - 64\n\n# Storing lookup columns and lookup table names\nZC.lookupindex &lt;- ZC.lookupindex+1\nZC.lookupcolumn[ZC.lookupindex] &lt;- \"StratificationID1\"\nZC.lookuptable[ZC.lookupindex] &lt;- \"Stratification1\"\nZC.data &lt;- subset(ZC.data, select = -c(Stratification1))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 21 columns.\"\n\n\nSimilarly, it’s possible YearStart always equals YearEnd, Class always equals Topic, and Data_Value always equals Data_Value_Alt. Code will be executed to see if these possible pairs always duplicate a value in the other; if so, the duplicate column will be dropped.\nSince there are NA values in both the Data_Value and Data_Value_Alt, and R code will halt if it tries to compare an NA with anything (even another NA value), this comparison will be conditional. For the purposes of this exercise:\n\nif both values are NA, the values are considered equal (but a comparison cannot be attempted by R).\nif one value is NA but the other is not, the values are considered not equal.\nif neither value is NA, a comparison will be performed to determine if the values are equal or not.\n\n\n# Set up variables to count differences\nZC.YearDeltas &lt;- 0\nZC.ClassDeltas &lt;- 0\nZC.DataDeltas &lt;- 0\nfor (ZC.ThisRow in 1:nrow(ZC.data))\n{\n  # Count if YearStart and YearEnd are different values\n  if (!(ZC.data$YearStart[ZC.ThisRow]==ZC.data$YearEnd[ZC.ThisRow]))\n    {\n      # Brackets not necessary; just respecting page margins\n      ZC.YearDeltas &lt;- ZC.YearDeltas+1\n    }\n  # Count if Class and Topic are different values\n  if(!(ZC.data$Class[ZC.ThisRow]==ZC.data$Topic[ZC.ThisRow]))\n    {\n      # Brackets not necessary; just respecting page margins\n      ZC.ClassDeltas &lt;- ZC.ClassDeltas+1\n    }\n  # There could be NA values for either Data_Value and/or Data_Value_Alt\n  # If both are NA, there is no difference\n  # If only one is NA, there is a difference\n  # Only compare if neither are NA\n  if (is.na(ZC.data$Data_Value[ZC.ThisRow])==is.na(ZC.data$Data_Value_Alt[ZC.ThisRow]))\n  {\n    # Either both are NA or neither or NA\n    # Comparison is not necessary (or valid!) if both are NA\n    if (!(is.na(ZC.data$Data_Value[ZC.ThisRow])))\n    {\n      # Neither are NA - compare values\n      if (!(ZC.data$Data_Value[ZC.ThisRow]==ZC.data$Data_Value_Alt[ZC.ThisRow]))\n      {\n        # Values are not equal\n        ZC.DataDeltas &lt;- ZC.DataDeltas+1\n      }\n    }\n  }\n  else\n  {\n    # One value is NA but not the other, so they are automatically not equal\n    ZC.DataDeltas &lt;- ZC.DataDeltas+1\n  }\n}\n# Report on deltas\nZC.printline &lt;- paste(\"There were\"\n                      , ZC.YearDeltas\n                      , \"row(s) with different values in YearStart and YearEnd.\"\n                      , sep = \" \")\nZC.printline\n\n[1] \"There were 0 row(s) with different values in YearStart and YearEnd.\"\n\nZC.printline &lt;- paste(\"There were\"\n                      , ZC.ClassDeltas\n                      , \"row(s) with different values in Class and Topic.\"\n                      , sep = \" \")\nZC.printline\n\n[1] \"There were 57015 row(s) with different values in Class and Topic.\"\n\nZC.printline &lt;- paste(\"There were\"\n                      , ZC.DataDeltas\n                      , \"row(s) with different values in Data_Value and Data_Value_Alt.\"\n                      , sep = \" \")\nZC.printline\n\n[1] \"There were 0 row(s) with different values in Data_Value and Data_Value_Alt.\"\n\n# Drop YearEnd if it always equals YearStart\nif (ZC.YearDeltas==0) \n  {\n    print(\"Dropping column YearEnd\")\n    ZC.data &lt;- subset(ZC.data, select = -c(YearEnd))\n  }\n\n[1] \"Dropping column YearEnd\"\n\n# Drop Topic if it always equals Class\nif (ZC.ClassDeltas==0) \n  {\n    print(\"Dropping column Topic\")\n    ZC.data &lt;- subset(ZC.data, select = -c(Topic))\n  }\n# Drop Data_Value_Alt if it always equals Data_Value\nif (ZC.DataDeltas==0)\n  {\n    print(\"Dropping column Data_Value_Alt\")\n    ZC.data &lt;- subset(ZC.data, select = -c(Data_Value_Alt))\n  }\n\n[1] \"Dropping column Data_Value_Alt\"\n\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 19 columns.\"\n\n\nFinally, some column names do not play well with SQL (when they contain unallowed characters), which will be used later to build the data for the charts. So, there will be a few manual manipulations of some column names:\n\nAge.years. will become AgeYears\nRace.Ethnicity will become RaceEthnic\n\n\n# change column names to be more SQL-friendly\nprint(typeof(ZC.data)) # TDL\n\n[1] \"list\"\n\nnames(ZC.data)[names(ZC.data) == 'Age.years.'] &lt;- 'AgeYears'\nnames(ZC.data)[names(ZC.data) == 'Race.Ethnicity'] &lt;- 'RaceEthnic'\n\nThe summary table will be rebuilt before continuing. This time, a new column, LU (for lookup) will be created. If a lookup table has been created for the master dataset column, LU will be set to one (1); otherwise, it will be set to zero (0).\n\n# Setup control variables and lists\nZC.index &lt;- 0\nZC.colname &lt;- list()\nZC.coltype &lt;- list()\nZC.NAvals &lt;- list()\nZC.unique &lt;- list()\nZC.lookups &lt;- list()\nZC.example &lt;- list()\n# Loop through the columns to populate the lists\nfor (ZC.ThisColumn in colnames(ZC.data))\n{\n  #Increment the index for the lists\n  ZC.index &lt;- ZC.index+1 \n  # Assign the column name\n  ZC.colname[ZC.index] &lt;- ZC.ThisColumn \n  # Assign the column type\n  ZC.coltype[ZC.index] &lt;- typeof(ZC.data[[ZC.ThisColumn]]) \n  # Count the number of NAs in the column\n  ZC.NAvals[ZC.index] &lt;- sum(is.na(ZC.data[[ZC.ThisColumn]])) \n  # Count the number of unique values in the column\n  ZC.TheseValues &lt;- table(ZC.data[[ZC.ThisColumn]])\n  ZC.unique[ZC.index] &lt;- length(ZC.TheseValues)\n  # Determine if the column has an associated lookup table\n  # Assume there is no lookup table\n  ZC.lookups[ZC.index] &lt;- 0\n  # Search for the column in the list of columns with associated lookup tables\n  # Flag it is the column is found to be associated with a lookup table\n  for (ZC.ThisLookupColumn in ZC.lookupcolumn)\n  {\n    if (ZC.ThisLookupColumn==ZC.colname[ZC.index]) \n    {\n      # Column found, flag it as having a lookup table association\n      ZC.lookups[ZC.index] &lt;- 1\n    }\n  }\n  # Find a suitable example value\n  # Prepare to loop through all the values to find an acceptable example\n  for (ZC.ThisRow in 1:nrow(ZC.data)) \n  {\n    # Start this FOR loop but prepare to break out of it\n    ZC.BreakFlag = 0\n    # As soon as an acceptable value is found, set the break flag to 1\n    # Retrieve the value in ThisRow\n    ZC.ThisExp &lt;- ZC.data[[ZC.ThisColumn]][ZC.ThisRow]\n    # Continue with this example only if it is not NA\n    if (!is.na(ZC.ThisExp))\n    {\n      # Subsequent processing depends on data type of current example\n      # Is the current example a character string?\n      if (typeof(ZC.ThisExp) == \"character\")\n      {\n        # Trim any leading and trailing spaces from the current example\n        # If there is nothing but spaces, only an empty string will survive\n        ZC.ThisExp &lt;- trimws(ZC.ThisExp)\n        # Continue with this example only if it is not empty\n        if (nchar(ZC.ThisExp)&gt;0)\n        {\n          # We have an acceptable example\n          ZC.BreakFlag &lt;- 1\n          # Let's cut it short if the example is too long\n          if (nchar(ZC.ThisExp)&gt;8)\n          {\n            ZC.ThisExp &lt;- paste(substr(ZC.ThisExp, 1, 5), \"...\", sep=\"\")\n          }\n        }\n      } else if (typeof(ZC.ThisExp) == \"integer\" | typeof(ZC.ThisExp) == \"double\")\n      {\n        # We would prefer a nonzero value, and we will loop through all the values if necessary to find one\n        if (!ZC.ThisExp==0) ZC.BreakFlag &lt;- 1\n      } else\n      { # The value could be boolean, complex, or something else\n        # We will take it\n        ZC.BreakFlag &lt;- 1\n      }\n    }\n    # If we have an acceptable value, we can stop looping through the rows\n    if (ZC.BreakFlag == 1) break\n  }\n  ZC.example[ZC.index] &lt;- ZC.ThisExp\n}\n# Bind the lists together in a single object\nZC.datasum02 &lt;- cbind(ZC.colname, ZC.coltype, ZC.NAvals, ZC.unique, ZC.lookups, ZC.example)\n# Apply more friendly column names\ncolnames(ZC.datasum02) &lt;- c(\"Column\"\n                          , \"Data Type\"\n                          , \"NA Values\"\n                          , \"Uniques\"\n                          , \"LU\"\n                          , \"Example\"\n)\nZC.datasum02\n\n      Column                      Data Type   NA Values Uniques LU Example   \n [1,] \"YearStart\"                 \"integer\"   0         12      0  2020      \n [2,] \"Class\"                     \"character\" 0         3       0  \"Physi...\"\n [3,] \"Topic\"                     \"character\" 0         3       0  \"Physi...\"\n [4,] \"Question\"                  \"character\" 0         9       0  \"Perce...\"\n [5,] \"Data_Value\"                \"double\"    9235      695     0  30.6      \n [6,] \"Low_Confidence_Limit\"      \"double\"    9235      666     0  29.4      \n [7,] \"High_Confidence_Limit\"     \"double\"    9235      761     0  31.8      \n [8,] \"Sample_Size\"               \"integer\"   9235      9761    0  31255     \n [9,] \"AgeYears\"                  \"character\" 0         7       0  \"25 - 34\" \n[10,] \"Education\"                 \"character\" 0         5       0  \"High ...\"\n[11,] \"Gender\"                    \"character\" 0         3       0  \"Female\"  \n[12,] \"Income\"                    \"character\" 0         8       0  \"$50,0...\"\n[13,] \"RaceEthnic\"                \"character\" 0         9       0  \"Hispanic\"\n[14,] \"ClassID\"                   \"character\" 0         3       0  \"PA\"      \n[15,] \"TopicID\"                   \"character\" 0         3       0  \"PA1\"     \n[16,] \"QuestionID\"                \"character\" 0         9       0  \"Q047\"    \n[17,] \"LocationID\"                \"integer\"   0         55      1  59        \n[18,] \"StratificationCategoryId1\" \"character\" 0         7       1  \"RACE\"    \n[19,] \"StratificationID1\"         \"character\" 0         29      1  \"RACEHIS\" \n\nprint(typeof(ZC.datasum02)) # TDL\n\n[1] \"list\"\n\n\nOnce you have the data processed and cleaned, perform some exploratory/descriptive analysis on this cleaned dataset. Make some summary tables, produce some figures. Try to summarize each variable in a way that it can be described by a distribution. For instance if you have a categorical variable, show what % are in each category. If you have a continuous variable, make a plot to see if it’s approximately normal, then try to summarize it to determine its mean and standard deviation.\nThe idea is that your descriptive analysis will provide enough information for your classmate to make synthetic data that looks similar, along the lines discussed in the synthetic data module.\nRemember to add both text to your Quarto file and comments into your code to explain what you are doing.\nNow that the dataset has been whittled down to nineteen (19) pertinent columns, graphics for each column will be generated. Using the summary table from above, chart type will be chosen as follows:\n\nAll columns with ten (10) or less unique values will be represented by a pie chart.\nAll columns with more than ten (10) but with twenty (20) or less unique values will be represented by a histogram (bar chart).\nAll other columns will be represented by a line plot.\n\nAdditionally, where the data is predetermined to be a continuous variable, a standard statistical summary will also be produced.\n\n\n# Convert summary table from list to dataframe\nZC.Summary &lt;- as.data.frame(ZC.datasum02)\n# Create list of columns predetermined to contain values in a continuous variable\nZC.StatsPlease &lt;- c(\"Data_Value\"\n                    , \"Low_Confidence_Limit\"\n                    , \"High_Confidence_Limit\"\n                    , \"Sample_Size\")\nZC.StatsPlease &lt;- unlist(ZC.StatsPlease)\nprint(ZC.StatsPlease) # TDL\n\n[1] \"Data_Value\"            \"Low_Confidence_Limit\"  \"High_Confidence_Limit\"\n[4] \"Sample_Size\"          \n\nprint(typeof(ZC.StatsPlease)) # TDL\n\n[1] \"character\"\n\n# Loop through each of the columns\nZC.ChartColumns &lt;- 1:nrow(ZC.Summary)\nfor (ZC.ChartThis in ZC.ChartColumns)\n{\n  # Each value loaded is embedded in a one-item list, so it has to be unlisted before using\n  # Load column name\n  ZC.ThisColumn &lt;- ZC.Summary$Column[ZC.ChartThis]\n  ZC.ThisColumn &lt;- unlist(ZC.ThisColumn)\n  # Load unique column count - determines chart type\n  ZC.ValueCount &lt;- ZC.Summary$Uniques[ZC.ChartThis]\n  ZC.ValueCount &lt;- unlist(ZC.ValueCount)\n  # source of chart data will be the same regardless of chart type\n  ZC.ThisQuery &lt;- paste (\"select \"\n                           , ZC.ThisColumn\n                           , \" as Labels, count(*) as Occurs from 'ZC.data' \"\n                           , \"group by \"\n                           , ZC.ThisColumn\n                           , \" order by \"\n                           , ZC.ThisColumn\n                           , sep=\"\")\n    # Run the Query\n    ZC.DataTable &lt;- sqldf(ZC.ThisQuery)\n    ZC.DataTable &lt;- as.data.frame(ZC.DataTable)\n    ZC.printline &lt;- paste(\"Data for \"\n                          , ZC.ThisColumn\n                          , \":\"\n                          , sep=\"\") # TDL\n    print(ZC.printline) # TDL\n    if(!(ZC.ValueCount&gt;20))\n      print(ZC.DataTable) # TDL\n    else\n    {\n      ZC.printline &lt;- paste(\"Data suppressed.  \"\n                            , ZC.ValueCount\n                            , \" rows.\"\n                            , sep=\"\"\n      )\n      print(ZC.printline)\n    }\n  if (!(ZC.ValueCount&gt;10))\n  {\n    # Drawing a pie chart\n  } else if (!(ZC.ValueCount&gt;20))\n  {\n    # building a histogram\n  } else\n  {\n    # Drawing a distribution plot\n  }\n  # printing summary statistics?\n    # if (grepl(ZC.ThisColumn, ZC.StatsPlease))\n    # {\n      # print(summary(ZC.data[[ZC.ThisColumn]]))\n    # }\n  print(\"******\") # TDL\n}\n\n[1] \"Data for YearStart:\"\n   Labels Occurs\n1    2011  10192\n2    2012   4368\n3    2013  10248\n4    2014   4536\n5    2015  10584\n6    2016   4620\n7    2017  13860\n8    2018   4620\n9    2019  13365\n10   2020   4536\n11   2021   7700\n12   2022   4620\n[1] \"******\"\n[1] \"Data for Class:\"\n                   Labels Occurs\n1   Fruits and Vegetables   9130\n2 Obesity / Weight Status  36234\n3       Physical Activity  47885\n[1] \"******\"\n[1] \"Data for Topic:\"\n                            Labels Occurs\n1 Fruits and Vegetables - Behavior   9130\n2          Obesity / Weight Status  36234\n3     Physical Activity - Behavior  47885\n[1] \"******\"\n[1] \"Data for Question:\"\n                                                                                                                                                                                                                                                 Labels\n1                                                                                                                                                                       Percent of adults aged 18 years and older who have an overweight classification\n2                                                                                                                                                                                            Percent of adults aged 18 years and older who have obesity\n3                                                  Percent of adults who achieve at least 150 minutes a week of moderate-intensity aerobic physical activity or 75 minutes a week of vigorous-intensity aerobic activity (or an equivalent combination)\n4 Percent of adults who achieve at least 150 minutes a week of moderate-intensity aerobic physical activity or 75 minutes a week of vigorous-intensity aerobic physical activity and engage in muscle-strengthening activities on 2 or more days a week\n5                                                 Percent of adults who achieve at least 300 minutes a week of moderate-intensity aerobic physical activity or 150 minutes a week of vigorous-intensity aerobic activity (or an equivalent combination)\n6                                                                                                                                                              Percent of adults who engage in muscle-strengthening activities on 2 or more days a week\n7                                                                                                                                                                                     Percent of adults who engage in no leisure-time physical activity\n8                                                                                                                                                                                 Percent of adults who report consuming fruit less than one time daily\n9                                                                                                                                                                            Percent of adults who report consuming vegetables less than one time daily\n  Occurs\n1  18117\n2  18117\n3   7449\n4   7449\n5   7449\n6   7449\n7  18089\n8   4565\n9   4565\n[1] \"******\"\n[1] \"Data for Data_Value:\"\n[1] \"Data suppressed.  695 rows.\"\n[1] \"******\"\n[1] \"Data for Low_Confidence_Limit:\"\n[1] \"Data suppressed.  666 rows.\"\n[1] \"******\"\n[1] \"Data for High_Confidence_Limit:\"\n[1] \"Data suppressed.  761 rows.\"\n[1] \"******\"\n[1] \"Data for Sample_Size:\"\n[1] \"Data suppressed.  9761 rows.\"\n[1] \"******\"\n[1] \"Data for AgeYears:\"\n       Labels Occurs\n1              73269\n2     18 - 24   3330\n3     25 - 34   3330\n4     35 - 44   3330\n5     45 - 54   3330\n6     55 - 64   3330\n7 65 or older   3330\n[1] \"******\"\n[1] \"Data for Education:\"\n                            Labels Occurs\n1                                   79929\n2                 College graduate   3330\n3             High school graduate   3330\n4            Less than high school   3330\n5 Some college or technical school   3330\n[1] \"******\"\n[1] \"Data for Gender:\"\n  Labels Occurs\n1         86589\n2 Female   3330\n3   Male   3330\n[1] \"******\"\n[1] \"Data for Income:\"\n              Labels Occurs\n1                     69939\n2  $15,000 - $24,999   3330\n3  $25,000 - $34,999   3330\n4  $35,000 - $49,999   3330\n5  $50,000 - $74,999   3330\n6 $75,000 or greater   3330\n7  Data not reported   3330\n8  Less than $15,000   3330\n[1] \"******\"\n[1] \"Data for RaceEthnic:\"\n                         Labels Occurs\n1                                66609\n2               2 or more races   3330\n3 American Indian/Alaska Native   3330\n4                         Asian   3330\n5     Hawaiian/Pacific Islander   3330\n6                      Hispanic   3330\n7            Non-Hispanic Black   3330\n8            Non-Hispanic White   3330\n9                         Other   3330\n[1] \"******\"\n[1] \"Data for ClassID:\"\n  Labels Occurs\n1     FV   9130\n2    OWS  36234\n3     PA  47885\n[1] \"******\"\n[1] \"Data for TopicID:\"\n  Labels Occurs\n1    FV1   9130\n2   OWS1  36234\n3    PA1  47885\n[1] \"******\"\n[1] \"Data for QuestionID:\"\n  Labels Occurs\n1   Q018   4565\n2   Q019   4565\n3   Q036  18117\n4   Q037  18117\n5   Q043   7449\n6   Q044   7449\n7   Q045   7449\n8   Q046   7449\n9   Q047  18089\n[1] \"******\"\n[1] \"Data for LocationID:\"\n[1] \"Data suppressed.  55 rows.\"\n[1] \"******\"\n[1] \"Data for StratificationCategoryId1:\"\n  Labels Occurs\n1             9\n2  AGEYR  19980\n3    EDU  13320\n4    GEN   6660\n5    INC  23310\n6    OVR   3330\n7   RACE  26640\n[1] \"******\"\n[1] \"Data for StratificationID1:\"\n[1] \"Data suppressed.  29 rows.\"\n[1] \"******\"\n\n\nIn a final step, update the _quarto.yml file and include a menu item for “Data Analysis Exercise” pointing to the new file. Follow the format of the existing entries. Remember to be very careful about the right amount of empty space. Re-create your website and make sure it all works and the new project shows up on the website.\n\n# No code changes were made here for this part of the assignment.\n\nDone.\n\n\n\nIf everything works as expected, commit and push your changes to GitHub. Instead of using the fork + pull-request workflow we’ve tried a few times, we’ll explore a different collaborative approach. In this approach, you and your collaborator work on the same repository. To that end, you need to add your classmate as collaborator. Go to Github.com, find your portfolio repository, go to Settings, then Collaborators. Choose Add Collaborator and add your classmate. Your classmate should receive an invitation, which they need to accept. With this, they are now able to directly push and pull to your repository, without them needing to create a fork. (You can remove them after this exercise if you don’t want them to be able to continue having write access to your repository).\n\n# No code changes were made here for this part of the assignment.\n\nDone.\n\n\n\n\n\n# START - Space Reserved for Collaborator\n\n\n# END - Space Reserved for Collaborator\n\n\n\n\n\n# Assignment 5 - END\n# Go Roadrunners!"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#startup",
    "href": "cdcdata-exercise/cdcdata-exercise.html#startup",
    "title": "cdcdata-exercise",
    "section": "",
    "text": "# DA 6833 02T\n# Summer 2024\n# School of Data Science\n# University of Texas at San Antonio\n\n# Zane Chumley\n# Banner ID: @01318598\n# UTSAid: wgs999\n\n\n\n\n\n# install.packages(\"simstudy\")\n\n\n\n\n\n# library(readxl) #for loading Excel files\n# library(dplyr) #for data processing/cleaning\n# library(tidyr) #for data processing/cleaning\n# library(tidyverse)\n# library(skimr) #for nice visualization of data \n# library(here) #to set paths\n# library(dslabs) #for data used in this assignment\n# library(tibble)\n# library(simstudy)\nlibrary(sqldf)\n\nLoading required package: gsubfn\n\n\nLoading required package: proto\n\n\nLoading required package: RSQLite\n\n\n\n\n\n\n# Define path to .csv file containing data\nZC.filepath &lt;- \"Nutrition__Physical_Activity__and_Obesity_-_Behavioral_Risk_Factor_Surveillance_System_20240703.csv\"\n# The master dataset may undergo dimensional changes throughout this exercise.  The changing dimensions will be tracked over time.  Variables to track these changing dimensions will be created.\nZC.dimindex &lt;- 0\nZC.datarows &lt;- list()\nZC.datacols &lt;- list()\n# There may be columns with data arbitrarily judged to have worthless data.  Variables will be created to store these column names for subsequent examination.\nZC.columnindex &lt;- 0\nZC.examinedcolumns &lt;- list()\n# There may be lookup tables created during this exercise to simplify the master dataset.  When created, the column containing the lookup table and the name of the lookup table will be stored.\nZC.lookupindex &lt;- 0\nZC.lookupcolumn &lt;- list()\nZC.lookuptable &lt;- list()"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#part-1",
    "href": "cdcdata-exercise/cdcdata-exercise.html#part-1",
    "title": "cdcdata-exercise",
    "section": "",
    "text": "Previously, you did a quick exploration of a dataset that came with an R package (gapminder data inside dslabs package). A lot of datasets can be found inside R packages. For instance, this page lists what is likely only a small fractionLinks to an external site.. The good and the bad about datasets that come with R packages is that they are often fairly clean/tidy. That’s unfortunately not how most “real world” datasets look like. Getting dirty and messy datasets and wrangling them into a form that is suitable for statistical analysis is part of most workflows and often takes a lot of time. We’ll start practicing this here by getting data that might or might not be very clean.\nGo to the CDC’s data website at https://data.cdc.gov/Links to an external site.. Browse through the site and identify a dataset of interest.\nWhich dataset you choose is up to you. I suggest you pick a dataset that has at least 100 observations with 5 different variables, and a mix of continuous and categorical ones. Often, 5 variables means 5 columns. That would be the case in properly formatted data. However, some of the data provided by the CDC is rather poorly formatted. For instance CDC’s dataset on traumatic brain injuryLinks to an external site. has the same variable (age) in separate columns, and it is also discretized. As we’ll discuss, these are two really bad things you can do to your data, so I recommend staying away from such datasets. There are plenty on that website, so I’m sure you’ll find one that is suitable and interesting to you.\n\n\n\nTo get the dataset you selected, it is easiest if you download the file to your computer and place it inside your portfolio repository. Note that in general, you should make each data analysis (or other) project its own GitHub repository, and always use a structure like the one provided in the Data Analysis Template (or something similar). However, for this small exercise and for logistic reasons, you’ll use your portfolio/website repository, and just a single folder. Make a new folder called cdcdata-exercise inside your portfolio repository. Place the data into that folder.\nRemember that GitHub doesn’t like large files. So if you pick a large data file (&gt;50MB), first place it somewhere outside your repository, then reduce it by e.g., writing some R code that selects only a portion of the data. Once it’s small enough, you can place it into the GitHub repository.\nWhile you should be able to find data for direct download from the CDC website, sometimes you need to write a bit of code to pull data from a source. This is usually done through an API. R has packages that make this relatively easy. If you ever encounter that situation, search online for instructions. Google/Stackoverflow are your friends to figure out what commands you need to write).\n\n\n\nNow, write code that explores the data. Add a new Quarto document called cdcdata-exercise.qmd to the folder you just created.\nStart by providing a brief description of the data, where you got it, what it contains. Also add a link to the source.\nThis assignment uses a dataset from the United States Center for Disease Control (CDC) titled, “Nutrition, Physical Activity, and Obesity - Behavioral Risk Factor Surveillance System.” The dataset and column definition can be found here.\nThe CDC describes the dataset as follows:\n\nThis dataset includes data on adult’s diet, physical activity, and weight status from Behavioral Risk Factor Surveillance System. This data is used for DNPAO’s Data, Trends, and Maps database, which provides national and state specific data on obesity, nutrition, physical activity, and breastfeeding.\n\n\n\n\n\n\n\nNote\n\n\n\nThe DNPAO is the CDC’s Division of Nutrition, Physical Activity, and Obesity.\n\n\nThen write code that reads/loads the data. As needed, process the data (e.g., if there are weird symbols in the data, or missing values coded as 999 or anything of that sort, write code to fix it.) If your dataset has a lot of variables, pick a few of them (at least 5).\nThe dataset is described by the CDC as 93.2k rows with 33 columns each. For this assignment, the entire dataset will be loaded.\n\n# Track time it takes to load\nZC.START_TIMESTAMP &lt;- proc.time()\n# load the data into a dataframe\nZC.data &lt;- read.csv(ZC.filepath)\nZC.loadtime &lt;- proc.time() - ZC.START_TIMESTAMP\n# Report the load time\nZC.printline &lt;- paste(\"The data took\"\n                      , ZC.loadtime[3]\n                      , \"seconds to load.\"\n                      , sep=\" \"\n)\nZC.printline\n\n[1] \"The data took 1.03 seconds to load.\"\n\n\nThere were no issues encountered while loading the data.\n\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 33 columns.\"\n\n\nTo get an overview of what we have just loaded, a table with summary information about each column is constructed.\n\n# Setup control variables and lists\nZC.index &lt;- 0\nZC.colname &lt;- list()\nZC.coltype &lt;- list()\nZC.NAvals &lt;- list()\nZC.unique &lt;- list()\nZC.example &lt;- list()\n# Loop through the columns to populate the lists\nfor (ZC.ThisColumn in colnames(ZC.data))\n{\n  #Increment the index for the lists\n  ZC.index &lt;- ZC.index+1 \n  # Assign the column name\n  ZC.colname[ZC.index] &lt;- ZC.ThisColumn \n  # Assign the column type\n  ZC.coltype[ZC.index] &lt;- typeof(ZC.data[[ZC.ThisColumn]]) \n  # Count the number of NAs in the column\n  ZC.NAvals[ZC.index] &lt;- sum(is.na(ZC.data[[ZC.ThisColumn]])) \n  # Count the numner of unique values in the column\n  ZC.TheseValues &lt;- table(ZC.data[[ZC.ThisColumn]])\n  ZC.unique[ZC.index] &lt;- length(ZC.TheseValues)\n  # Find a suitable example value\n  # Prepare to loop through all the values to find an acceptable example\n  for (ZC.ThisRow in 1:nrow(ZC.data)) \n  {\n    # Start this FOR loop but prepare to break out of it\n    ZC.BreakFlag = 0\n    # As soon as an acceptable value is found, set the break flag to 1\n    # Retrieve the value in ThisRow\n    ZC.ThisExp &lt;- ZC.data[[ZC.ThisColumn]][ZC.ThisRow]\n    # Continue with this example only if it is not NA\n    if (!is.na(ZC.ThisExp))\n    {\n      # Subsequent processing depends on data type of current example\n      # Is the current example a character string?\n      if (typeof(ZC.ThisExp) == \"character\")\n      {\n        # Trim any leading and trailing spaces from the current example\n        # If there is nothing but spaces, only an empty string will survive\n        ZC.ThisExp &lt;- trimws(ZC.ThisExp)\n        # Continue with this example only if it is not empty\n        if (nchar(ZC.ThisExp)&gt;0)\n        {\n          # We have an acceptable example\n          ZC.BreakFlag &lt;- 1\n          # Let's cut it short if the example is too long\n          if (nchar(ZC.ThisExp)&gt;8)\n          {\n            ZC.ThisExp &lt;- paste(substr(ZC.ThisExp, 1, 5), \"...\", sep=\"\")\n          }\n        }\n      } else if (typeof(ZC.ThisExp) == \"integer\" | typeof(ZC.ThisExp) == \"double\")\n      {\n        # We would prefer a nonzero value, and we will loop through all the values if necessary to find one\n        if (!ZC.ThisExp==0) ZC.BreakFlag &lt;- 1\n      } else\n      { # The value could be boolean, complex, or something else\n        # We will take it\n        ZC.BreakFlag &lt;- 1\n      }\n    }\n    # If we have an acceptable value, we can stop looping through the rows\n    if (ZC.BreakFlag == 1) break\n  }\n  ZC.example[ZC.index] &lt;- ZC.ThisExp\n}\n# Bind the lists together in a single object\nZC.datasum01 &lt;- cbind(ZC.colname, ZC.coltype, ZC.NAvals, ZC.unique, ZC.example)\n# Apply more friendly column names\ncolnames(ZC.datasum01) &lt;- c(\"Column\"\n                          , \"Data Type\"\n                          , \"NA Values\"\n                          , \"Uniques\"\n                          , \"Example\"\n)\nZC.datasum01\n\n      Column                       Data Type   NA Values Uniques Example   \n [1,] \"YearStart\"                  \"integer\"   0         12      2020      \n [2,] \"YearEnd\"                    \"integer\"   0         12      2020      \n [3,] \"LocationAbbr\"               \"character\" 0         55      \"US\"      \n [4,] \"LocationDesc\"               \"character\" 0         55      \"National\"\n [5,] \"Datasource\"                 \"character\" 0         2       \"Behav...\"\n [6,] \"Class\"                      \"character\" 0         3       \"Physi...\"\n [7,] \"Topic\"                      \"character\" 0         3       \"Physi...\"\n [8,] \"Question\"                   \"character\" 0         9       \"Perce...\"\n [9,] \"Data_Value_Unit\"            \"logical\"   93249     0       NA        \n[10,] \"Data_Value_Type\"            \"character\" 0         1       \"Value\"   \n[11,] \"Data_Value\"                 \"double\"    9235      695     30.6      \n[12,] \"Data_Value_Alt\"             \"double\"    9235      695     30.6      \n[13,] \"Data_Value_Footnote_Symbol\" \"character\" 0         2       \"~\"       \n[14,] \"Data_Value_Footnote\"        \"character\" 0         2       \"Data ...\"\n[15,] \"Low_Confidence_Limit\"       \"double\"    9235      666     29.4      \n[16,] \"High_Confidence_Limit\"      \"double\"    9235      761     31.8      \n[17,] \"Sample_Size\"                \"integer\"   9235      9761    31255     \n[18,] \"Total\"                      \"character\" 0         2       \"Total\"   \n[19,] \"Age.years.\"                 \"character\" 0         7       \"25 - 34\" \n[20,] \"Education\"                  \"character\" 0         5       \"High ...\"\n[21,] \"Gender\"                     \"character\" 0         3       \"Female\"  \n[22,] \"Income\"                     \"character\" 0         8       \"$50,0...\"\n[23,] \"Race.Ethnicity\"             \"character\" 0         9       \"Hispanic\"\n[24,] \"GeoLocation\"                \"character\" 0         106     \"(13.4...\"\n[25,] \"ClassID\"                    \"character\" 0         3       \"PA\"      \n[26,] \"TopicID\"                    \"character\" 0         3       \"PA1\"     \n[27,] \"QuestionID\"                 \"character\" 0         9       \"Q047\"    \n[28,] \"DataValueTypeID\"            \"character\" 0         1       \"VALUE\"   \n[29,] \"LocationID\"                 \"integer\"   0         55      59        \n[30,] \"StratificationCategory1\"    \"character\" 0         7       \"Race/...\"\n[31,] \"Stratification1\"            \"character\" 0         29      \"Hispanic\"\n[32,] \"StratificationCategoryId1\"  \"character\" 0         7       \"RACE\"    \n[33,] \"StratificationID1\"          \"character\" 0         29      \"RACEHIS\" \n\n\nThe above output stipulates that the values in Data_Value_Unit are worthless, confirmed by two (2) distinct findings:\n\nall 93,239 values are “NA”\nzero (0) unique values\n\nAdditionally, there is only one unique value in the following columns:\n\nData_Value_Type\nDataValueTypeID\n\nTherefore, all three (3) of these columns will be dropped from the dataframe.\n\nZC.data &lt;- subset(ZC.data, select = -c(Data_Value_Unit, Data_Value_Type, DataValueTypeID))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 30 columns.\"\n\n\nThere are also some column with unique counts so low they appear to offer little or no value. They will be dropped if they are so judged to be unvalued. The columns to be examined are:\n\nDatasource\nData_Value_Footnote_Symbol\nData_Value_Footnote\nTotal\n\nFor each of these columns, the distinct values will be displayed for evaluation.\n\n# create list of columns to be examined\nZC.examinedcolumns[1] &lt;- \"Datasource\"\nZC.examinedcolumns[2] &lt;- \"Data_Value_Footnote_Symbol\"\nZC.examinedcolumns[3] &lt;- \"Data_Value_Footnote\"\nZC.examinedcolumns[4] &lt;- \"Total\"\n# loop through the columns to be examined, building and displaying a temporary table of the unique values and their counts.\nfor (ZC.ExamineThis in ZC.examinedcolumns)\n{\n  # create section separator\n  ZC.printline &lt;- \"********************************\"\n  print (ZC.printline)\n  # build SQL statement syntax\n  ZC.RunThis &lt;- paste(\"select \"\n                      , ZC.ExamineThis\n                      ,\", count(*) as Occurs from 'ZC.data' group by \"\n                      , ZC.ExamineThis\n                      , sep = \"\")\n  # display SQL statement for posterity\n  ZC.printline &lt;- paste(\"Running: \"\n                        , ZC.RunThis\n                        , sep = \"\")\n  print(ZC.printline)\n  # load results into temporary table\n  ZC.ThisTable &lt;- sqldf(ZC.RunThis)\n  # display contents of temporary table\n  ZC.printline &lt;- paste(\"Unique Values of \"\n                        , ZC.ExamineThis\n                        , \":\"\n                        , sep=\"\"\n                        )\n  print(ZC.printline)\n  # display results of temporary table\n  print(ZC.ThisTable)\n}\n\n[1] \"********************************\"\n[1] \"Running: select Datasource, count(*) as Occurs from 'ZC.data' group by Datasource\"\n[1] \"Unique Values of Datasource:\"\n                                  Datasource Occurs\n1                                      BRFSS   4620\n2 Behavioral Risk Factor Surveillance System  88629\n[1] \"********************************\"\n[1] \"Running: select Data_Value_Footnote_Symbol, count(*) as Occurs from 'ZC.data' group by Data_Value_Footnote_Symbol\"\n[1] \"Unique Values of Data_Value_Footnote_Symbol:\"\n  Data_Value_Footnote_Symbol Occurs\n1                             84014\n2                          ~   9235\n[1] \"********************************\"\n[1] \"Running: select Data_Value_Footnote, count(*) as Occurs from 'ZC.data' group by Data_Value_Footnote\"\n[1] \"Unique Values of Data_Value_Footnote:\"\n                                      Data_Value_Footnote Occurs\n1                                                          84014\n2 Data not available because sample size is insufficient.   9235\n[1] \"********************************\"\n[1] \"Running: select Total, count(*) as Occurs from 'ZC.data' group by Total\"\n[1] \"Unique Values of Total:\"\n  Total Occurs\n1        89919\n2 Total   3330\n\n\nBased on the above results, Datasource will be dropped immediately, as the unique values seem synonyms for each other, so there is really only one value in the column. The other three (3) columns require additional consideration.\n\nZC.data &lt;- subset(ZC.data, select = -c(Datasource))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 29 columns.\"\n\n\nThe Total column unique values don’t seem to tell much. The CDC describes the data in this column as:\n\nTotal/Overall breakout category\n\nGiven the vagueness of the description and the unique values, the Total column will also be dropped.\n\nZC.data &lt;- subset(ZC.data, select = -c(Total))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 28 columns.\"\n\n\nThe GeoLocation column appears to have highly-coded data. The CDC describes this column as:\n\nLatitude & Longitude to be provided for formatting GeoLocation or Geocode in the format (latitude, longitude)\n\nTherefore, this column will also be dropped.\n\nZC.data &lt;- subset(ZC.data, select = -c(GeoLocation))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 27 columns.\"\n\n\nThe count of the only nonblank value in both the Data_Value_Footnote_Symbol and Data_Value_Footnote have been seen before: coincidence or not, there are the exact name number of NA values in the Data_Value and Data_Value_Alt columns. If it turns out there is a meaningful value in Data_Value_Footnote_Symbol and Data_Value_Footnote for each NA in Data_Value and Data_Value_Alt, then the Data_Value_Footnote_Symbol and Data_Value_Footnote columns can be considered duplicative information for NA values in the Data_Value and Data_Value_Alt columns.\nTo examine, we will build lists of the rows where the following conditions occur, and then compare them to see if they are equal:\n\nWhere Data_Value_Footnote_Symbol is not blank\nWhere Data_Value_Footnote is not blank\nWhere Data_Value is NA\nWhere Data_Value_Alt is NA\n\n\n# create lists to store row numbers where specified conditions occur\n# store rows where Data_Value_Footnote_Symbol is not blank\nZC.DVFS &lt;- list() \n# store rows where Data_Value_Footnote is not blank\nZC.DVF &lt;- list() \n# store rows where Data_Value is NA\nZC.DV &lt;- list() \n# store rows where Data_Value_Alt is NA\nZC.DVA &lt;- list() \n# create index variable for each list\nZC.DVFSIndex &lt;- 0\nZC.DVFIndex &lt;- 0\nZC.DVIndex &lt;- 0\nZC.DVAIndex &lt;- 0\n# loop through the rows in the master dataset and store indices where conditions are met\nZC.RowRange &lt;- 1:nrow(ZC.data)\nfor (ZC.CheckRow in ZC.RowRange)\n{\n  # pull Data_Value_Footnote_Symbol value in current row\n  ZC.ThisValue &lt;- ZC.data$Data_Value_Footnote_Symbol[ZC.CheckRow]\n  if (!(is.null(ZC.ThisValue)))\n  {\n    # Value is not NULL\n    # But is it empty or blank?\n    ZC.ThisValue &lt;- trimws(ZC.ThisValue)\n    if (nchar(ZC.ThisValue)&gt;0)\n    {\n      # value is not blank, so store the row number\n      ZC.DVFSIndex &lt;- ZC.DVFSIndex+1\n      ZC.DVFS[ZC.DVFSIndex] &lt;- ZC.CheckRow\n    }\n  }\n  # pull Data_Value_Footnote value in current row\n  ZC.ThisValue &lt;- ZC.data$Data_Value_Footnote[ZC.CheckRow]\n  if (!(is.null(ZC.ThisValue)))\n  {\n    # Value is not NULL\n    # But is it empty or blank?\n    ZC.ThisValue &lt;- trimws(ZC.ThisValue)\n    if (nchar(ZC.ThisValue)&gt;0)\n    {\n      # value is not blank, so store the row number\n      ZC.DVFIndex &lt;- ZC.DVFIndex+1\n      ZC.DVF[ZC.DVFIndex] &lt;- ZC.CheckRow\n    }\n  }\n  # is Data_Value an NA value?\n  if (is.na(ZC.data$Data_Value[ZC.CheckRow]))\n  {\n      # value is NA, so store the row number\n      ZC.DVIndex &lt;- ZC.DVIndex+1\n      ZC.DV[ZC.DVIndex] &lt;- ZC.CheckRow\n  }\n  # is Data_Value_Alt an NA value?\n  if (is.na(ZC.data$Data_Value_Alt[ZC.CheckRow]))\n  {\n      # value is NA, so store the row number\n      ZC.DVAIndex &lt;- ZC.DVAIndex+1\n      ZC.DVA[ZC.DVAIndex] &lt;- ZC.CheckRow\n  }\n}\n\nNow that the lists of the row numbers where the conditions occur have been built, they need to be compared. They only need to be compared if they are all the same length, so that will be checked first.\n\n# Compare the list of row numbers where conditions were met\n# Are all the lists of the same length?  No need to compare if not.\n# Create an indicator flag to indicate if the row numbers match.\n# They can't match if the count of row numbers does not match, so assume it is FALSE\nZC.CompEquals &lt;- FALSE\nif (ZC.DVFSIndex==ZC.DVFIndex&ZC.DVFIndex==ZC.DVIndex&ZC.DVIndex==ZC.DVAIndex)\n{\n  # The lists are the same length.  A loop will be used to compare value by value.\n  # No need to continue the loop as soon as a difference if found\n  # Set the indicator to TRUE; change to FALSE if an unequal row number is found\n  ZC.CompEquals &lt;- TRUE\n  # The row numbers in the list are actually embedded inside lists of one (1) item. \n  ZC.CompRange &lt;- 1:ZC.DVIndex\n  for (ZC.CompRow in ZC.CompRange)\n  {\n    # The row numbers in the list are actually embedded inside lists of one (1) item. \n    # We'll break each value out of it's list before we compare.\n    ZC.ThisDVFS &lt;- unlist(ZC.DVFS[ZC.CompRow])\n    ZC.ThisDVF &lt;- unlist(ZC.DVF[ZC.CompRow])\n    ZC.ThisDV &lt;- unlist(ZC.DV[ZC.CompRow])\n    ZC.ThisDVA &lt;- unlist(ZC.DVA[ZC.CompRow])\n    if(!(ZC.ThisDVFS==ZC.ThisDVF&ZC.ThisDVF==ZC.ThisDV&ZC.ThisDV==ZC.ThisDVA))\n    {\n      # We found a row number that did not match.  Set indicator value to FALSE and stop\n      ZC.CompEquals &lt;- FALSE\n      break\n    }\n  }\n}\nZC.ThisDVFS # TDL\n\n[1] 93241\n\ntypeof(ZC.ThisDVFS) #TDL\n\n[1] \"integer\"\n\nZC.CompRow # TDL\n\n[1] 9235\n\ntypeof(ZC.CompRow) #TDL\n\n[1] \"integer\"\n\nZC.DVFS[ZC.CompRow] # TDL\n\n[[1]]\n[1] 93241\n\ntypeof(ZC.DVFS[ZC.CompRow]) # TDL\n\n[1] \"list\"\n\nZC.DVF[ZC.CompRow] #TDL\n\n[[1]]\n[1] 93241\n\ntypeof(ZC.DVF[ZC.CompRow]) # TDL\n\n[1] \"list\"\n\n# Did the indicator flag survive every row number comparison?\nif (ZC.CompEquals)\n{\n  print(\"Row numbers where conditions are met match.\")\n} else\n{\n  print(\"Row numbers where conditions are met do not match.\")\n}\n\n[1] \"Row numbers where conditions are met match.\"\n\n\nThe lists of row numbers where the conditions were met match exactly. Therefore, the data in columns Data_Value_Footnote_Symbol and Data_Value_Footnote can be considered redundant for cases when the values in Data_Value and Data_Value_Alt are NA. Having already established that the data in columns Data_Value_Footnote_Symbol and Data_Value_Footnote is blank when the values in Data_Value and Data_Value_Alt are not NA, we can remove the columns with blank/redundant data.\n\nZC.data &lt;- subset(ZC.data, select = -c(Data_Value_Footnote_Symbol, Data_Value_Footnote))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 25 columns.\"\n\n\nNext, the count of unique values, coupled with the column names, suggests a one-to-one relationship between the data in one column and the data in another. In proven true, we can reduce repetitiveness by archiving off the duplicated data into a separate lookup table. Column combinations to consider are:\n\nLocationID, a potential lookup for both LocationAbbr and LocationDesc\nStratificationCategoryId1, a potential lookup for StratificationCategory1\nStratificationID1, a potential lookup for Stratification1\n\nThe validity of an index/lookup relationship between the above potentials will be explored, beginning with LocationID.\nA SQL statement will be utilized to count the number of times each combination of the LocationID, LocationAbbr, and LocationDesc occurs in the data. Since there are 55 unique values in each of these columns, if there are a total of 55 combinations, then a one-to-one(-to-one) correlation is established.\n\nsqldf(\"select LocationID, LocationAbbr, LocationDesc, count(*) as Occurs from 'ZC.data' group by LocationID, LocationAbbr, LocationDesc\")\n\n   LocationID LocationAbbr         LocationDesc Occurs\n1           1           AL              Alabama   1736\n2           2           AK               Alaska   1736\n3           4           AZ              Arizona   1736\n4           5           AR             Arkansas   1736\n5           6           CA           California   1736\n6           8           CO             Colorado   1736\n7           9           CT          Connecticut   1736\n8          10           DE             Delaware   1736\n9          11           DC District of Columbia   1736\n10         12           FL              Florida   1736\n11         13           GA              Georgia   1736\n12         15           HI               Hawaii   1736\n13         16           ID                Idaho   1736\n14         17           IL             Illinois   1736\n15         18           IN              Indiana   1736\n16         19           IA                 Iowa   1736\n17         20           KS               Kansas   1736\n18         21           KY             Kentucky   1736\n19         22           LA            Louisiana   1736\n20         23           ME                Maine   1736\n21         24           MD             Maryland   1736\n22         25           MA        Massachusetts   1736\n23         26           MI             Michigan   1736\n24         27           MN            Minnesota   1736\n25         28           MS          Mississippi   1736\n26         29           MO             Missouri   1736\n27         30           MT              Montana   1736\n28         31           NE             Nebraska   1736\n29         32           NV               Nevada   1736\n30         33           NH        New Hampshire   1736\n31         34           NJ           New Jersey   1493\n32         35           NM           New Mexico   1736\n33         36           NY             New York   1736\n34         37           NC       North Carolina   1736\n35         38           ND         North Dakota   1736\n36         39           OH                 Ohio   1736\n37         40           OK             Oklahoma   1736\n38         41           OR               Oregon   1736\n39         42           PA         Pennsylvania   1736\n40         44           RI         Rhode Island   1736\n41         45           SC       South Carolina   1736\n42         46           SD         South Dakota   1736\n43         47           TN            Tennessee   1736\n44         48           TX                Texas   1736\n45         49           UT                 Utah   1736\n46         50           VT              Vermont   1736\n47         51           VA             Virginia   1736\n48         53           WA           Washington   1736\n49         54           WV        West Virginia   1736\n50         55           WI            Wisconsin   1736\n51         56           WY              Wyoming   1736\n52         59           US             National   1736\n53         66           GU                 Guam   1260\n54         72           PR          Puerto Rico   1316\n55         78           VI       Virgin Islands    644\n\n\nThere are only 55 total unique combinations for these three (3) variables, so a lookup table for LocationAbbr and another for LocationDesc will be created, both with LocationID as the lookup value. The lookup value and the first lookup table will be stored as a relationship. Afterwards, the columns containing the lookup values will be dropped.\n\n# Creating and displaying top of lookup tables\nZC.lookupLocationAbbr &lt;- sqldf(\"select distinct LocationID, LocationAbbr from 'ZC.data' order by LocationID\")\nhead(ZC.lookupLocationAbbr)\n\n  LocationID LocationAbbr\n1          1           AL\n2          2           AK\n3          4           AZ\n4          5           AR\n5          6           CA\n6          8           CO\n\nZC.lookupLocationDesc &lt;- sqldf(\"select distinct LocationID, LocationDesc from 'ZC.data' order by LocationID\")\nhead(ZC.lookupLocationDesc)\n\n  LocationID LocationDesc\n1          1      Alabama\n2          2       Alaska\n3          4      Arizona\n4          5     Arkansas\n5          6   California\n6          8     Colorado\n\n# Storing lookup columns and lookup table names\nZC.lookupindex &lt;- ZC.lookupindex+1\nZC.lookupcolumn[ZC.lookupindex] &lt;- \"LocationID\"\nZC.lookuptable[ZC.lookupindex] &lt;- \"LocationAbbr\"\nZC.data &lt;- subset(ZC.data, select = -c(LocationAbbr, LocationDesc))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 23 columns.\"\n\n\nA SQL statement will be utilized to count the number of times each combination of the StratificationCategoryId1 and StratificationCategory1 occurs in the data. Since there are seven (7) unique values in each of these columns, if there are a total of seven (7) combinations, then a one-to-one correlation is established.\n\nsqldf(\"select StratificationCategoryId1, StratificationCategory1, count(*) as Occurs from 'ZC.data' group by StratificationCategoryId1, StratificationCategory1\")\n\n  StratificationCategoryId1 StratificationCategory1 Occurs\n1                                                        9\n2                     AGEYR             Age (years)  19980\n3                       EDU               Education  13320\n4                       GEN                  Gender   6660\n5                       INC                  Income  23310\n6                       OVR                   Total   3330\n7                      RACE          Race/Ethnicity  26640\n\n\nThere are only seven (7) total unique combinations for these two (2) variables, so a lookup table for StratificationCategory1 will be created with StratificationCategoryId1 as the lookup value. The lookup value and the lookup table will be stored as a relationship. Afterwards, the columns containing the lookup values will be dropped.\n\n# Creating and displaying top of lookup tables\nZC.StratificationCategory1 &lt;- sqldf(\"select distinct StratificationCategoryId1, StratificationCategory1 from 'ZC.data' order by StratificationCategoryId1\")\nhead(ZC.StratificationCategory1)\n\n  StratificationCategoryId1 StratificationCategory1\n1                                                  \n2                     AGEYR             Age (years)\n3                       EDU               Education\n4                       GEN                  Gender\n5                       INC                  Income\n6                       OVR                   Total\n\n# Storing lookup columns and lookup table names\nZC.lookupindex &lt;- ZC.lookupindex+1\nZC.lookupcolumn[ZC.lookupindex] &lt;- \"StratificationCategoryId1\"\nZC.lookuptable[ZC.lookupindex] &lt;- \"StratificationCategory1\"\nZC.data &lt;- subset(ZC.data, select = -c(StratificationCategory1))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 22 columns.\"\n\n\nA SQL statement will be utilized to count the number of times each combination of the StratificationID1 and Stratification1 occurs in the data. Since there are 29 unique values in each of these columns, if there are a total of 29 combinations, then a one-to-one correlation is established.\n\nsqldf(\"select StratificationID1, Stratification1, count(*) as Occurs from 'ZC.data' group by StratificationID1, Stratification1\")\n\n   StratificationID1                  Stratification1 Occurs\n1                                                          9\n2          AGEYR1824                          18 - 24   3330\n3          AGEYR2534                          25 - 34   3330\n4          AGEYR3544                          35 - 44   3330\n5          AGEYR4554                          45 - 54   3330\n6          AGEYR5564                          55 - 64   3330\n7        AGEYR65PLUS                      65 or older   3330\n8          EDUCOGRAD                 College graduate   3330\n9           EDUCOTEC Some college or technical school   3330\n10             EDUHS            Less than high school   3330\n11         EDUHSGRAD             High school graduate   3330\n12            FEMALE                           Female   3330\n13           INC1525                $15,000 - $24,999   3330\n14           INC2535                $25,000 - $34,999   3330\n15           INC3550                $35,000 - $49,999   3330\n16           INC5075                $50,000 - $74,999   3330\n17         INC75PLUS               $75,000 or greater   3330\n18         INCLESS15                Less than $15,000   3330\n19             INCNR                Data not reported   3330\n20              MALE                             Male   3330\n21           OVERALL                            Total   3330\n22         RACE2PLUS                  2 or more races   3330\n23           RACEASN                            Asian   3330\n24           RACEBLK               Non-Hispanic Black   3330\n25           RACEHIS                         Hispanic   3330\n26           RACEHPI        Hawaiian/Pacific Islander   3330\n27           RACENAA    American Indian/Alaska Native   3330\n28           RACEOTH                            Other   3330\n29           RACEWHT               Non-Hispanic White   3330\n\n\nThere are only 29 total unique combinations for these two (2) variables, so a lookup table for Stratification1 will be created with StratificationID1 as the lookup value. The lookup value and the lookup table will be stored as a relationship. Afterwards, the columns containing the lookup values will be dropped.\n\n# Creating and displaying top of lookup tables\nZC.Stratification1 &lt;- sqldf(\"select distinct StratificationID1, Stratification1 from 'ZC.data' order by StratificationID1\")\nhead(ZC.Stratification1)\n\n  StratificationID1 Stratification1\n1                                  \n2         AGEYR1824         18 - 24\n3         AGEYR2534         25 - 34\n4         AGEYR3544         35 - 44\n5         AGEYR4554         45 - 54\n6         AGEYR5564         55 - 64\n\n# Storing lookup columns and lookup table names\nZC.lookupindex &lt;- ZC.lookupindex+1\nZC.lookupcolumn[ZC.lookupindex] &lt;- \"StratificationID1\"\nZC.lookuptable[ZC.lookupindex] &lt;- \"Stratification1\"\nZC.data &lt;- subset(ZC.data, select = -c(Stratification1))\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 21 columns.\"\n\n\nSimilarly, it’s possible YearStart always equals YearEnd, Class always equals Topic, and Data_Value always equals Data_Value_Alt. Code will be executed to see if these possible pairs always duplicate a value in the other; if so, the duplicate column will be dropped.\nSince there are NA values in both the Data_Value and Data_Value_Alt, and R code will halt if it tries to compare an NA with anything (even another NA value), this comparison will be conditional. For the purposes of this exercise:\n\nif both values are NA, the values are considered equal (but a comparison cannot be attempted by R).\nif one value is NA but the other is not, the values are considered not equal.\nif neither value is NA, a comparison will be performed to determine if the values are equal or not.\n\n\n# Set up variables to count differences\nZC.YearDeltas &lt;- 0\nZC.ClassDeltas &lt;- 0\nZC.DataDeltas &lt;- 0\nfor (ZC.ThisRow in 1:nrow(ZC.data))\n{\n  # Count if YearStart and YearEnd are different values\n  if (!(ZC.data$YearStart[ZC.ThisRow]==ZC.data$YearEnd[ZC.ThisRow]))\n    {\n      # Brackets not necessary; just respecting page margins\n      ZC.YearDeltas &lt;- ZC.YearDeltas+1\n    }\n  # Count if Class and Topic are different values\n  if(!(ZC.data$Class[ZC.ThisRow]==ZC.data$Topic[ZC.ThisRow]))\n    {\n      # Brackets not necessary; just respecting page margins\n      ZC.ClassDeltas &lt;- ZC.ClassDeltas+1\n    }\n  # There could be NA values for either Data_Value and/or Data_Value_Alt\n  # If both are NA, there is no difference\n  # If only one is NA, there is a difference\n  # Only compare if neither are NA\n  if (is.na(ZC.data$Data_Value[ZC.ThisRow])==is.na(ZC.data$Data_Value_Alt[ZC.ThisRow]))\n  {\n    # Either both are NA or neither or NA\n    # Comparison is not necessary (or valid!) if both are NA\n    if (!(is.na(ZC.data$Data_Value[ZC.ThisRow])))\n    {\n      # Neither are NA - compare values\n      if (!(ZC.data$Data_Value[ZC.ThisRow]==ZC.data$Data_Value_Alt[ZC.ThisRow]))\n      {\n        # Values are not equal\n        ZC.DataDeltas &lt;- ZC.DataDeltas+1\n      }\n    }\n  }\n  else\n  {\n    # One value is NA but not the other, so they are automatically not equal\n    ZC.DataDeltas &lt;- ZC.DataDeltas+1\n  }\n}\n# Report on deltas\nZC.printline &lt;- paste(\"There were\"\n                      , ZC.YearDeltas\n                      , \"row(s) with different values in YearStart and YearEnd.\"\n                      , sep = \" \")\nZC.printline\n\n[1] \"There were 0 row(s) with different values in YearStart and YearEnd.\"\n\nZC.printline &lt;- paste(\"There were\"\n                      , ZC.ClassDeltas\n                      , \"row(s) with different values in Class and Topic.\"\n                      , sep = \" \")\nZC.printline\n\n[1] \"There were 57015 row(s) with different values in Class and Topic.\"\n\nZC.printline &lt;- paste(\"There were\"\n                      , ZC.DataDeltas\n                      , \"row(s) with different values in Data_Value and Data_Value_Alt.\"\n                      , sep = \" \")\nZC.printline\n\n[1] \"There were 0 row(s) with different values in Data_Value and Data_Value_Alt.\"\n\n# Drop YearEnd if it always equals YearStart\nif (ZC.YearDeltas==0) \n  {\n    print(\"Dropping column YearEnd\")\n    ZC.data &lt;- subset(ZC.data, select = -c(YearEnd))\n  }\n\n[1] \"Dropping column YearEnd\"\n\n# Drop Topic if it always equals Class\nif (ZC.ClassDeltas==0) \n  {\n    print(\"Dropping column Topic\")\n    ZC.data &lt;- subset(ZC.data, select = -c(Topic))\n  }\n# Drop Data_Value_Alt if it always equals Data_Value\nif (ZC.DataDeltas==0)\n  {\n    print(\"Dropping column Data_Value_Alt\")\n    ZC.data &lt;- subset(ZC.data, select = -c(Data_Value_Alt))\n  }\n\n[1] \"Dropping column Data_Value_Alt\"\n\n# Report master dataset dimensions\n# Increase dimension index\nZC.dimindex &lt;- ZC.dimindex+1\nZC.TheseDims &lt;- dim(ZC.data)\nZC.datarows[ZC.dimindex] &lt;- ZC.TheseDims[1]\nZC.datacols[ZC.dimindex] &lt;- ZC.TheseDims[2]\nZC.printline &lt;- paste(\"The master dataset is currently\"\n                        , ZC.datarows[ZC.dimindex]\n                        , \"rows by\"\n                        , ZC.datacols[ZC.dimindex]\n                        , \"columns.\"\n                        , sep=\" \")\nZC.printline\n\n[1] \"The master dataset is currently 93249 rows by 19 columns.\"\n\n\nFinally, some column names do not play well with SQL (when they contain unallowed characters), which will be used later to build the data for the charts. So, there will be a few manual manipulations of some column names:\n\nAge.years. will become AgeYears\nRace.Ethnicity will become RaceEthnic\n\n\n# change column names to be more SQL-friendly\nprint(typeof(ZC.data)) # TDL\n\n[1] \"list\"\n\nnames(ZC.data)[names(ZC.data) == 'Age.years.'] &lt;- 'AgeYears'\nnames(ZC.data)[names(ZC.data) == 'Race.Ethnicity'] &lt;- 'RaceEthnic'\n\nThe summary table will be rebuilt before continuing. This time, a new column, LU (for lookup) will be created. If a lookup table has been created for the master dataset column, LU will be set to one (1); otherwise, it will be set to zero (0).\n\n# Setup control variables and lists\nZC.index &lt;- 0\nZC.colname &lt;- list()\nZC.coltype &lt;- list()\nZC.NAvals &lt;- list()\nZC.unique &lt;- list()\nZC.lookups &lt;- list()\nZC.example &lt;- list()\n# Loop through the columns to populate the lists\nfor (ZC.ThisColumn in colnames(ZC.data))\n{\n  #Increment the index for the lists\n  ZC.index &lt;- ZC.index+1 \n  # Assign the column name\n  ZC.colname[ZC.index] &lt;- ZC.ThisColumn \n  # Assign the column type\n  ZC.coltype[ZC.index] &lt;- typeof(ZC.data[[ZC.ThisColumn]]) \n  # Count the number of NAs in the column\n  ZC.NAvals[ZC.index] &lt;- sum(is.na(ZC.data[[ZC.ThisColumn]])) \n  # Count the number of unique values in the column\n  ZC.TheseValues &lt;- table(ZC.data[[ZC.ThisColumn]])\n  ZC.unique[ZC.index] &lt;- length(ZC.TheseValues)\n  # Determine if the column has an associated lookup table\n  # Assume there is no lookup table\n  ZC.lookups[ZC.index] &lt;- 0\n  # Search for the column in the list of columns with associated lookup tables\n  # Flag it is the column is found to be associated with a lookup table\n  for (ZC.ThisLookupColumn in ZC.lookupcolumn)\n  {\n    if (ZC.ThisLookupColumn==ZC.colname[ZC.index]) \n    {\n      # Column found, flag it as having a lookup table association\n      ZC.lookups[ZC.index] &lt;- 1\n    }\n  }\n  # Find a suitable example value\n  # Prepare to loop through all the values to find an acceptable example\n  for (ZC.ThisRow in 1:nrow(ZC.data)) \n  {\n    # Start this FOR loop but prepare to break out of it\n    ZC.BreakFlag = 0\n    # As soon as an acceptable value is found, set the break flag to 1\n    # Retrieve the value in ThisRow\n    ZC.ThisExp &lt;- ZC.data[[ZC.ThisColumn]][ZC.ThisRow]\n    # Continue with this example only if it is not NA\n    if (!is.na(ZC.ThisExp))\n    {\n      # Subsequent processing depends on data type of current example\n      # Is the current example a character string?\n      if (typeof(ZC.ThisExp) == \"character\")\n      {\n        # Trim any leading and trailing spaces from the current example\n        # If there is nothing but spaces, only an empty string will survive\n        ZC.ThisExp &lt;- trimws(ZC.ThisExp)\n        # Continue with this example only if it is not empty\n        if (nchar(ZC.ThisExp)&gt;0)\n        {\n          # We have an acceptable example\n          ZC.BreakFlag &lt;- 1\n          # Let's cut it short if the example is too long\n          if (nchar(ZC.ThisExp)&gt;8)\n          {\n            ZC.ThisExp &lt;- paste(substr(ZC.ThisExp, 1, 5), \"...\", sep=\"\")\n          }\n        }\n      } else if (typeof(ZC.ThisExp) == \"integer\" | typeof(ZC.ThisExp) == \"double\")\n      {\n        # We would prefer a nonzero value, and we will loop through all the values if necessary to find one\n        if (!ZC.ThisExp==0) ZC.BreakFlag &lt;- 1\n      } else\n      { # The value could be boolean, complex, or something else\n        # We will take it\n        ZC.BreakFlag &lt;- 1\n      }\n    }\n    # If we have an acceptable value, we can stop looping through the rows\n    if (ZC.BreakFlag == 1) break\n  }\n  ZC.example[ZC.index] &lt;- ZC.ThisExp\n}\n# Bind the lists together in a single object\nZC.datasum02 &lt;- cbind(ZC.colname, ZC.coltype, ZC.NAvals, ZC.unique, ZC.lookups, ZC.example)\n# Apply more friendly column names\ncolnames(ZC.datasum02) &lt;- c(\"Column\"\n                          , \"Data Type\"\n                          , \"NA Values\"\n                          , \"Uniques\"\n                          , \"LU\"\n                          , \"Example\"\n)\nZC.datasum02\n\n      Column                      Data Type   NA Values Uniques LU Example   \n [1,] \"YearStart\"                 \"integer\"   0         12      0  2020      \n [2,] \"Class\"                     \"character\" 0         3       0  \"Physi...\"\n [3,] \"Topic\"                     \"character\" 0         3       0  \"Physi...\"\n [4,] \"Question\"                  \"character\" 0         9       0  \"Perce...\"\n [5,] \"Data_Value\"                \"double\"    9235      695     0  30.6      \n [6,] \"Low_Confidence_Limit\"      \"double\"    9235      666     0  29.4      \n [7,] \"High_Confidence_Limit\"     \"double\"    9235      761     0  31.8      \n [8,] \"Sample_Size\"               \"integer\"   9235      9761    0  31255     \n [9,] \"AgeYears\"                  \"character\" 0         7       0  \"25 - 34\" \n[10,] \"Education\"                 \"character\" 0         5       0  \"High ...\"\n[11,] \"Gender\"                    \"character\" 0         3       0  \"Female\"  \n[12,] \"Income\"                    \"character\" 0         8       0  \"$50,0...\"\n[13,] \"RaceEthnic\"                \"character\" 0         9       0  \"Hispanic\"\n[14,] \"ClassID\"                   \"character\" 0         3       0  \"PA\"      \n[15,] \"TopicID\"                   \"character\" 0         3       0  \"PA1\"     \n[16,] \"QuestionID\"                \"character\" 0         9       0  \"Q047\"    \n[17,] \"LocationID\"                \"integer\"   0         55      1  59        \n[18,] \"StratificationCategoryId1\" \"character\" 0         7       1  \"RACE\"    \n[19,] \"StratificationID1\"         \"character\" 0         29      1  \"RACEHIS\" \n\nprint(typeof(ZC.datasum02)) # TDL\n\n[1] \"list\"\n\n\nOnce you have the data processed and cleaned, perform some exploratory/descriptive analysis on this cleaned dataset. Make some summary tables, produce some figures. Try to summarize each variable in a way that it can be described by a distribution. For instance if you have a categorical variable, show what % are in each category. If you have a continuous variable, make a plot to see if it’s approximately normal, then try to summarize it to determine its mean and standard deviation.\nThe idea is that your descriptive analysis will provide enough information for your classmate to make synthetic data that looks similar, along the lines discussed in the synthetic data module.\nRemember to add both text to your Quarto file and comments into your code to explain what you are doing.\nNow that the dataset has been whittled down to nineteen (19) pertinent columns, graphics for each column will be generated. Using the summary table from above, chart type will be chosen as follows:\n\nAll columns with ten (10) or less unique values will be represented by a pie chart.\nAll columns with more than ten (10) but with twenty (20) or less unique values will be represented by a histogram (bar chart).\nAll other columns will be represented by a line plot.\n\nAdditionally, where the data is predetermined to be a continuous variable, a standard statistical summary will also be produced.\n\n\n# Convert summary table from list to dataframe\nZC.Summary &lt;- as.data.frame(ZC.datasum02)\n# Create list of columns predetermined to contain values in a continuous variable\nZC.StatsPlease &lt;- c(\"Data_Value\"\n                    , \"Low_Confidence_Limit\"\n                    , \"High_Confidence_Limit\"\n                    , \"Sample_Size\")\nZC.StatsPlease &lt;- unlist(ZC.StatsPlease)\nprint(ZC.StatsPlease) # TDL\n\n[1] \"Data_Value\"            \"Low_Confidence_Limit\"  \"High_Confidence_Limit\"\n[4] \"Sample_Size\"          \n\nprint(typeof(ZC.StatsPlease)) # TDL\n\n[1] \"character\"\n\n# Loop through each of the columns\nZC.ChartColumns &lt;- 1:nrow(ZC.Summary)\nfor (ZC.ChartThis in ZC.ChartColumns)\n{\n  # Each value loaded is embedded in a one-item list, so it has to be unlisted before using\n  # Load column name\n  ZC.ThisColumn &lt;- ZC.Summary$Column[ZC.ChartThis]\n  ZC.ThisColumn &lt;- unlist(ZC.ThisColumn)\n  # Load unique column count - determines chart type\n  ZC.ValueCount &lt;- ZC.Summary$Uniques[ZC.ChartThis]\n  ZC.ValueCount &lt;- unlist(ZC.ValueCount)\n  # source of chart data will be the same regardless of chart type\n  ZC.ThisQuery &lt;- paste (\"select \"\n                           , ZC.ThisColumn\n                           , \" as Labels, count(*) as Occurs from 'ZC.data' \"\n                           , \"group by \"\n                           , ZC.ThisColumn\n                           , \" order by \"\n                           , ZC.ThisColumn\n                           , sep=\"\")\n    # Run the Query\n    ZC.DataTable &lt;- sqldf(ZC.ThisQuery)\n    ZC.DataTable &lt;- as.data.frame(ZC.DataTable)\n    ZC.printline &lt;- paste(\"Data for \"\n                          , ZC.ThisColumn\n                          , \":\"\n                          , sep=\"\") # TDL\n    print(ZC.printline) # TDL\n    if(!(ZC.ValueCount&gt;20))\n      print(ZC.DataTable) # TDL\n    else\n    {\n      ZC.printline &lt;- paste(\"Data suppressed.  \"\n                            , ZC.ValueCount\n                            , \" rows.\"\n                            , sep=\"\"\n      )\n      print(ZC.printline)\n    }\n  if (!(ZC.ValueCount&gt;10))\n  {\n    # Drawing a pie chart\n  } else if (!(ZC.ValueCount&gt;20))\n  {\n    # building a histogram\n  } else\n  {\n    # Drawing a distribution plot\n  }\n  # printing summary statistics?\n    # if (grepl(ZC.ThisColumn, ZC.StatsPlease))\n    # {\n      # print(summary(ZC.data[[ZC.ThisColumn]]))\n    # }\n  print(\"******\") # TDL\n}\n\n[1] \"Data for YearStart:\"\n   Labels Occurs\n1    2011  10192\n2    2012   4368\n3    2013  10248\n4    2014   4536\n5    2015  10584\n6    2016   4620\n7    2017  13860\n8    2018   4620\n9    2019  13365\n10   2020   4536\n11   2021   7700\n12   2022   4620\n[1] \"******\"\n[1] \"Data for Class:\"\n                   Labels Occurs\n1   Fruits and Vegetables   9130\n2 Obesity / Weight Status  36234\n3       Physical Activity  47885\n[1] \"******\"\n[1] \"Data for Topic:\"\n                            Labels Occurs\n1 Fruits and Vegetables - Behavior   9130\n2          Obesity / Weight Status  36234\n3     Physical Activity - Behavior  47885\n[1] \"******\"\n[1] \"Data for Question:\"\n                                                                                                                                                                                                                                                 Labels\n1                                                                                                                                                                       Percent of adults aged 18 years and older who have an overweight classification\n2                                                                                                                                                                                            Percent of adults aged 18 years and older who have obesity\n3                                                  Percent of adults who achieve at least 150 minutes a week of moderate-intensity aerobic physical activity or 75 minutes a week of vigorous-intensity aerobic activity (or an equivalent combination)\n4 Percent of adults who achieve at least 150 minutes a week of moderate-intensity aerobic physical activity or 75 minutes a week of vigorous-intensity aerobic physical activity and engage in muscle-strengthening activities on 2 or more days a week\n5                                                 Percent of adults who achieve at least 300 minutes a week of moderate-intensity aerobic physical activity or 150 minutes a week of vigorous-intensity aerobic activity (or an equivalent combination)\n6                                                                                                                                                              Percent of adults who engage in muscle-strengthening activities on 2 or more days a week\n7                                                                                                                                                                                     Percent of adults who engage in no leisure-time physical activity\n8                                                                                                                                                                                 Percent of adults who report consuming fruit less than one time daily\n9                                                                                                                                                                            Percent of adults who report consuming vegetables less than one time daily\n  Occurs\n1  18117\n2  18117\n3   7449\n4   7449\n5   7449\n6   7449\n7  18089\n8   4565\n9   4565\n[1] \"******\"\n[1] \"Data for Data_Value:\"\n[1] \"Data suppressed.  695 rows.\"\n[1] \"******\"\n[1] \"Data for Low_Confidence_Limit:\"\n[1] \"Data suppressed.  666 rows.\"\n[1] \"******\"\n[1] \"Data for High_Confidence_Limit:\"\n[1] \"Data suppressed.  761 rows.\"\n[1] \"******\"\n[1] \"Data for Sample_Size:\"\n[1] \"Data suppressed.  9761 rows.\"\n[1] \"******\"\n[1] \"Data for AgeYears:\"\n       Labels Occurs\n1              73269\n2     18 - 24   3330\n3     25 - 34   3330\n4     35 - 44   3330\n5     45 - 54   3330\n6     55 - 64   3330\n7 65 or older   3330\n[1] \"******\"\n[1] \"Data for Education:\"\n                            Labels Occurs\n1                                   79929\n2                 College graduate   3330\n3             High school graduate   3330\n4            Less than high school   3330\n5 Some college or technical school   3330\n[1] \"******\"\n[1] \"Data for Gender:\"\n  Labels Occurs\n1         86589\n2 Female   3330\n3   Male   3330\n[1] \"******\"\n[1] \"Data for Income:\"\n              Labels Occurs\n1                     69939\n2  $15,000 - $24,999   3330\n3  $25,000 - $34,999   3330\n4  $35,000 - $49,999   3330\n5  $50,000 - $74,999   3330\n6 $75,000 or greater   3330\n7  Data not reported   3330\n8  Less than $15,000   3330\n[1] \"******\"\n[1] \"Data for RaceEthnic:\"\n                         Labels Occurs\n1                                66609\n2               2 or more races   3330\n3 American Indian/Alaska Native   3330\n4                         Asian   3330\n5     Hawaiian/Pacific Islander   3330\n6                      Hispanic   3330\n7            Non-Hispanic Black   3330\n8            Non-Hispanic White   3330\n9                         Other   3330\n[1] \"******\"\n[1] \"Data for ClassID:\"\n  Labels Occurs\n1     FV   9130\n2    OWS  36234\n3     PA  47885\n[1] \"******\"\n[1] \"Data for TopicID:\"\n  Labels Occurs\n1    FV1   9130\n2   OWS1  36234\n3    PA1  47885\n[1] \"******\"\n[1] \"Data for QuestionID:\"\n  Labels Occurs\n1   Q018   4565\n2   Q019   4565\n3   Q036  18117\n4   Q037  18117\n5   Q043   7449\n6   Q044   7449\n7   Q045   7449\n8   Q046   7449\n9   Q047  18089\n[1] \"******\"\n[1] \"Data for LocationID:\"\n[1] \"Data suppressed.  55 rows.\"\n[1] \"******\"\n[1] \"Data for StratificationCategoryId1:\"\n  Labels Occurs\n1             9\n2  AGEYR  19980\n3    EDU  13320\n4    GEN   6660\n5    INC  23310\n6    OVR   3330\n7   RACE  26640\n[1] \"******\"\n[1] \"Data for StratificationID1:\"\n[1] \"Data suppressed.  29 rows.\"\n[1] \"******\"\n\n\nIn a final step, update the _quarto.yml file and include a menu item for “Data Analysis Exercise” pointing to the new file. Follow the format of the existing entries. Remember to be very careful about the right amount of empty space. Re-create your website and make sure it all works and the new project shows up on the website.\n\n# No code changes were made here for this part of the assignment.\n\nDone.\n\n\n\nIf everything works as expected, commit and push your changes to GitHub. Instead of using the fork + pull-request workflow we’ve tried a few times, we’ll explore a different collaborative approach. In this approach, you and your collaborator work on the same repository. To that end, you need to add your classmate as collaborator. Go to Github.com, find your portfolio repository, go to Settings, then Collaborators. Choose Add Collaborator and add your classmate. Your classmate should receive an invitation, which they need to accept. With this, they are now able to directly push and pull to your repository, without them needing to create a fork. (You can remove them after this exercise if you don’t want them to be able to continue having write access to your repository).\n\n# No code changes were made here for this part of the assignment.\n\nDone."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#end-of-assignment-5",
    "href": "cdcdata-exercise/cdcdata-exercise.html#end-of-assignment-5",
    "title": "cdcdata-exercise",
    "section": "",
    "text": "# Assignment 5 - END\n# Go Roadrunners!"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#part-2",
    "href": "cdcdata-exercise/cdcdata-exercise.html#part-2",
    "title": "cdcdata-exercise",
    "section": "",
    "text": "# START - Space Reserved for Collaborator\n\n\n# END - Space Reserved for Collaborator"
  }
]